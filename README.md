# Walmart-Recruiting---Store-Sales-Forecasting

მოცემული გვქონდა Walmart-ის სხვადასხვა რეგიონში მდებარე 45 მაღაზიის გაყიდვების მონაცემები 2010-2012 წლებში. მიზანი იყო თითოეული მაღაზიის მასშტაბით გაყიდვების პროგნოზირება. 
გადმოგვეცემოდა შემდეგი ფაილები : 
train.csv მთავარი სატრენინგო ფაილი 5 სვეტით :  Store, Dept, Date, Weekly_Sales (target), isHoliday.
test.csv სატესტო ფაილი. იგივე რაც train.csv Weekly_Sales გარეშე.
და features.csv - დამატებითი მონაცემებისთვის, როგორიცაა ტემპერატურა, საწვავის ფასი, ფასდაკლების კვირეული, უმუშევრობისა და სამომხმარებლო ფასების ინდექსი.
ასევე გადმოცემული გვქონდა 4 კონკრეტული დღესასწაულის თარიღები: Super Bowl, Labor Day, მადლიერების დღე და შობა.

**მონაცემების ანალიზი :** ავაგეთ სხვადასხვა პლოტი, რომ დაგვენახა ზოგადი ტრენდი, სეზონურობა, თუ რა მნიშვნელობა ჰქონდა გაყიდვებზე მაღაზიის ტიპსა თუ დეპარტამენტს, რამდენად განმსაზღვრელი იყო დღესასწაულის პერიოდი, რა კორელაცია იყო ფიჩერებს შორის. გამოვიყენეთ ავტოკორელაციისა და  ნაწილობრივი ავტოკორელაციის ფუნქციაც.

**მონაცემების დამუშავება :** საჭირო იყო NaN მნიშვნელობების შევსება. Markdown შემთხვევაში NaN მნიშვნელობა სავარაუდოდ ფასდაკლების არარსებობაზე მიუთითებდა და შევავსეთ 0-ით. სხვა შემთხვევაში გამოვიყენეთ ffill(), რაც უჯრას წინა მნიშვნელობით ავსებს. ასევე შევცვალეთ დროის ფორმატი, კატეგორიული ცვლადები რიცხვითში  გადავიყვანეთ და მოვახდინეთ მონაცემების ნორმალიზაცია/სტანდარტიზაცია. 

**მონაცემთა ინჟინერია :** დავამატეთ ლაგ ფიჩერები და მოძრავი საშუალოები. ასევე isHoliday გავშალეთ 4-დან ერთ კონკრეტულ დღესასწაულად.

**ტესტირებული მოდელები :**

**Deep Learning მოდელები**

1. **N-BEATS** (Neural Basis Expansion Analysis for Time Series) იყენებს ბლოკებს. ეს ბლოკები ერთმანეთზე სტეკივით ლაგდება. მექანიზმი ასეთია : პირველი ბლოკი ცდილობს წარსულის ახსნას (backcast), გარკვეული პატერნის, ტრენდის ან სეზონურობის. შემდეგ იწინასწარმეტყველებს მომავლის გარკვეულ ნაწილს (forecast). მეორე ბლოკი უკვე პირველის backcast-ის გარეშე არსებულ ინფორმაციას შეხედავს და ა.შ. ეს მოდელი N-BEATSx-გან განსხვავებით უყურებს მხოლოდ target მნიშვნელობებს და დამატებით ინფორმაციას არ ითვალისწინებს უკეთესი წინასწარმეტყველებისთვის.
ასე რომ, პრეპროცესინგი აღარ იყო საჭირო. გამოვიყენე neuralforecast ბიბლიოთეკა. შევქმენი ქვეკლასი N_beats, დავარეგულირე gamma (learning rate რამდენად მცირდება), input_size (რამდენი კვირის ინფორმაცია გადაეცემა მოდელს წინასწარმეტყველებისთვის), horizon (რამდენი კვირა უნდა იწინასწარმეტყველოს), stack types (როგორ ბლოკებს იყენებს მოდელი პატერნების დასაჭერად. ძირითადია შემდეგი 3 : identity, trend, seasonality), number of blocks, batch size. საბოლოო wmae მივიღე 2087


**Tree-Based Models** 
1. **LightGBM** წარმოადგენს ensemble არქიტექტურას - ანუ ბევრი სუსტი ხისგან საბოლოოდ ვიღებთ კარგს. არის ძალიან სწრაფი და მოქნილი, მაგრამ overfit დიდი შანსია. 
Preprocessing : დავმერჯე train store  და feature-თან. CPI და Unemployment-ში NaN მნიშვნელობები შევავსე ffill()-ით, ხოლო markdown-ში 0-ით, რაც ფასდაკლების არარსებობაზე მიუთითებს. კატეგორიული ცვლადები numerical-ში გადავიყვანე. Date ფორმატი გავასწორე.
Feature Engineering : isHoliday-ს დავუმატე 4 დღესასწაულის აღმნიშვნელი სვეტი : მადლიერების დღე, შობა, super bowl, labor day. ასევე დავამატე days since last და days to next holiday. ასევე lag (1, 2, 3) და moving average სვეტები. Date-ს მიხედვით დავამატე წლის, თვის, კვირისა და დღის აღმნიშვნელი სვეტებიც. 
დავსპლიტე დატა 3 ნაწილად : train, validation და test. დავაtuning learning rate, number of leaves, subsample, colsample_bytree. პლოტებზე ჩანს დარანკული ფიჩერები, და თითოეული პარამეტრის მნიშვნელობა საბოლოო შედეგზე. wmae = 589, თუმცა კეგლზე უფრო მეტი მომცა. 


**Classical Statistical Time-Series Models**
1. **ARIMA** - 
