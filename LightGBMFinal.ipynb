{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4cPYckBh5YKKYYk5r/tYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvani2/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/LightGBMFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmL4iKfgdJcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe15cd8-935e-441c-937f-bbf568a54799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "100% 2.70M/2.70M [00:00<00:00, 511MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  features.csv.zip\n",
            "  inflating: features.csv            \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  sampleSubmission.csv.zip\n",
            "  inflating: sampleSubmission.csv    \n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/cs231n/assignments/finalproject/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting.zip\n",
        "!unzip features.csv.zip\n",
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip sampleSubmission.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "features = pd.read_csv(\"features.csv\")\n",
        "stores = pd.read_csv(\"stores.csv\")"
      ],
      "metadata": {
        "id": "ZXitZUhTd-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    def merge_data(self, main_df, features_df, stores_df):\n",
        "        merged = pd.merge(main_df, features_df, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\n",
        "        merged = pd.merge(merged, stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n",
        "\n",
        "    def convert_date(self, df):\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        return df\n",
        "\n",
        "    def extract_date_features(self, df):\n",
        "        df[\"Year\"] = df[\"Date\"].dt.year\n",
        "        df[\"Month\"] = df[\"Date\"].dt.month\n",
        "        df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "        df[\"Day\"] = df[\"Date\"].dt.dayofweek\n",
        "        return df\n",
        "\n",
        "    def fill_markdowns(self, df):\n",
        "        markdown_cols = [f\"MarkDown{i}\" for i in range(1, 6)]\n",
        "        df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "        return df\n",
        "\n",
        "    def fill_economics(self, df):\n",
        "        df[\"CPI\"] = df[\"CPI\"].ffill().bfill()\n",
        "        df[\"Unemployment\"] = df[\"Unemployment\"].ffill().bfill()\n",
        "        return df\n",
        "\n",
        "    def encode_types(self, df):\n",
        "        df[\"Type\"] = df[\"Type\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
        "        return df\n",
        "\n",
        "    def process_data(self, main_df, features_df, stores_df):\n",
        "        df = self.merge_data(main_df, features_df, stores_df)\n",
        "        df = self.convert_date(df)\n",
        "        df = self.extract_date_features(df)\n",
        "        df = self.fill_markdowns(df)\n",
        "        df = self.fill_economics(df)\n",
        "        df = self.encode_types(df)\n",
        "        return df"
      ],
      "metadata": {
        "id": "PwKqPmMfeB3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = DataProcessor()\n",
        "\n",
        "train_merged = processor.process_data(train, features, stores)\n",
        "test_merged = processor.process_data(test, features, stores)"
      ],
      "metadata": {
        "id": "EGc1O_Qyepaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged.shape, test_merged.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH6WKExVeqWp",
        "outputId": "1683070c-263c-422b-8c41-9fd7fb7fb84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((421570, 20), (115064, 19))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not train_merged['Date'].is_monotonic_increasing:\n",
        "    train_merged = train_merged.sort_values(by='Date')\n",
        "\n",
        "display(train_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "7ZKcKPqJfBtJ",
        "outputId": "a2c5beea-4d7a-401d-fa7d-f829f60f3791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50      False        42.31   \n",
              "277665     29     5 2010-02-05      15552.08      False        24.36   \n",
              "277808     29     6 2010-02-05       3200.22      False        24.36   \n",
              "277951     29     7 2010-02-05      10820.05      False        24.36   \n",
              "278094     29     8 2010-02-05      20055.64      False        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
              "0            2.572        0.0        0.0        0.0        0.0        0.0   \n",
              "277665       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "277808       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "277951       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "278094       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "\n",
              "               CPI  Unemployment  Type    Size  Year  Month  Week  Day  \n",
              "0       211.096358         8.106     0  151315  2010      2     5    4  \n",
              "277665  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "277808  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "277951  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "278094  131.527903        10.064     1   93638  2010      2     5    4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>MarkDown4</th>\n",
              "      <th>MarkDown5</th>\n",
              "      <th>CPI</th>\n",
              "      <th>Unemployment</th>\n",
              "      <th>Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>False</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>211.096358</td>\n",
              "      <td>8.106</td>\n",
              "      <td>0</td>\n",
              "      <td>151315</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2170447d-c18d-4393-aed1-3a83d81d0286\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2170447d-c18d-4393-aed1-3a83d81d0286')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2170447d-c18d-4393-aed1-3a83d81d0286 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(train_merged\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Store\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 29,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          29,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dept\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2010-02-05 00:00:00\",\n        \"max\": \"2010-02-05 00:00:00\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2010-02-05 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weekly_Sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8381.925873522147,\n        \"min\": 3200.22,\n        \"max\": 24924.5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          15552.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IsHoliday\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.027484039224246,\n        \"min\": 24.36,\n        \"max\": 42.31,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          24.36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel_Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0965981366279908,\n        \"min\": 2.572,\n        \"max\": 2.788,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.788\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.5840948489266,\n        \"min\": 131.5279032,\n        \"max\": 211.0963582,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          131.5279032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unemployment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8756442199889177,\n        \"min\": 8.106,\n        \"max\": 10.064,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10.064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25793,\n        \"min\": 93638,\n        \"max\": 151315,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          93638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2010\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Month\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Week\",\n      \"properties\": {\n        \"dtype\": \"UInt32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Day\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged['Weekly_Sales_Lag1'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).fillna(0)\n",
        "train_merged['Weekly_Sales_Lag2'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2).fillna(0)\n",
        "train_merged['Weekly_Sales_Lag3'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(3).fillna(0)\n",
        "\n",
        "train_merged['Weekly_Sales_MA4'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=4).mean()).fillna(0)\n",
        "train_merged['Weekly_Sales_MA12'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=12).mean()).fillna(0)\n",
        "train_merged['Weekly_Sales_MA26'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=26).mean()).fillna(0)\n",
        "\n",
        "display(train_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "ailJAOHWhDp3",
        "outputId": "109d1dff-ece6-4284-e52f-23396f269066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50      False        42.31   \n",
              "277665     29     5 2010-02-05      15552.08      False        24.36   \n",
              "277808     29     6 2010-02-05       3200.22      False        24.36   \n",
              "277951     29     7 2010-02-05      10820.05      False        24.36   \n",
              "278094     29     8 2010-02-05      20055.64      False        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  ...  Year  Month  Week  \\\n",
              "0            2.572        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277665       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277808       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277951       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "278094       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "\n",
              "        Day  Weekly_Sales_Lag1  Weekly_Sales_Lag2  Weekly_Sales_Lag3  \\\n",
              "0         4                0.0                0.0                0.0   \n",
              "277665    4                0.0                0.0                0.0   \n",
              "277808    4                0.0                0.0                0.0   \n",
              "277951    4                0.0                0.0                0.0   \n",
              "278094    4                0.0                0.0                0.0   \n",
              "\n",
              "        Weekly_Sales_MA4  Weekly_Sales_MA12  Weekly_Sales_MA26  \n",
              "0                    0.0                0.0                0.0  \n",
              "277665               0.0                0.0                0.0  \n",
              "277808               0.0                0.0                0.0  \n",
              "277951               0.0                0.0                0.0  \n",
              "278094               0.0                0.0                0.0  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17483e73-e4cb-455a-9fb7-5671a4eedb02\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>...</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "      <th>Weekly_Sales_Lag1</th>\n",
              "      <th>Weekly_Sales_Lag2</th>\n",
              "      <th>Weekly_Sales_Lag3</th>\n",
              "      <th>Weekly_Sales_MA4</th>\n",
              "      <th>Weekly_Sales_MA12</th>\n",
              "      <th>Weekly_Sales_MA26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>False</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17483e73-e4cb-455a-9fb7-5671a4eedb02')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-17483e73-e4cb-455a-9fb7-5671a4eedb02 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-17483e73-e4cb-455a-9fb7-5671a4eedb02');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "major_holiday_dates = train_merged[train_merged['IsHoliday'] == True]['Date'].unique()\n",
        "\n",
        "def days_to_next_holiday(date, holidays):\n",
        "    future_holidays = holidays[holidays > date]\n",
        "    if len(future_holidays) > 0:\n",
        "        return (future_holidays.min() - date).days\n",
        "    return -1 # Or some other indicator for no future holidays\n",
        "\n",
        "def days_since_last_holiday(date, holidays):\n",
        "    past_holidays = holidays[holidays < date]\n",
        "    if len(past_holidays) > 0:\n",
        "        return (date - past_holidays.max()).days\n",
        "    return -1 # Or some other indicator for no past holidays\n",
        "\n",
        "# Apply holiday proximity features to both train and test data\n",
        "train_merged['Days_To_Next_Holiday'] = train_merged['Date'].apply(lambda x: days_to_next_holiday(x, major_holiday_dates))\n",
        "train_merged['Days_Since_Last_Holiday'] = train_merged['Date'].apply(lambda x: days_since_last_holiday(x, major_holiday_dates))\n",
        "\n",
        "test_merged['Days_To_Next_Holiday'] = test_merged['Date'].apply(lambda x: days_to_next_holiday(x, major_holiday_dates))\n",
        "test_merged['Days_Since_Last_Holiday'] = test_merged['Date'].apply(lambda x: days_since_last_holiday(x, major_holiday_dates))\n",
        "\n",
        "holiday_dates = {\n",
        "    \"Super_Bowl\": [\"2010-02-12\", \"2011-02-11\", \"2012-02-10\", \"2013-02-08\"],\n",
        "    \"Labor_Day\": [\"2010-09-10\", \"2011-09-09\", \"2012-09-07\", \"2013-09-06\"],\n",
        "    \"Thanksgiving\": [\"2010-11-26\", \"2011-11-25\", \"2012-11-23\", \"2013-11-29\"],\n",
        "    \"Christmas\": [\"2010-12-31\", \"2011-12-30\", \"2012-12-28\", \"2013-12-27\"]\n",
        "}\n",
        "\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    train_merged[holiday] = train_merged[\"Date\"].isin(pd.to_datetime(dates)).astype(int)\n",
        "    test_merged[holiday] = test_merged[\"Date\"].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "train_merged['IsHoliday'] = train_merged['IsHoliday'].astype(int)\n",
        "test_merged['IsHoliday'] = test_merged['IsHoliday'].astype(int)\n",
        "\n",
        "display(train_merged.head())\n",
        "display(test_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "vPXVIfStjU_K",
        "outputId": "65ab8c20-21ce-49db-f2b4-88988925e7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50          0        42.31   \n",
              "277665     29     5 2010-02-05      15552.08          0        24.36   \n",
              "277808     29     6 2010-02-05       3200.22          0        24.36   \n",
              "277951     29     7 2010-02-05      10820.05          0        24.36   \n",
              "278094     29     8 2010-02-05      20055.64          0        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  ...  Weekly_Sales_Lag3  \\\n",
              "0            2.572        0.0        0.0        0.0  ...                0.0   \n",
              "277665       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "277808       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "277951       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "278094       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "\n",
              "        Weekly_Sales_MA4  Weekly_Sales_MA12  Weekly_Sales_MA26  \\\n",
              "0                    0.0                0.0                0.0   \n",
              "277665               0.0                0.0                0.0   \n",
              "277808               0.0                0.0                0.0   \n",
              "277951               0.0                0.0                0.0   \n",
              "278094               0.0                0.0                0.0   \n",
              "\n",
              "        Days_To_Next_Holiday  Days_Since_Last_Holiday  Super_Bowl  Labor_Day  \\\n",
              "0                          7                       -1           0          0   \n",
              "277665                     7                       -1           0          0   \n",
              "277808                     7                       -1           0          0   \n",
              "277951                     7                       -1           0          0   \n",
              "278094                     7                       -1           0          0   \n",
              "\n",
              "        Thanksgiving  Christmas  \n",
              "0                  0          0  \n",
              "277665             0          0  \n",
              "277808             0          0  \n",
              "277951             0          0  \n",
              "278094             0          0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-414e9d70-6a2c-4993-8e25-0f168b4165c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>...</th>\n",
              "      <th>Weekly_Sales_Lag3</th>\n",
              "      <th>Weekly_Sales_MA4</th>\n",
              "      <th>Weekly_Sales_MA12</th>\n",
              "      <th>Weekly_Sales_MA26</th>\n",
              "      <th>Days_To_Next_Holiday</th>\n",
              "      <th>Days_Since_Last_Holiday</th>\n",
              "      <th>Super_Bowl</th>\n",
              "      <th>Labor_Day</th>\n",
              "      <th>Thanksgiving</th>\n",
              "      <th>Christmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>0</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-414e9d70-6a2c-4993-8e25-0f168b4165c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-414e9d70-6a2c-4993-8e25-0f168b4165c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-414e9d70-6a2c-4993-8e25-0f168b4165c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-955dac06-f636-4b86-823b-7c167e96dcce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-955dac06-f636-4b86-823b-7c167e96dcce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-955dac06-f636-4b86-823b-7c167e96dcce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  \\\n",
              "0      1     1 2012-11-02          0        55.32       3.386    6766.44   \n",
              "1      1     1 2012-11-09          0        61.24       3.314   11421.32   \n",
              "2      1     1 2012-11-16          0        52.92       3.252    9696.28   \n",
              "3      1     1 2012-11-23          1        56.23       3.211     883.59   \n",
              "4      1     1 2012-11-30          0        52.34       3.207    2460.03   \n",
              "\n",
              "   MarkDown2  MarkDown3  MarkDown4  ...  Year  Month  Week  Day  \\\n",
              "0    5147.70      50.82    3639.90  ...  2012     11    44    4   \n",
              "1    3370.89      40.28    4646.79  ...  2012     11    45    4   \n",
              "2     292.10     103.78    1133.15  ...  2012     11    46    4   \n",
              "3       4.17   74910.32     209.91  ...  2012     11    47    4   \n",
              "4       0.00    3838.35     150.57  ...  2012     11    48    4   \n",
              "\n",
              "   Days_To_Next_Holiday  Days_Since_Last_Holiday  Super_Bowl  Labor_Day  \\\n",
              "0                    -1                       56           0          0   \n",
              "1                    -1                       63           0          0   \n",
              "2                    -1                       70           0          0   \n",
              "3                    -1                       77           0          0   \n",
              "4                    -1                       84           0          0   \n",
              "\n",
              "   Thanksgiving  Christmas  \n",
              "0             0          0  \n",
              "1             0          0  \n",
              "2             0          0  \n",
              "3             1          0  \n",
              "4             0          0  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce22b629-cba7-4b57-980d-920c668da1e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>MarkDown4</th>\n",
              "      <th>...</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "      <th>Days_To_Next_Holiday</th>\n",
              "      <th>Days_Since_Last_Holiday</th>\n",
              "      <th>Super_Bowl</th>\n",
              "      <th>Labor_Day</th>\n",
              "      <th>Thanksgiving</th>\n",
              "      <th>Christmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-02</td>\n",
              "      <td>0</td>\n",
              "      <td>55.32</td>\n",
              "      <td>3.386</td>\n",
              "      <td>6766.44</td>\n",
              "      <td>5147.70</td>\n",
              "      <td>50.82</td>\n",
              "      <td>3639.90</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>0</td>\n",
              "      <td>61.24</td>\n",
              "      <td>3.314</td>\n",
              "      <td>11421.32</td>\n",
              "      <td>3370.89</td>\n",
              "      <td>40.28</td>\n",
              "      <td>4646.79</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-16</td>\n",
              "      <td>0</td>\n",
              "      <td>52.92</td>\n",
              "      <td>3.252</td>\n",
              "      <td>9696.28</td>\n",
              "      <td>292.10</td>\n",
              "      <td>103.78</td>\n",
              "      <td>1133.15</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-23</td>\n",
              "      <td>1</td>\n",
              "      <td>56.23</td>\n",
              "      <td>3.211</td>\n",
              "      <td>883.59</td>\n",
              "      <td>4.17</td>\n",
              "      <td>74910.32</td>\n",
              "      <td>209.91</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>47</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-30</td>\n",
              "      <td>0</td>\n",
              "      <td>52.34</td>\n",
              "      <td>3.207</td>\n",
              "      <td>2460.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3838.35</td>\n",
              "      <td>150.57</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>48</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce22b629-cba7-4b57-980d-920c668da1e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce22b629-cba7-4b57-980d-920c668da1e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce22b629-cba7-4b57-980d-920c668da1e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-82e3a822-09d3-41c3-95bb-3ef29d787b81\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82e3a822-09d3-41c3-95bb-3ef29d787b81')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-82e3a822-09d3-41c3-95bb-3ef29d787b81 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the split points (e.g., 80% for training, 10% for validation, 10% for testing)\n",
        "train_size = int(len(train_merged) * 0.8)\n",
        "val_size = int(len(train_merged) * 0.1)\n",
        "test_size = len(train_merged) - train_size - val_size\n",
        "\n",
        "# Find the dates at the calculated split points\n",
        "train_split_date = train_merged.iloc[train_size]['Date']\n",
        "val_split_date = train_merged.iloc[train_size + val_size]['Date']\n",
        "\n",
        "# Find the closest Friday to the split dates within the dataset\n",
        "# We can iterate through the dates in the merged dataframe to find the exact Friday\n",
        "def find_closest_friday(date, df_dates):\n",
        "    closest_friday = None\n",
        "    for d in df_dates:\n",
        "        if d >= date and d.dayofweek == 4: # Friday is dayofweek 4\n",
        "            closest_friday = d\n",
        "            break\n",
        "    return closest_friday\n",
        "\n",
        "all_dates = train_merged['Date'].unique()\n",
        "train_split_date_friday = find_closest_friday(train_split_date, all_dates)\n",
        "val_split_date_friday = find_closest_friday(val_split_date, all_dates)\n",
        "\n",
        "print(f\"Original train split date: {train_split_date}\")\n",
        "print(f\"Adjusted train split date (Friday): {train_split_date_friday}\")\n",
        "print(f\"Original validation split date: {val_split_date}\")\n",
        "print(f\"Adjusted validation split date (Friday): {val_split_date_friday}\")\n",
        "\n",
        "# Split the data based on the adjusted Friday dates\n",
        "train_df = train_merged[train_merged['Date'] < train_split_date_friday]\n",
        "val_df = train_merged[(train_merged['Date'] >= train_split_date_friday) & (train_merged['Date'] < val_split_date_friday)]\n",
        "test_df = train_merged[train_merged['Date'] >= val_split_date_friday]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Training set shape after resetting index:\", train_df.shape)\n",
        "print(\"Validation set shape after resetting index:\", val_df.shape)\n",
        "print(\"Test set shape after resetting index:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGL4MaqbjYDF",
        "outputId": "bcf0cf71-e4d4-4faa-cff9-89620116b417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train split date: 2012-04-13 00:00:00\n",
            "Adjusted train split date (Friday): 2012-04-13 00:00:00\n",
            "Original validation split date: 2012-07-20 00:00:00\n",
            "Adjusted validation split date (Friday): 2012-07-20 00:00:00\n",
            "Training set shape after resetting index: (335761, 32)\n",
            "Validation set shape after resetting index: (41394, 32)\n",
            "Test set shape after resetting index: (44415, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "x_Me1Ssij6xR",
        "outputId": "78ad107c-bcc4-4f53-b6f6-18efe8c9ea31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtvani22\u001b[0m (\u001b[33mfinal-project-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Weekly_Sales\"\n",
        "DROP_COLS = [\"Date\", TARGET]\n",
        "\n",
        "X_train = train_df.drop(columns=DROP_COLS)\n",
        "X_val = val_df.drop(columns=DROP_COLS)\n",
        "X_test = test_df.drop(columns=DROP_COLS)\n",
        "\n",
        "y_train = train_df[TARGET]\n",
        "y_val = val_df[TARGET]\n",
        "y_test = test_df[TARGET]"
      ],
      "metadata": {
        "id": "wnyvGN5Aj9MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def WMAE(y_true, y_pred, weights):\n",
        "    return np.round(np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights), 5)\n",
        "\n",
        "train_weights = np.where(train_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "val_weights = np.where(val_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "test_weights = np.where(test_df[\"IsHoliday\"] == 1, 5, 1)"
      ],
      "metadata": {
        "id": "zTnw0jZekMwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Val shape:\", val_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "# Check date ranges\n",
        "print(\"\\n--- DATE RANGES ---\")\n",
        "print(\"Train: \", train_df['Date'].min(), \"->\", train_df['Date'].max())\n",
        "print(\"Val:   \", val_df['Date'].min(), \"->\", val_df['Date'].max())\n",
        "print(\"Test:  \", test_df['Date'].min(), \"->\", test_df['Date'].max())\n",
        "\n",
        "# Confirm time order integrity\n",
        "if train_df['Date'].max() < val_df['Date'].min() < test_df['Date'].min():\n",
        "    print(\"\\nâœ… Date-based split looks consistent (no obvious leakage).\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ WARNING: Date-based splits may be overlapping or unordered.\")\n",
        "\n",
        "# Check for overlapping rows (based on Date + Store + Dept as unique keys)\n",
        "train_keys = set(zip(train_df['Date'], train_df['Store'], train_df['Dept']))\n",
        "val_keys = set(zip(val_df['Date'], val_df['Store'], val_df['Dept']))\n",
        "test_keys = set(zip(test_df['Date'], test_df['Store'], test_df['Dept']))\n",
        "\n",
        "print(\"\\n--- DUPLICATE CHECKS ---\")\n",
        "print(\"Train âˆ© Val overlap:\", len(train_keys & val_keys))\n",
        "print(\"Train âˆ© Test overlap:\", len(train_keys & test_keys))\n",
        "print(\"Val âˆ© Test overlap:\", len(val_keys & test_keys))\n",
        "\n",
        "if len(train_keys & test_keys) == 0 and len(val_keys & test_keys) == 0:\n",
        "    print(\"\\nâœ… No row leakage between train/val/test.\")\n",
        "else:\n",
        "    print(\"\\nâŒ Data leakage detected between splits!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWNqaUsQKKaQ",
        "outputId": "634f1155-9600-48d2-e4df-b4ca760b87bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (335761, 32)\n",
            "Val shape: (41394, 32)\n",
            "Test shape: (44415, 32)\n",
            "\n",
            "--- DATE RANGES ---\n",
            "Train:  2010-02-05 00:00:00 -> 2012-04-06 00:00:00\n",
            "Val:    2012-04-13 00:00:00 -> 2012-07-13 00:00:00\n",
            "Test:   2012-07-20 00:00:00 -> 2012-10-26 00:00:00\n",
            "\n",
            "âœ… Date-based split looks consistent (no obvious leakage).\n",
            "\n",
            "--- DUPLICATE CHECKS ---\n",
            "Train âˆ© Val overlap: 0\n",
            "Train âˆ© Test overlap: 0\n",
            "Val âˆ© Test overlap: 0\n",
            "\n",
            "âœ… No row leakage between train/val/test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6JS-k4RkPHR",
        "outputId": "31e8c2da-12c9-40b6-abf7-4272449f9726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((335761, 30), (41394, 30), (44415, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"walmart-forecasting\",\n",
        "    entity=\"final-project-ml\",\n",
        "    name=\"lightGBM1\",\n",
        "    config={\n",
        "        \"model\": \"LightGBM\",\n",
        "        \"params\": {\n",
        "            \"num_leaves\": 70,\n",
        "            \"max_depth\": 15,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"min_child_samples\": 20,\n",
        "            \"subsample\": 0.8,\n",
        "            \"n_estimators\": 1000\n",
        "        },\n",
        "        \"metric\": \"WMAE\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Prepare model\n",
        "model = lgb.LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    num_leaves=70,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    n_estimators=1000,\n",
        "    random_state=42,\n",
        "    verbose=-1 # Suppress LightGBM output\n",
        ")\n",
        "\n",
        "# Track best score manually\n",
        "best_val_wmae = float(\"inf\")\n",
        "early_stopping_rounds = 50\n",
        "no_improve = 0\n",
        "\n",
        "for i in range(1, 51):\n",
        "    model.n_estimators = i\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_preds = model.predict(X_train)\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "    val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": i,\n",
        "        \"train_WMAE\": train_wmae,\n",
        "        \"val_WMAE\": val_wmae\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "    if val_wmae < best_val_wmae:\n",
        "        best_val_wmae = val_wmae\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    if no_improve >= early_stopping_rounds:\n",
        "        print(f\"Early stopping at epoch {i}\")\n",
        "        break\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_hmRuBT9LcZG",
        "outputId": "c262cef8-e1af-4347-e2ca-f8e61167d9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–‚â–ƒâ–…â–†â–‡â–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–†â–…â–„â–ƒâ–‚â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–‡â–…â–„â–ƒâ–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_WMAE</td><td>7737.12991</td></tr><tr><td>val_WMAE</td><td>7443.19162</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM1</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/jiomzep6' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/jiomzep6</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_112735-jiomzep6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_112858-j9oyjziu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">lightGBM1</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train WMAE = 13904.68, Val WMAE = 13727.58\n",
            "Epoch 2: Train WMAE = 12570.42, Val WMAE = 12383.81\n",
            "Epoch 3: Train WMAE = 11382.58, Val WMAE = 11175.04\n",
            "Epoch 4: Train WMAE = 10316.89, Val WMAE = 10090.68\n",
            "Epoch 5: Train WMAE = 9362.06, Val WMAE = 9114.21\n",
            "Epoch 6: Train WMAE = 8505.91, Val WMAE = 8236.79\n",
            "Epoch 7: Train WMAE = 7737.13, Val WMAE = 7443.19\n",
            "Epoch 8: Train WMAE = 7047.57, Val WMAE = 6736.73\n",
            "Epoch 9: Train WMAE = 6425.96, Val WMAE = 6100.87\n",
            "Epoch 10: Train WMAE = 5866.51, Val WMAE = 5533.35\n",
            "Epoch 11: Train WMAE = 5371.84, Val WMAE = 5018.94\n",
            "Epoch 12: Train WMAE = 4928.85, Val WMAE = 4564.49\n",
            "Epoch 13: Train WMAE = 4534.29, Val WMAE = 4155.98\n",
            "Epoch 14: Train WMAE = 4178.23, Val WMAE = 3790.81\n",
            "Epoch 15: Train WMAE = 3863.25, Val WMAE = 3468.67\n",
            "Epoch 16: Train WMAE = 3581.19, Val WMAE = 3180.51\n",
            "Epoch 17: Train WMAE = 3330.58, Val WMAE = 2921.86\n",
            "Epoch 18: Train WMAE = 3107.22, Val WMAE = 2695.53\n",
            "Epoch 19: Train WMAE = 2908.60, Val WMAE = 2491.75\n",
            "Epoch 20: Train WMAE = 2736.82, Val WMAE = 2313.32\n",
            "Epoch 21: Train WMAE = 2582.87, Val WMAE = 2155.70\n",
            "Epoch 22: Train WMAE = 2447.50, Val WMAE = 2018.44\n",
            "Epoch 23: Train WMAE = 2330.52, Val WMAE = 1897.00\n",
            "Epoch 24: Train WMAE = 2225.79, Val WMAE = 1790.94\n",
            "Epoch 25: Train WMAE = 2128.26, Val WMAE = 1698.70\n",
            "Epoch 26: Train WMAE = 2042.19, Val WMAE = 1619.18\n",
            "Epoch 27: Train WMAE = 1968.98, Val WMAE = 1553.58\n",
            "Epoch 28: Train WMAE = 1903.59, Val WMAE = 1495.20\n",
            "Epoch 29: Train WMAE = 1847.25, Val WMAE = 1444.05\n",
            "Epoch 30: Train WMAE = 1796.11, Val WMAE = 1398.99\n",
            "Epoch 31: Train WMAE = 1751.25, Val WMAE = 1361.68\n",
            "Epoch 32: Train WMAE = 1709.70, Val WMAE = 1325.59\n",
            "Epoch 33: Train WMAE = 1671.54, Val WMAE = 1293.87\n",
            "Epoch 34: Train WMAE = 1639.13, Val WMAE = 1265.75\n",
            "Epoch 35: Train WMAE = 1609.31, Val WMAE = 1240.64\n",
            "Epoch 36: Train WMAE = 1583.28, Val WMAE = 1220.25\n",
            "Epoch 37: Train WMAE = 1558.36, Val WMAE = 1199.98\n",
            "Epoch 38: Train WMAE = 1537.72, Val WMAE = 1183.25\n",
            "Epoch 39: Train WMAE = 1516.19, Val WMAE = 1170.23\n",
            "Epoch 40: Train WMAE = 1495.30, Val WMAE = 1155.98\n",
            "Epoch 41: Train WMAE = 1477.86, Val WMAE = 1143.99\n",
            "Epoch 42: Train WMAE = 1459.98, Val WMAE = 1130.45\n",
            "Epoch 43: Train WMAE = 1447.03, Val WMAE = 1124.29\n",
            "Epoch 44: Train WMAE = 1436.16, Val WMAE = 1116.03\n",
            "Epoch 45: Train WMAE = 1425.10, Val WMAE = 1112.71\n",
            "Epoch 46: Train WMAE = 1411.70, Val WMAE = 1105.62\n",
            "Epoch 47: Train WMAE = 1399.55, Val WMAE = 1096.77\n",
            "Epoch 48: Train WMAE = 1387.10, Val WMAE = 1088.09\n",
            "Epoch 49: Train WMAE = 1377.33, Val WMAE = 1081.13\n",
            "Epoch 50: Train WMAE = 1367.60, Val WMAE = 1074.63\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>1367.60498</td></tr><tr><td>val_WMAE</td><td>1074.63362</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM1</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_112858-j9oyjziu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New Experiment**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jmyJK6j1VBBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YoY Sales Feature (same week, previous year) ---\n",
        "train_merged['YoY_Sales'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52).fillna(0)\n",
        "\n",
        "# --- Fourier Features for Weekly Seasonality ---\n",
        "train_merged['sin_week'] = np.sin(2 * np.pi * train_merged['Week'] / 52)\n",
        "train_merged['cos_week'] = np.cos(2 * np.pi * train_merged['Week'] / 52)\n",
        "\n",
        "test_merged['sin_week'] = np.sin(2 * np.pi * test_merged['Week'] / 52)\n",
        "test_merged['cos_week'] = np.cos(2 * np.pi * test_merged['Week'] / 52)"
      ],
      "metadata": {
        "id": "BqpBJJ8oSUyo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the split points (e.g., 80% for training, 10% for validation, 10% for testing)\n",
        "train_size = int(len(train_merged) * 0.8)\n",
        "val_size = int(len(train_merged) * 0.1)\n",
        "test_size = len(train_merged) - train_size - val_size\n",
        "\n",
        "# Find the dates at the calculated split points\n",
        "train_split_date = train_merged.iloc[train_size]['Date']\n",
        "val_split_date = train_merged.iloc[train_size + val_size]['Date']\n",
        "\n",
        "# Find the closest Friday to the split dates within the dataset\n",
        "# We can iterate through the dates in the merged dataframe to find the exact Friday\n",
        "def find_closest_friday(date, df_dates):\n",
        "    closest_friday = None\n",
        "    for d in df_dates:\n",
        "        if d >= date and d.dayofweek == 4: # Friday is dayofweek 4\n",
        "            closest_friday = d\n",
        "            break\n",
        "    return closest_friday\n",
        "\n",
        "all_dates = train_merged['Date'].unique()\n",
        "train_split_date_friday = find_closest_friday(train_split_date, all_dates)\n",
        "val_split_date_friday = find_closest_friday(val_split_date, all_dates)\n",
        "\n",
        "print(f\"Original train split date: {train_split_date}\")\n",
        "print(f\"Adjusted train split date (Friday): {train_split_date_friday}\")\n",
        "print(f\"Original validation split date: {val_split_date}\")\n",
        "print(f\"Adjusted validation split date (Friday): {val_split_date_friday}\")\n",
        "\n",
        "# Split the data based on the adjusted Friday dates\n",
        "train_df = train_merged[train_merged['Date'] < train_split_date_friday]\n",
        "val_df = train_merged[(train_merged['Date'] >= train_split_date_friday) & (train_merged['Date'] < val_split_date_friday)]\n",
        "test_df = train_merged[train_merged['Date'] >= val_split_date_friday]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Training set shape after resetting index:\", train_df.shape)\n",
        "print(\"Validation set shape after resetting index:\", val_df.shape)\n",
        "print(\"Test set shape after resetting index:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1Q7hf7XXfNz",
        "outputId": "1790574c-c5a7-4771-d0a5-363bc45730ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train split date: 2012-04-13 00:00:00\n",
            "Adjusted train split date (Friday): 2012-04-13 00:00:00\n",
            "Original validation split date: 2012-07-20 00:00:00\n",
            "Adjusted validation split date (Friday): 2012-07-20 00:00:00\n",
            "Training set shape after resetting index: (335761, 35)\n",
            "Validation set shape after resetting index: (41394, 35)\n",
            "Test set shape after resetting index: (44415, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Weekly_Sales\"\n",
        "DROP_COLS = [\"Date\", TARGET]\n",
        "\n",
        "X_train = train_df.drop(columns=DROP_COLS)\n",
        "X_val = val_df.drop(columns=DROP_COLS)\n",
        "X_test = test_df.drop(columns=DROP_COLS)\n",
        "\n",
        "y_train = train_df[TARGET]\n",
        "y_val = val_df[TARGET]\n",
        "y_test = test_df[TARGET]"
      ],
      "metadata": {
        "id": "cCQ1zVrNXoXS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import wandb\n",
        "\n",
        "# --- Define Hyperparameter Grid ---\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 70],\n",
        "    'max_depth': [10, 15],\n",
        "    'learning_rate': [0.05],\n",
        "    'min_child_samples': [20],\n",
        "    'subsample': [0.8],\n",
        "    'feature_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_freq': [1, 5]\n",
        "}\n",
        "\n",
        "# --- Iterate Over All Combinations ---\n",
        "for combo in itertools.product(*param_grid.values()):\n",
        "    params = dict(zip(param_grid.keys(), combo))\n",
        "\n",
        "    # Initialize wandb run\n",
        "    wandb.init(\n",
        "        project=\"walmart-forecasting\",\n",
        "        name=f\"lightGBM adv | {params}\",\n",
        "        config=params\n",
        "    )\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        objective=\"regression\",\n",
        "        n_estimators=1000,\n",
        "        random_state=42,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    best_val_wmae = float(\"inf\")\n",
        "    early_stopping_rounds = 50\n",
        "    rounds_no_improve = 0\n",
        "\n",
        "    for i in range(1, 51):\n",
        "        model.n_estimators = i\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        train_preds = model.predict(X_train)\n",
        "        val_preds = model.predict(X_val)\n",
        "\n",
        "        train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "        val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": i,\n",
        "            \"train_WMAE\": train_wmae,\n",
        "            \"val_WMAE\": val_wmae\n",
        "        })\n",
        "\n",
        "        print(f\"[{params}] Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "        if val_wmae < best_val_wmae:\n",
        "            best_val_wmae = val_wmae\n",
        "            rounds_no_improve = 0\n",
        "        else:\n",
        "            rounds_no_improve += 1\n",
        "\n",
        "        if rounds_no_improve >= early_stopping_rounds:\n",
        "            print(f\"Early stopping for {params} at epoch {i}\")\n",
        "            break\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mIbTCjxNXt2R",
        "outputId": "b3c9a7f3-77f7-4fbf-ff28-9438e3a15b57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–‚â–ƒâ–„â–…â–…â–†â–‡â–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–‚â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_WMAE</td><td>10054.94894</td></tr><tr><td>val_WMAE</td><td>9710.89563</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/972331bp' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/972331bp</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123334-972331bp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_123405-dnu36rwq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14672.21, Val WMAE = 14472.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13982.61, Val WMAE = 13760.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13323.21, Val WMAE = 13084.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12698.48, Val WMAE = 12447.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12107.39, Val WMAE = 11841.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11553.42, Val WMAE = 11261.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11031.09, Val WMAE = 10716.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10525.46, Val WMAE = 10199.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10054.95, Val WMAE = 9710.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.78, Val WMAE = 9238.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.22, Val WMAE = 8793.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8784.80, Val WMAE = 8369.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8394.59, Val WMAE = 7972.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8026.59, Val WMAE = 7595.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7684.52, Val WMAE = 7231.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7351.87, Val WMAE = 6887.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7037.63, Val WMAE = 6563.44\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6744.97, Val WMAE = 6252.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6465.65, Val WMAE = 5959.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6212.20, Val WMAE = 5690.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5961.66, Val WMAE = 5432.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5724.19, Val WMAE = 5188.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5498.82, Val WMAE = 4956.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5292.81, Val WMAE = 4733.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5100.53, Val WMAE = 4528.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4908.30, Val WMAE = 4329.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4726.61, Val WMAE = 4144.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4556.19, Val WMAE = 3971.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4398.09, Val WMAE = 3808.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4248.43, Val WMAE = 3650.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4103.43, Val WMAE = 3502.29\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3966.03, Val WMAE = 3361.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3837.37, Val WMAE = 3231.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3718.16, Val WMAE = 3108.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3604.33, Val WMAE = 2991.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3497.36, Val WMAE = 2880.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3395.24, Val WMAE = 2779.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3303.22, Val WMAE = 2680.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3213.03, Val WMAE = 2590.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3126.61, Val WMAE = 2506.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3046.60, Val WMAE = 2425.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2969.51, Val WMAE = 2349.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2898.73, Val WMAE = 2278.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2831.86, Val WMAE = 2212.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2769.24, Val WMAE = 2152.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2710.46, Val WMAE = 2095.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2654.37, Val WMAE = 2042.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2602.40, Val WMAE = 1993.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2553.51, Val WMAE = 1944.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2511.53, Val WMAE = 1900.70\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2511.52568</td></tr><tr><td>val_WMAE</td><td>1900.70206</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123405-dnu36rwq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_123726-0ntms0an</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14672.21, Val WMAE = 14472.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13983.66, Val WMAE = 13759.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13324.48, Val WMAE = 13089.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12698.94, Val WMAE = 12452.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12109.24, Val WMAE = 11843.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11561.91, Val WMAE = 11263.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11037.94, Val WMAE = 10718.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10536.70, Val WMAE = 10201.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10065.97, Val WMAE = 9713.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9619.47, Val WMAE = 9241.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9199.57, Val WMAE = 8798.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8794.92, Val WMAE = 8376.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8402.98, Val WMAE = 7977.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8031.81, Val WMAE = 7598.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7689.84, Val WMAE = 7235.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7359.57, Val WMAE = 6894.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7048.80, Val WMAE = 6571.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6757.30, Val WMAE = 6261.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6475.70, Val WMAE = 5966.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6224.89, Val WMAE = 5696.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5974.58, Val WMAE = 5439.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5734.47, Val WMAE = 5192.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5507.06, Val WMAE = 4961.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5300.65, Val WMAE = 4740.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5107.03, Val WMAE = 4536.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4915.75, Val WMAE = 4336.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4736.75, Val WMAE = 4154.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4565.51, Val WMAE = 3981.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4404.01, Val WMAE = 3814.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4255.74, Val WMAE = 3653.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4112.33, Val WMAE = 3504.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3976.91, Val WMAE = 3362.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3849.25, Val WMAE = 3232.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3728.82, Val WMAE = 3105.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3617.34, Val WMAE = 2987.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3510.41, Val WMAE = 2879.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3407.92, Val WMAE = 2776.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3312.67, Val WMAE = 2676.63\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3219.18, Val WMAE = 2587.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3132.51, Val WMAE = 2501.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3051.70, Val WMAE = 2422.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2978.17, Val WMAE = 2347.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2906.69, Val WMAE = 2276.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2839.62, Val WMAE = 2211.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2777.64, Val WMAE = 2149.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2719.08, Val WMAE = 2091.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2661.76, Val WMAE = 2038.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2608.29, Val WMAE = 1990.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2559.77, Val WMAE = 1943.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2515.52, Val WMAE = 1898.40\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2515.52185</td></tr><tr><td>val_WMAE</td><td>1898.40246</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123726-0ntms0an/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124105-1e9pdx1h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14671.59, Val WMAE = 14471.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13982.55, Val WMAE = 13759.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13322.87, Val WMAE = 13083.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12696.80, Val WMAE = 12446.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12103.74, Val WMAE = 11841.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11549.92, Val WMAE = 11262.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11028.03, Val WMAE = 10717.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10523.66, Val WMAE = 10198.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10054.16, Val WMAE = 9710.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.78, Val WMAE = 9237.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.10, Val WMAE = 8794.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8783.95, Val WMAE = 8372.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8392.80, Val WMAE = 7973.95\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8026.58, Val WMAE = 7591.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7684.31, Val WMAE = 7228.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7352.99, Val WMAE = 6887.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7039.35, Val WMAE = 6562.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6748.09, Val WMAE = 6249.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6466.21, Val WMAE = 5958.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6213.55, Val WMAE = 5689.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5965.29, Val WMAE = 5431.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5727.22, Val WMAE = 5184.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5502.15, Val WMAE = 4951.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5295.76, Val WMAE = 4732.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5103.52, Val WMAE = 4529.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4912.49, Val WMAE = 4330.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4728.46, Val WMAE = 4145.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4559.03, Val WMAE = 3972.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4399.39, Val WMAE = 3809.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4252.13, Val WMAE = 3655.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4107.21, Val WMAE = 3507.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3969.86, Val WMAE = 3365.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3842.40, Val WMAE = 3236.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3722.70, Val WMAE = 3111.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3611.06, Val WMAE = 2986.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3504.82, Val WMAE = 2876.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3401.64, Val WMAE = 2774.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3309.60, Val WMAE = 2676.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3217.56, Val WMAE = 2587.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3130.69, Val WMAE = 2502.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3049.55, Val WMAE = 2422.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2972.17, Val WMAE = 2345.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2900.57, Val WMAE = 2275.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2833.22, Val WMAE = 2209.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2771.87, Val WMAE = 2149.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2713.66, Val WMAE = 2091.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2657.19, Val WMAE = 2037.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2605.32, Val WMAE = 1989.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2556.24, Val WMAE = 1942.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2513.13, Val WMAE = 1898.54\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2513.1291</td></tr><tr><td>val_WMAE</td><td>1898.53506</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124105-1e9pdx1h/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124442-kooa2xxw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14671.59, Val WMAE = 14471.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13983.53, Val WMAE = 13759.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13322.23, Val WMAE = 13084.51\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12695.89, Val WMAE = 12446.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12101.62, Val WMAE = 11840.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11547.89, Val WMAE = 11261.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11024.92, Val WMAE = 10716.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10519.85, Val WMAE = 10198.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10048.96, Val WMAE = 9710.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9599.61, Val WMAE = 9236.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9180.95, Val WMAE = 8794.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8776.76, Val WMAE = 8371.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8389.32, Val WMAE = 7970.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8020.48, Val WMAE = 7595.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7680.31, Val WMAE = 7235.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7352.83, Val WMAE = 6890.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7040.01, Val WMAE = 6563.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6748.71, Val WMAE = 6253.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6468.41, Val WMAE = 5962.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6217.72, Val WMAE = 5692.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5965.32, Val WMAE = 5434.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5726.35, Val WMAE = 5189.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5501.37, Val WMAE = 4958.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5296.05, Val WMAE = 4737.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5102.61, Val WMAE = 4534.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4912.03, Val WMAE = 4335.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4733.84, Val WMAE = 4153.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4563.04, Val WMAE = 3976.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4399.31, Val WMAE = 3812.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4250.47, Val WMAE = 3653.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4106.63, Val WMAE = 3509.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3968.95, Val WMAE = 3367.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3840.39, Val WMAE = 3238.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3723.41, Val WMAE = 3116.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3611.98, Val WMAE = 2996.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3505.47, Val WMAE = 2884.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3403.27, Val WMAE = 2782.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3310.21, Val WMAE = 2681.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3218.18, Val WMAE = 2592.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3131.03, Val WMAE = 2506.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3048.59, Val WMAE = 2425.29\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2972.96, Val WMAE = 2347.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2901.54, Val WMAE = 2276.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2833.63, Val WMAE = 2211.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2772.52, Val WMAE = 2151.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2713.77, Val WMAE = 2093.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2658.68, Val WMAE = 2040.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2605.94, Val WMAE = 1992.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2554.71, Val WMAE = 1946.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2511.30, Val WMAE = 1901.53\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2511.3015</td></tr><tr><td>val_WMAE</td><td>1901.53248</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124442-kooa2xxw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124841-nyrbluhl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14670.09, Val WMAE = 14472.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13981.81, Val WMAE = 13757.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13318.68, Val WMAE = 13083.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12696.31, Val WMAE = 12446.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12101.59, Val WMAE = 11839.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11551.91, Val WMAE = 11260.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11030.10, Val WMAE = 10715.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10522.95, Val WMAE = 10198.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10051.91, Val WMAE = 9710.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.62, Val WMAE = 9239.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.47, Val WMAE = 8795.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8783.31, Val WMAE = 8373.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8391.48, Val WMAE = 7975.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8025.94, Val WMAE = 7597.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7686.19, Val WMAE = 7233.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7353.27, Val WMAE = 6890.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7040.27, Val WMAE = 6561.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6750.44, Val WMAE = 6253.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6467.65, Val WMAE = 5961.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6214.09, Val WMAE = 5692.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5964.30, Val WMAE = 5431.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5724.86, Val WMAE = 5186.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5502.53, Val WMAE = 4953.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5296.34, Val WMAE = 4734.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5105.23, Val WMAE = 4528.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4914.12, Val WMAE = 4328.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4734.06, Val WMAE = 4146.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4562.63, Val WMAE = 3971.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4402.12, Val WMAE = 3810.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4254.77, Val WMAE = 3652.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4111.23, Val WMAE = 3505.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3973.60, Val WMAE = 3366.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3846.07, Val WMAE = 3235.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3725.05, Val WMAE = 3112.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3612.61, Val WMAE = 2993.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3506.69, Val WMAE = 2881.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3403.47, Val WMAE = 2780.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3309.33, Val WMAE = 2682.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3215.77, Val WMAE = 2588.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3127.18, Val WMAE = 2501.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3046.34, Val WMAE = 2422.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2970.44, Val WMAE = 2347.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2898.11, Val WMAE = 2276.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2829.73, Val WMAE = 2210.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2768.53, Val WMAE = 2151.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2709.73, Val WMAE = 2093.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2654.32, Val WMAE = 2039.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2601.15, Val WMAE = 1990.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2551.94, Val WMAE = 1943.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2508.44, Val WMAE = 1899.41\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2508.44183</td></tr><tr><td>val_WMAE</td><td>1899.40623</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124841-nyrbluhl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_125217-i5dooxcz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14670.09, Val WMAE = 14472.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13981.81, Val WMAE = 13757.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13318.68, Val WMAE = 13083.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12696.31, Val WMAE = 12446.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12101.59, Val WMAE = 11839.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11551.91, Val WMAE = 11260.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11030.10, Val WMAE = 10715.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10522.95, Val WMAE = 10198.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10051.91, Val WMAE = 9710.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9606.62, Val WMAE = 9239.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9188.47, Val WMAE = 8795.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8783.31, Val WMAE = 8373.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8391.48, Val WMAE = 7975.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8025.94, Val WMAE = 7597.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7686.19, Val WMAE = 7233.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7353.27, Val WMAE = 6890.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7040.27, Val WMAE = 6561.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6750.44, Val WMAE = 6253.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6467.65, Val WMAE = 5961.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6214.09, Val WMAE = 5692.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5964.30, Val WMAE = 5431.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5724.86, Val WMAE = 5186.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5502.53, Val WMAE = 4953.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5296.34, Val WMAE = 4734.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5105.23, Val WMAE = 4528.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4914.12, Val WMAE = 4328.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4734.06, Val WMAE = 4146.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4562.63, Val WMAE = 3971.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4402.12, Val WMAE = 3810.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4254.77, Val WMAE = 3652.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4111.23, Val WMAE = 3505.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3973.60, Val WMAE = 3366.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3846.07, Val WMAE = 3235.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3725.05, Val WMAE = 3112.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3612.61, Val WMAE = 2993.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3506.69, Val WMAE = 2881.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3403.47, Val WMAE = 2780.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3309.33, Val WMAE = 2682.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3215.77, Val WMAE = 2588.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3127.18, Val WMAE = 2501.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3046.34, Val WMAE = 2422.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2970.44, Val WMAE = 2347.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2898.11, Val WMAE = 2276.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2829.73, Val WMAE = 2210.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2768.53, Val WMAE = 2151.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2709.73, Val WMAE = 2093.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2654.32, Val WMAE = 2039.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2601.15, Val WMAE = 1990.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2551.94, Val WMAE = 1943.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2508.44, Val WMAE = 1899.41\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2508.44183</td></tr><tr><td>val_WMAE</td><td>1899.40623</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_125217-i5dooxcz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_125616-650eeo7y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14658.17, Val WMAE = 14478.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13958.90, Val WMAE = 13770.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13296.46, Val WMAE = 13095.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12672.80, Val WMAE = 12456.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12074.58, Val WMAE = 11852.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11513.12, Val WMAE = 11276.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10993.46, Val WMAE = 10731.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10490.01, Val WMAE = 10210.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10010.03, Val WMAE = 9717.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9552.88, Val WMAE = 9247.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.63, Val WMAE = 8804.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8730.47, Val WMAE = 8381.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8347.48, Val WMAE = 7981.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7983.76, Val WMAE = 7602.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7642.29, Val WMAE = 7238.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7315.19, Val WMAE = 6895.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7007.14, Val WMAE = 6570.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6714.75, Val WMAE = 6262.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6433.59, Val WMAE = 5967.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6171.73, Val WMAE = 5690.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5924.51, Val WMAE = 5426.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5688.45, Val WMAE = 5182.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5464.44, Val WMAE = 4947.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5258.94, Val WMAE = 4729.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5065.02, Val WMAE = 4521.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4879.18, Val WMAE = 4320.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4700.02, Val WMAE = 4133.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4531.32, Val WMAE = 3958.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4370.87, Val WMAE = 3791.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4217.34, Val WMAE = 3633.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4076.63, Val WMAE = 3486.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3940.52, Val WMAE = 3344.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3811.43, Val WMAE = 3213.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3690.51, Val WMAE = 3088.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3573.82, Val WMAE = 2969.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3468.69, Val WMAE = 2855.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3367.77, Val WMAE = 2752.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3269.90, Val WMAE = 2654.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3178.10, Val WMAE = 2563.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3088.95, Val WMAE = 2476.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3011.13, Val WMAE = 2397.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2934.32, Val WMAE = 2320.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2862.49, Val WMAE = 2251.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2794.19, Val WMAE = 2184.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2729.91, Val WMAE = 2120.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2671.90, Val WMAE = 2063.63\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2616.46, Val WMAE = 2010.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2563.94, Val WMAE = 1960.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2514.17, Val WMAE = 1913.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2469.01, Val WMAE = 1869.15\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2469.01018</td></tr><tr><td>val_WMAE</td><td>1869.14746</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_125616-650eeo7y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130030-fnnkz2m9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14658.17, Val WMAE = 14478.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13955.60, Val WMAE = 13770.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13298.72, Val WMAE = 13095.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12671.40, Val WMAE = 12455.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12078.10, Val WMAE = 11850.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11516.17, Val WMAE = 11275.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10995.16, Val WMAE = 10727.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10489.22, Val WMAE = 10206.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10013.01, Val WMAE = 9713.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9559.55, Val WMAE = 9246.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9141.41, Val WMAE = 8803.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8731.95, Val WMAE = 8381.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8345.42, Val WMAE = 7979.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7985.15, Val WMAE = 7598.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7643.29, Val WMAE = 7232.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7317.58, Val WMAE = 6889.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7006.25, Val WMAE = 6560.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6714.92, Val WMAE = 6249.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6437.36, Val WMAE = 5957.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6178.23, Val WMAE = 5683.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5928.39, Val WMAE = 5424.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5177.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5468.34, Val WMAE = 4943.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5260.21, Val WMAE = 4721.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5063.97, Val WMAE = 4513.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4875.20, Val WMAE = 4318.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4696.71, Val WMAE = 4132.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4528.65, Val WMAE = 3955.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4368.80, Val WMAE = 3788.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4216.64, Val WMAE = 3631.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4074.79, Val WMAE = 3482.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3940.46, Val WMAE = 3341.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3813.32, Val WMAE = 3210.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3692.87, Val WMAE = 3087.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3576.48, Val WMAE = 2966.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3465.59, Val WMAE = 2853.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3364.06, Val WMAE = 2750.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3268.49, Val WMAE = 2653.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3176.08, Val WMAE = 2561.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3089.64, Val WMAE = 2473.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3008.89, Val WMAE = 2392.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2933.71, Val WMAE = 2316.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2863.66, Val WMAE = 2247.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2796.44, Val WMAE = 2180.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2733.05, Val WMAE = 2117.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2674.28, Val WMAE = 2060.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2618.66, Val WMAE = 2006.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2563.88, Val WMAE = 1956.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2514.74, Val WMAE = 1909.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2468.37, Val WMAE = 1866.74\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.37435</td></tr><tr><td>val_WMAE</td><td>1866.74286</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130030-fnnkz2m9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130515-6id0nzdf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14660.66, Val WMAE = 14475.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13958.61, Val WMAE = 13768.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13298.75, Val WMAE = 13096.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.93, Val WMAE = 12455.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12081.36, Val WMAE = 11849.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11519.56, Val WMAE = 11274.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10998.20, Val WMAE = 10728.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10488.34, Val WMAE = 10207.47\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10008.43, Val WMAE = 9714.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9553.80, Val WMAE = 9247.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.23, Val WMAE = 8801.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8727.44, Val WMAE = 8378.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8341.21, Val WMAE = 7978.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7978.31, Val WMAE = 7600.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7634.00, Val WMAE = 7234.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7309.49, Val WMAE = 6889.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7000.54, Val WMAE = 6569.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6706.90, Val WMAE = 6260.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6431.80, Val WMAE = 5967.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6170.79, Val WMAE = 5691.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5923.22, Val WMAE = 5429.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5687.13, Val WMAE = 5182.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5462.88, Val WMAE = 4948.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5255.07, Val WMAE = 4726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5059.63, Val WMAE = 4520.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4872.75, Val WMAE = 4321.51\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4694.25, Val WMAE = 4136.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4527.77, Val WMAE = 3962.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4367.77, Val WMAE = 3795.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4215.38, Val WMAE = 3635.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4072.13, Val WMAE = 3486.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3938.68, Val WMAE = 3345.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3807.99, Val WMAE = 3214.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3687.37, Val WMAE = 3090.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3573.10, Val WMAE = 2972.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3468.08, Val WMAE = 2859.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3365.11, Val WMAE = 2754.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3268.48, Val WMAE = 2657.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3176.97, Val WMAE = 2565.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3090.47, Val WMAE = 2479.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3010.86, Val WMAE = 2399.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2935.63, Val WMAE = 2322.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2861.77, Val WMAE = 2251.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2794.20, Val WMAE = 2182.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2728.37, Val WMAE = 2119.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2668.91, Val WMAE = 2063.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2613.76, Val WMAE = 2010.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2561.80, Val WMAE = 1960.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2512.73, Val WMAE = 1914.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2467.01, Val WMAE = 1869.56\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2467.00893</td></tr><tr><td>val_WMAE</td><td>1869.56022</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130515-6id0nzdf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130941-x3x5ccy0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14660.66, Val WMAE = 14475.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13961.92, Val WMAE = 13765.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13299.76, Val WMAE = 13093.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12676.06, Val WMAE = 12456.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12082.03, Val WMAE = 11848.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11520.10, Val WMAE = 11271.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10997.17, Val WMAE = 10725.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10488.74, Val WMAE = 10206.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10008.78, Val WMAE = 9716.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9557.33, Val WMAE = 9244.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9137.73, Val WMAE = 8799.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8726.91, Val WMAE = 8376.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8343.59, Val WMAE = 7979.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7979.35, Val WMAE = 7599.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7639.26, Val WMAE = 7239.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7313.78, Val WMAE = 6895.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7004.19, Val WMAE = 6572.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6714.14, Val WMAE = 6262.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6439.64, Val WMAE = 5970.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6177.64, Val WMAE = 5692.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5928.11, Val WMAE = 5430.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5691.57, Val WMAE = 5183.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5470.89, Val WMAE = 4950.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5263.18, Val WMAE = 4729.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5067.89, Val WMAE = 4523.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4877.63, Val WMAE = 4323.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4702.96, Val WMAE = 4135.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4534.05, Val WMAE = 3959.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4374.79, Val WMAE = 3794.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4222.51, Val WMAE = 3636.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4080.11, Val WMAE = 3488.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3944.60, Val WMAE = 3347.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3816.10, Val WMAE = 3216.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3695.75, Val WMAE = 3090.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3583.38, Val WMAE = 2971.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3478.36, Val WMAE = 2859.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3373.44, Val WMAE = 2757.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3273.09, Val WMAE = 2659.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3181.17, Val WMAE = 2567.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3095.58, Val WMAE = 2481.44\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3014.35, Val WMAE = 2402.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2936.94, Val WMAE = 2325.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2866.13, Val WMAE = 2254.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2798.38, Val WMAE = 2190.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2733.64, Val WMAE = 2126.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2674.77, Val WMAE = 2069.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2620.08, Val WMAE = 2012.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2566.19, Val WMAE = 1960.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2517.33, Val WMAE = 1914.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2472.05, Val WMAE = 1872.79\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2472.04602</td></tr><tr><td>val_WMAE</td><td>1872.78625</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130941-x3x5ccy0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_131421-7a12rrlm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14661.70, Val WMAE = 14474.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13960.41, Val WMAE = 13765.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13303.25, Val WMAE = 13091.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.37, Val WMAE = 12453.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12080.51, Val WMAE = 11845.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11514.42, Val WMAE = 11270.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10994.03, Val WMAE = 10726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10486.18, Val WMAE = 10208.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10005.06, Val WMAE = 9715.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9553.68, Val WMAE = 9249.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.26, Val WMAE = 8805.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8728.52, Val WMAE = 8381.35\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8347.24, Val WMAE = 7983.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7979.55, Val WMAE = 7602.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7635.82, Val WMAE = 7237.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7308.41, Val WMAE = 6891.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7000.19, Val WMAE = 6565.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6711.60, Val WMAE = 6256.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6433.36, Val WMAE = 5963.88\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6172.56, Val WMAE = 5686.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5926.89, Val WMAE = 5427.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5179.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5468.06, Val WMAE = 4946.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5261.71, Val WMAE = 4723.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5066.25, Val WMAE = 4518.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4877.69, Val WMAE = 4320.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4699.42, Val WMAE = 4137.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4531.13, Val WMAE = 3961.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4371.26, Val WMAE = 3796.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4219.60, Val WMAE = 3638.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4076.49, Val WMAE = 3490.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3940.28, Val WMAE = 3349.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3811.58, Val WMAE = 3218.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3693.12, Val WMAE = 3095.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3580.77, Val WMAE = 2974.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3470.44, Val WMAE = 2862.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3368.37, Val WMAE = 2758.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3270.77, Val WMAE = 2660.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3181.24, Val WMAE = 2568.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3094.89, Val WMAE = 2482.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3014.11, Val WMAE = 2401.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2935.01, Val WMAE = 2323.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2863.68, Val WMAE = 2252.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2795.76, Val WMAE = 2185.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2732.13, Val WMAE = 2123.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2673.35, Val WMAE = 2065.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2616.55, Val WMAE = 2010.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2563.71, Val WMAE = 1958.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2514.45, Val WMAE = 1911.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2468.55, Val WMAE = 1867.11\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.55389</td></tr><tr><td>val_WMAE</td><td>1867.10872</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_131421-7a12rrlm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_131829-rc1e2a7i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14661.70, Val WMAE = 14474.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13960.41, Val WMAE = 13765.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13303.25, Val WMAE = 13091.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12675.37, Val WMAE = 12453.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12080.51, Val WMAE = 11845.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11514.42, Val WMAE = 11270.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10994.03, Val WMAE = 10726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10486.18, Val WMAE = 10208.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10005.06, Val WMAE = 9715.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9553.68, Val WMAE = 9249.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9136.26, Val WMAE = 8805.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8728.52, Val WMAE = 8381.35\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8347.24, Val WMAE = 7983.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7979.55, Val WMAE = 7602.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7635.82, Val WMAE = 7237.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7308.41, Val WMAE = 6891.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7000.19, Val WMAE = 6565.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6711.60, Val WMAE = 6256.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6433.36, Val WMAE = 5963.88\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6172.56, Val WMAE = 5686.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5926.89, Val WMAE = 5427.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5179.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5468.06, Val WMAE = 4946.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5261.71, Val WMAE = 4723.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5066.25, Val WMAE = 4518.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4877.69, Val WMAE = 4320.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4699.42, Val WMAE = 4137.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4531.13, Val WMAE = 3961.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4371.26, Val WMAE = 3796.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4219.60, Val WMAE = 3638.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4076.49, Val WMAE = 3490.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3940.28, Val WMAE = 3349.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3811.58, Val WMAE = 3218.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3693.12, Val WMAE = 3095.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3580.77, Val WMAE = 2974.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3470.44, Val WMAE = 2862.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3368.37, Val WMAE = 2758.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3270.77, Val WMAE = 2660.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3181.24, Val WMAE = 2568.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3094.89, Val WMAE = 2482.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3014.11, Val WMAE = 2401.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2935.01, Val WMAE = 2323.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2863.68, Val WMAE = 2252.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2795.76, Val WMAE = 2185.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2732.13, Val WMAE = 2123.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2673.35, Val WMAE = 2065.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2616.55, Val WMAE = 2010.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2563.71, Val WMAE = 1958.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2514.45, Val WMAE = 1911.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2468.55, Val WMAE = 1867.11\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.55389</td></tr><tr><td>val_WMAE</td><td>1867.10872</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_131829-rc1e2a7i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_132233-6uw83lvq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14658.38, Val WMAE = 14479.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13965.27, Val WMAE = 13771.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13302.66, Val WMAE = 13096.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.45, Val WMAE = 12456.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12081.50, Val WMAE = 11851.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11518.57, Val WMAE = 11276.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10982.40, Val WMAE = 10730.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10475.42, Val WMAE = 10213.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 9999.56, Val WMAE = 9721.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9548.32, Val WMAE = 9249.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9123.83, Val WMAE = 8807.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8722.04, Val WMAE = 8385.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8335.28, Val WMAE = 7986.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7976.10, Val WMAE = 7604.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7628.50, Val WMAE = 7239.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7298.51, Val WMAE = 6895.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 6986.12, Val WMAE = 6567.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6697.69, Val WMAE = 6260.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6423.04, Val WMAE = 5969.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6160.69, Val WMAE = 5693.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5914.50, Val WMAE = 5431.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5680.80, Val WMAE = 5186.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5457.90, Val WMAE = 4952.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5251.10, Val WMAE = 4731.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5052.86, Val WMAE = 4524.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4863.99, Val WMAE = 4325.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4686.07, Val WMAE = 4140.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4518.38, Val WMAE = 3962.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4361.36, Val WMAE = 3795.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4209.57, Val WMAE = 3635.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4066.81, Val WMAE = 3485.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3932.57, Val WMAE = 3343.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3804.26, Val WMAE = 3210.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3683.94, Val WMAE = 3087.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3569.77, Val WMAE = 2969.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3462.25, Val WMAE = 2860.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3359.90, Val WMAE = 2755.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3262.31, Val WMAE = 2656.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3172.72, Val WMAE = 2563.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3085.07, Val WMAE = 2474.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3003.97, Val WMAE = 2392.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2925.34, Val WMAE = 2314.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2854.16, Val WMAE = 2242.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2786.58, Val WMAE = 2174.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2723.25, Val WMAE = 2112.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2663.97, Val WMAE = 2055.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2608.25, Val WMAE = 2000.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2554.78, Val WMAE = 1949.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2504.06, Val WMAE = 1902.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2457.09, Val WMAE = 1858.56\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2457.09293</td></tr><tr><td>val_WMAE</td><td>1858.56373</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_132233-6uw83lvq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_132642-q7dsxe4q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14658.38, Val WMAE = 14479.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13961.74, Val WMAE = 13772.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13299.84, Val WMAE = 13099.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12671.59, Val WMAE = 12460.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12079.45, Val WMAE = 11852.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11520.72, Val WMAE = 11277.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10989.30, Val WMAE = 10728.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10483.66, Val WMAE = 10209.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10005.70, Val WMAE = 9719.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9553.37, Val WMAE = 9253.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9124.44, Val WMAE = 8808.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8715.10, Val WMAE = 8386.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8328.54, Val WMAE = 7985.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7968.92, Val WMAE = 7609.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7626.74, Val WMAE = 7244.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7300.35, Val WMAE = 6897.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 6988.23, Val WMAE = 6569.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6697.34, Val WMAE = 6263.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6423.56, Val WMAE = 5971.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6165.74, Val WMAE = 5696.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5919.92, Val WMAE = 5433.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5684.23, Val WMAE = 5185.47\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5463.35, Val WMAE = 4953.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5252.55, Val WMAE = 4731.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5055.20, Val WMAE = 4523.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4868.11, Val WMAE = 4325.11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-1892232168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         return self._Booster.predict(  # type: ignore[union-attr]\n\u001b[0m\u001b[1;32m   1037\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4746\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4748\u001b[0;31m         return predictor.predict(\n\u001b[0m\u001b[1;32m   4749\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4750\u001b[0m             \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             )\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             preds, nrow = self.__pred_for_np2d(\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             return self.__inner_predict_np2d(\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mout_num_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         _safe_call(\n\u001b[0;32m-> 1291\u001b[0;31m             _LIB.LGBM_BoosterPredictForMat(\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"walmart-forecasting\",\n",
        "    entity=\"final-project-ml\",\n",
        "    name=\"lightGBM Last\",\n",
        "    config={\n",
        "        \"model\": \"LightGBM\",\n",
        "        \"params\": {\n",
        "            \"num_leaves\": 70,\n",
        "            \"max_depth\": 15,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"min_child_samples\": 20,\n",
        "            \"subsample\": 0.8,\n",
        "            \"feature_fraction\": 0.9,\n",
        "            \"bagging_fraction\": 0.9,\n",
        "            \"bagging_freq\": 5,\n",
        "            \"n_estimators\": 1000\n",
        "        },\n",
        "        \"metric\": \"WMAE\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Prepare model\n",
        "model = lgb.LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    num_leaves=70,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    feature_fraction=0.9,\n",
        "    bagging_fraction=0.9,\n",
        "    bagging_freq=5,\n",
        "    n_estimators=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Track best score manually\n",
        "best_val_wmae = float(\"inf\")\n",
        "early_stopping_rounds = 50\n",
        "no_improve = 0\n",
        "\n",
        "for i in range(1, 51):\n",
        "    model.n_estimators = i\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_preds = model.predict(X_train)\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "    val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": i,\n",
        "        \"train_WMAE\": train_wmae,\n",
        "        \"val_WMAE\": val_wmae\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "    if val_wmae < best_val_wmae:\n",
        "        best_val_wmae = val_wmae\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    if no_improve >= early_stopping_rounds:\n",
        "        print(f\"Early stopping at epoch {i}\")\n",
        "        break\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "La4V92fEoDuw",
        "outputId": "d9f69067-541d-4535-8e2d-2e3dc907c71a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_WMAE</td><td>4868.11249</td></tr><tr><td>val_WMAE</td><td>4325.10699</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_132642-q7dsxe4q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_133212-rynldwyk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">lightGBM Last</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train WMAE = 13904.13, Val WMAE = 13728.75\n",
            "Epoch 2: Train WMAE = 12572.83, Val WMAE = 12383.03\n",
            "Epoch 3: Train WMAE = 11387.31, Val WMAE = 11174.81\n",
            "Epoch 4: Train WMAE = 10317.00, Val WMAE = 10084.97\n",
            "Epoch 5: Train WMAE = 9362.62, Val WMAE = 9111.22\n",
            "Epoch 6: Train WMAE = 8504.55, Val WMAE = 8228.25\n",
            "Epoch 7: Train WMAE = 7747.89, Val WMAE = 7435.69\n",
            "Epoch 8: Train WMAE = 7052.26, Val WMAE = 6727.27\n",
            "Epoch 9: Train WMAE = 6430.14, Val WMAE = 6087.99\n",
            "Epoch 10: Train WMAE = 5878.61, Val WMAE = 5520.06\n",
            "Epoch 11: Train WMAE = 5395.82, Val WMAE = 4996.53\n",
            "Epoch 12: Train WMAE = 4947.51, Val WMAE = 4536.17\n",
            "Epoch 13: Train WMAE = 4550.49, Val WMAE = 4126.11\n",
            "Epoch 14: Train WMAE = 4196.10, Val WMAE = 3763.04\n",
            "Epoch 15: Train WMAE = 3880.93, Val WMAE = 3440.64\n",
            "Epoch 16: Train WMAE = 3591.48, Val WMAE = 3152.92\n",
            "Epoch 17: Train WMAE = 3341.09, Val WMAE = 2899.38\n",
            "Epoch 18: Train WMAE = 3120.48, Val WMAE = 2682.02\n",
            "Epoch 19: Train WMAE = 2923.50, Val WMAE = 2482.68\n",
            "Epoch 20: Train WMAE = 2753.97, Val WMAE = 2311.26\n",
            "Epoch 21: Train WMAE = 2596.67, Val WMAE = 2153.09\n",
            "Epoch 22: Train WMAE = 2457.56, Val WMAE = 2016.00\n",
            "Epoch 23: Train WMAE = 2333.20, Val WMAE = 1896.12\n",
            "Epoch 24: Train WMAE = 2227.50, Val WMAE = 1788.03\n",
            "Epoch 25: Train WMAE = 2133.64, Val WMAE = 1698.91\n",
            "Epoch 26: Train WMAE = 2051.53, Val WMAE = 1621.55\n",
            "Epoch 27: Train WMAE = 1975.90, Val WMAE = 1554.71\n",
            "Epoch 28: Train WMAE = 1911.20, Val WMAE = 1495.94\n",
            "Epoch 29: Train WMAE = 1851.48, Val WMAE = 1442.44\n",
            "Epoch 30: Train WMAE = 1800.26, Val WMAE = 1395.87\n",
            "Epoch 31: Train WMAE = 1753.09, Val WMAE = 1356.72\n",
            "Epoch 32: Train WMAE = 1713.50, Val WMAE = 1324.49\n",
            "Epoch 33: Train WMAE = 1677.52, Val WMAE = 1297.01\n",
            "Epoch 34: Train WMAE = 1643.73, Val WMAE = 1267.88\n",
            "Epoch 35: Train WMAE = 1616.19, Val WMAE = 1247.04\n",
            "Epoch 36: Train WMAE = 1593.17, Val WMAE = 1228.51\n",
            "Epoch 37: Train WMAE = 1567.02, Val WMAE = 1209.14\n",
            "Epoch 38: Train WMAE = 1545.44, Val WMAE = 1194.29\n",
            "Epoch 39: Train WMAE = 1525.58, Val WMAE = 1179.83\n",
            "Epoch 40: Train WMAE = 1507.62, Val WMAE = 1166.57\n",
            "Epoch 41: Train WMAE = 1490.80, Val WMAE = 1155.45\n",
            "Epoch 42: Train WMAE = 1474.74, Val WMAE = 1143.41\n",
            "Epoch 43: Train WMAE = 1456.19, Val WMAE = 1128.12\n",
            "Epoch 44: Train WMAE = 1440.57, Val WMAE = 1117.55\n",
            "Epoch 45: Train WMAE = 1429.60, Val WMAE = 1110.55\n",
            "Epoch 46: Train WMAE = 1416.69, Val WMAE = 1104.69\n",
            "Epoch 47: Train WMAE = 1406.02, Val WMAE = 1101.72\n",
            "Epoch 48: Train WMAE = 1394.11, Val WMAE = 1097.18\n",
            "Epoch 49: Train WMAE = 1382.63, Val WMAE = 1090.40\n",
            "Epoch 50: Train WMAE = 1370.71, Val WMAE = 1081.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_WMAE</td><td>â–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_WMAE</td><td>â–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>1370.71208</td></tr><tr><td>val_WMAE</td><td>1081.61612</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM Last</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_133212-rynldwyk/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}