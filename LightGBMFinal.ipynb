{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4cPYckBh5YKKYYk5r/tYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvani2/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/LightGBMFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmL4iKfgdJcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe15cd8-935e-441c-937f-bbf568a54799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Mounted at /content/drive\n",
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "100% 2.70M/2.70M [00:00<00:00, 511MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  features.csv.zip\n",
            "  inflating: features.csv            \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  sampleSubmission.csv.zip\n",
            "  inflating: sampleSubmission.csv    \n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/cs231n/assignments/finalproject/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting.zip\n",
        "!unzip features.csv.zip\n",
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip sampleSubmission.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "features = pd.read_csv(\"features.csv\")\n",
        "stores = pd.read_csv(\"stores.csv\")"
      ],
      "metadata": {
        "id": "ZXitZUhTd-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    def merge_data(self, main_df, features_df, stores_df):\n",
        "        merged = pd.merge(main_df, features_df, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\n",
        "        merged = pd.merge(merged, stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n",
        "\n",
        "    def convert_date(self, df):\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "        return df\n",
        "\n",
        "    def extract_date_features(self, df):\n",
        "        df[\"Year\"] = df[\"Date\"].dt.year\n",
        "        df[\"Month\"] = df[\"Date\"].dt.month\n",
        "        df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "        df[\"Day\"] = df[\"Date\"].dt.dayofweek\n",
        "        return df\n",
        "\n",
        "    def fill_markdowns(self, df):\n",
        "        markdown_cols = [f\"MarkDown{i}\" for i in range(1, 6)]\n",
        "        df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "        return df\n",
        "\n",
        "    def fill_economics(self, df):\n",
        "        df[\"CPI\"] = df[\"CPI\"].ffill().bfill()\n",
        "        df[\"Unemployment\"] = df[\"Unemployment\"].ffill().bfill()\n",
        "        return df\n",
        "\n",
        "    def encode_types(self, df):\n",
        "        df[\"Type\"] = df[\"Type\"].map({\"A\": 0, \"B\": 1, \"C\": 2})\n",
        "        return df\n",
        "\n",
        "    def process_data(self, main_df, features_df, stores_df):\n",
        "        df = self.merge_data(main_df, features_df, stores_df)\n",
        "        df = self.convert_date(df)\n",
        "        df = self.extract_date_features(df)\n",
        "        df = self.fill_markdowns(df)\n",
        "        df = self.fill_economics(df)\n",
        "        df = self.encode_types(df)\n",
        "        return df"
      ],
      "metadata": {
        "id": "PwKqPmMfeB3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = DataProcessor()\n",
        "\n",
        "train_merged = processor.process_data(train, features, stores)\n",
        "test_merged = processor.process_data(test, features, stores)"
      ],
      "metadata": {
        "id": "EGc1O_Qyepaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged.shape, test_merged.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH6WKExVeqWp",
        "outputId": "1683070c-263c-422b-8c41-9fd7fb7fb84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((421570, 20), (115064, 19))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not train_merged['Date'].is_monotonic_increasing:\n",
        "    train_merged = train_merged.sort_values(by='Date')\n",
        "\n",
        "display(train_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "7ZKcKPqJfBtJ",
        "outputId": "a2c5beea-4d7a-401d-fa7d-f829f60f3791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50      False        42.31   \n",
              "277665     29     5 2010-02-05      15552.08      False        24.36   \n",
              "277808     29     6 2010-02-05       3200.22      False        24.36   \n",
              "277951     29     7 2010-02-05      10820.05      False        24.36   \n",
              "278094     29     8 2010-02-05      20055.64      False        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
              "0            2.572        0.0        0.0        0.0        0.0        0.0   \n",
              "277665       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "277808       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "277951       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "278094       2.788        0.0        0.0        0.0        0.0        0.0   \n",
              "\n",
              "               CPI  Unemployment  Type    Size  Year  Month  Week  Day  \n",
              "0       211.096358         8.106     0  151315  2010      2     5    4  \n",
              "277665  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "277808  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "277951  131.527903        10.064     1   93638  2010      2     5    4  \n",
              "278094  131.527903        10.064     1   93638  2010      2     5    4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>MarkDown4</th>\n",
              "      <th>MarkDown5</th>\n",
              "      <th>CPI</th>\n",
              "      <th>Unemployment</th>\n",
              "      <th>Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>False</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>211.096358</td>\n",
              "      <td>8.106</td>\n",
              "      <td>0</td>\n",
              "      <td>151315</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>131.527903</td>\n",
              "      <td>10.064</td>\n",
              "      <td>1</td>\n",
              "      <td>93638</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a3bbfaf-5a95-4a9e-b6b7-ffa788f2190b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2170447d-c18d-4393-aed1-3a83d81d0286\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2170447d-c18d-4393-aed1-3a83d81d0286')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2170447d-c18d-4393-aed1-3a83d81d0286 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(train_merged\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Store\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 29,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          29,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dept\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2010-02-05 00:00:00\",\n        \"max\": \"2010-02-05 00:00:00\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2010-02-05 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weekly_Sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8381.925873522147,\n        \"min\": 3200.22,\n        \"max\": 24924.5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          15552.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IsHoliday\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.027484039224246,\n        \"min\": 24.36,\n        \"max\": 42.31,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          24.36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel_Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0965981366279908,\n        \"min\": 2.572,\n        \"max\": 2.788,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.788\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MarkDown5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.5840948489266,\n        \"min\": 131.5279032,\n        \"max\": 211.0963582,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          131.5279032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unemployment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8756442199889177,\n        \"min\": 8.106,\n        \"max\": 10.064,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10.064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25793,\n        \"min\": 93638,\n        \"max\": 151315,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          93638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2010\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Month\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Week\",\n      \"properties\": {\n        \"dtype\": \"UInt32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Day\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged['Weekly_Sales_Lag1'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).fillna(0)\n",
        "train_merged['Weekly_Sales_Lag2'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2).fillna(0)\n",
        "train_merged['Weekly_Sales_Lag3'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(3).fillna(0)\n",
        "\n",
        "train_merged['Weekly_Sales_MA4'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=4).mean()).fillna(0)\n",
        "train_merged['Weekly_Sales_MA12'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=12).mean()).fillna(0)\n",
        "train_merged['Weekly_Sales_MA26'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(lambda x: x.rolling(window=26).mean()).fillna(0)\n",
        "\n",
        "display(train_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "ailJAOHWhDp3",
        "outputId": "109d1dff-ece6-4284-e52f-23396f269066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50      False        42.31   \n",
              "277665     29     5 2010-02-05      15552.08      False        24.36   \n",
              "277808     29     6 2010-02-05       3200.22      False        24.36   \n",
              "277951     29     7 2010-02-05      10820.05      False        24.36   \n",
              "278094     29     8 2010-02-05      20055.64      False        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  ...  Year  Month  Week  \\\n",
              "0            2.572        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277665       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277808       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "277951       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "278094       2.788        0.0        0.0        0.0  ...  2010      2     5   \n",
              "\n",
              "        Day  Weekly_Sales_Lag1  Weekly_Sales_Lag2  Weekly_Sales_Lag3  \\\n",
              "0         4                0.0                0.0                0.0   \n",
              "277665    4                0.0                0.0                0.0   \n",
              "277808    4                0.0                0.0                0.0   \n",
              "277951    4                0.0                0.0                0.0   \n",
              "278094    4                0.0                0.0                0.0   \n",
              "\n",
              "        Weekly_Sales_MA4  Weekly_Sales_MA12  Weekly_Sales_MA26  \n",
              "0                    0.0                0.0                0.0  \n",
              "277665               0.0                0.0                0.0  \n",
              "277808               0.0                0.0                0.0  \n",
              "277951               0.0                0.0                0.0  \n",
              "278094               0.0                0.0                0.0  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17483e73-e4cb-455a-9fb7-5671a4eedb02\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>...</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "      <th>Weekly_Sales_Lag1</th>\n",
              "      <th>Weekly_Sales_Lag2</th>\n",
              "      <th>Weekly_Sales_Lag3</th>\n",
              "      <th>Weekly_Sales_MA4</th>\n",
              "      <th>Weekly_Sales_MA12</th>\n",
              "      <th>Weekly_Sales_MA26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>False</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>False</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17483e73-e4cb-455a-9fb7-5671a4eedb02')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-17483e73-e4cb-455a-9fb7-5671a4eedb02 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-17483e73-e4cb-455a-9fb7-5671a4eedb02');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f039e8f6-baf3-485a-b1ab-8983d7c8fe9c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "major_holiday_dates = train_merged[train_merged['IsHoliday'] == True]['Date'].unique()\n",
        "\n",
        "def days_to_next_holiday(date, holidays):\n",
        "    future_holidays = holidays[holidays > date]\n",
        "    if len(future_holidays) > 0:\n",
        "        return (future_holidays.min() - date).days\n",
        "    return -1 # Or some other indicator for no future holidays\n",
        "\n",
        "def days_since_last_holiday(date, holidays):\n",
        "    past_holidays = holidays[holidays < date]\n",
        "    if len(past_holidays) > 0:\n",
        "        return (date - past_holidays.max()).days\n",
        "    return -1 # Or some other indicator for no past holidays\n",
        "\n",
        "# Apply holiday proximity features to both train and test data\n",
        "train_merged['Days_To_Next_Holiday'] = train_merged['Date'].apply(lambda x: days_to_next_holiday(x, major_holiday_dates))\n",
        "train_merged['Days_Since_Last_Holiday'] = train_merged['Date'].apply(lambda x: days_since_last_holiday(x, major_holiday_dates))\n",
        "\n",
        "test_merged['Days_To_Next_Holiday'] = test_merged['Date'].apply(lambda x: days_to_next_holiday(x, major_holiday_dates))\n",
        "test_merged['Days_Since_Last_Holiday'] = test_merged['Date'].apply(lambda x: days_since_last_holiday(x, major_holiday_dates))\n",
        "\n",
        "holiday_dates = {\n",
        "    \"Super_Bowl\": [\"2010-02-12\", \"2011-02-11\", \"2012-02-10\", \"2013-02-08\"],\n",
        "    \"Labor_Day\": [\"2010-09-10\", \"2011-09-09\", \"2012-09-07\", \"2013-09-06\"],\n",
        "    \"Thanksgiving\": [\"2010-11-26\", \"2011-11-25\", \"2012-11-23\", \"2013-11-29\"],\n",
        "    \"Christmas\": [\"2010-12-31\", \"2011-12-30\", \"2012-12-28\", \"2013-12-27\"]\n",
        "}\n",
        "\n",
        "for holiday, dates in holiday_dates.items():\n",
        "    train_merged[holiday] = train_merged[\"Date\"].isin(pd.to_datetime(dates)).astype(int)\n",
        "    test_merged[holiday] = test_merged[\"Date\"].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "train_merged['IsHoliday'] = train_merged['IsHoliday'].astype(int)\n",
        "test_merged['IsHoliday'] = test_merged['IsHoliday'].astype(int)\n",
        "\n",
        "display(train_merged.head())\n",
        "display(test_merged.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "vPXVIfStjU_K",
        "outputId": "65ab8c20-21ce-49db-f2b4-88988925e7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  \\\n",
              "0           1     1 2010-02-05      24924.50          0        42.31   \n",
              "277665     29     5 2010-02-05      15552.08          0        24.36   \n",
              "277808     29     6 2010-02-05       3200.22          0        24.36   \n",
              "277951     29     7 2010-02-05      10820.05          0        24.36   \n",
              "278094     29     8 2010-02-05      20055.64          0        24.36   \n",
              "\n",
              "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  ...  Weekly_Sales_Lag3  \\\n",
              "0            2.572        0.0        0.0        0.0  ...                0.0   \n",
              "277665       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "277808       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "277951       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "278094       2.788        0.0        0.0        0.0  ...                0.0   \n",
              "\n",
              "        Weekly_Sales_MA4  Weekly_Sales_MA12  Weekly_Sales_MA26  \\\n",
              "0                    0.0                0.0                0.0   \n",
              "277665               0.0                0.0                0.0   \n",
              "277808               0.0                0.0                0.0   \n",
              "277951               0.0                0.0                0.0   \n",
              "278094               0.0                0.0                0.0   \n",
              "\n",
              "        Days_To_Next_Holiday  Days_Since_Last_Holiday  Super_Bowl  Labor_Day  \\\n",
              "0                          7                       -1           0          0   \n",
              "277665                     7                       -1           0          0   \n",
              "277808                     7                       -1           0          0   \n",
              "277951                     7                       -1           0          0   \n",
              "278094                     7                       -1           0          0   \n",
              "\n",
              "        Thanksgiving  Christmas  \n",
              "0                  0          0  \n",
              "277665             0          0  \n",
              "277808             0          0  \n",
              "277951             0          0  \n",
              "278094             0          0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-414e9d70-6a2c-4993-8e25-0f168b4165c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>Weekly_Sales</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>...</th>\n",
              "      <th>Weekly_Sales_Lag3</th>\n",
              "      <th>Weekly_Sales_MA4</th>\n",
              "      <th>Weekly_Sales_MA12</th>\n",
              "      <th>Weekly_Sales_MA26</th>\n",
              "      <th>Days_To_Next_Holiday</th>\n",
              "      <th>Days_Since_Last_Holiday</th>\n",
              "      <th>Super_Bowl</th>\n",
              "      <th>Labor_Day</th>\n",
              "      <th>Thanksgiving</th>\n",
              "      <th>Christmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>24924.50</td>\n",
              "      <td>0</td>\n",
              "      <td>42.31</td>\n",
              "      <td>2.572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277665</th>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>15552.08</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277808</th>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>3200.22</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277951</th>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>10820.05</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278094</th>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "      <td>2010-02-05</td>\n",
              "      <td>20055.64</td>\n",
              "      <td>0</td>\n",
              "      <td>24.36</td>\n",
              "      <td>2.788</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-414e9d70-6a2c-4993-8e25-0f168b4165c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-414e9d70-6a2c-4993-8e25-0f168b4165c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-414e9d70-6a2c-4993-8e25-0f168b4165c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-955dac06-f636-4b86-823b-7c167e96dcce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-955dac06-f636-4b86-823b-7c167e96dcce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-955dac06-f636-4b86-823b-7c167e96dcce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  \\\n",
              "0      1     1 2012-11-02          0        55.32       3.386    6766.44   \n",
              "1      1     1 2012-11-09          0        61.24       3.314   11421.32   \n",
              "2      1     1 2012-11-16          0        52.92       3.252    9696.28   \n",
              "3      1     1 2012-11-23          1        56.23       3.211     883.59   \n",
              "4      1     1 2012-11-30          0        52.34       3.207    2460.03   \n",
              "\n",
              "   MarkDown2  MarkDown3  MarkDown4  ...  Year  Month  Week  Day  \\\n",
              "0    5147.70      50.82    3639.90  ...  2012     11    44    4   \n",
              "1    3370.89      40.28    4646.79  ...  2012     11    45    4   \n",
              "2     292.10     103.78    1133.15  ...  2012     11    46    4   \n",
              "3       4.17   74910.32     209.91  ...  2012     11    47    4   \n",
              "4       0.00    3838.35     150.57  ...  2012     11    48    4   \n",
              "\n",
              "   Days_To_Next_Holiday  Days_Since_Last_Holiday  Super_Bowl  Labor_Day  \\\n",
              "0                    -1                       56           0          0   \n",
              "1                    -1                       63           0          0   \n",
              "2                    -1                       70           0          0   \n",
              "3                    -1                       77           0          0   \n",
              "4                    -1                       84           0          0   \n",
              "\n",
              "   Thanksgiving  Christmas  \n",
              "0             0          0  \n",
              "1             0          0  \n",
              "2             0          0  \n",
              "3             1          0  \n",
              "4             0          0  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce22b629-cba7-4b57-980d-920c668da1e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>Dept</th>\n",
              "      <th>Date</th>\n",
              "      <th>IsHoliday</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Fuel_Price</th>\n",
              "      <th>MarkDown1</th>\n",
              "      <th>MarkDown2</th>\n",
              "      <th>MarkDown3</th>\n",
              "      <th>MarkDown4</th>\n",
              "      <th>...</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Week</th>\n",
              "      <th>Day</th>\n",
              "      <th>Days_To_Next_Holiday</th>\n",
              "      <th>Days_Since_Last_Holiday</th>\n",
              "      <th>Super_Bowl</th>\n",
              "      <th>Labor_Day</th>\n",
              "      <th>Thanksgiving</th>\n",
              "      <th>Christmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-02</td>\n",
              "      <td>0</td>\n",
              "      <td>55.32</td>\n",
              "      <td>3.386</td>\n",
              "      <td>6766.44</td>\n",
              "      <td>5147.70</td>\n",
              "      <td>50.82</td>\n",
              "      <td>3639.90</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>0</td>\n",
              "      <td>61.24</td>\n",
              "      <td>3.314</td>\n",
              "      <td>11421.32</td>\n",
              "      <td>3370.89</td>\n",
              "      <td>40.28</td>\n",
              "      <td>4646.79</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-16</td>\n",
              "      <td>0</td>\n",
              "      <td>52.92</td>\n",
              "      <td>3.252</td>\n",
              "      <td>9696.28</td>\n",
              "      <td>292.10</td>\n",
              "      <td>103.78</td>\n",
              "      <td>1133.15</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-23</td>\n",
              "      <td>1</td>\n",
              "      <td>56.23</td>\n",
              "      <td>3.211</td>\n",
              "      <td>883.59</td>\n",
              "      <td>4.17</td>\n",
              "      <td>74910.32</td>\n",
              "      <td>209.91</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>47</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-30</td>\n",
              "      <td>0</td>\n",
              "      <td>52.34</td>\n",
              "      <td>3.207</td>\n",
              "      <td>2460.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3838.35</td>\n",
              "      <td>150.57</td>\n",
              "      <td>...</td>\n",
              "      <td>2012</td>\n",
              "      <td>11</td>\n",
              "      <td>48</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce22b629-cba7-4b57-980d-920c668da1e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce22b629-cba7-4b57-980d-920c668da1e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce22b629-cba7-4b57-980d-920c668da1e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-82e3a822-09d3-41c3-95bb-3ef29d787b81\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82e3a822-09d3-41c3-95bb-3ef29d787b81')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-82e3a822-09d3-41c3-95bb-3ef29d787b81 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the split points (e.g., 80% for training, 10% for validation, 10% for testing)\n",
        "train_size = int(len(train_merged) * 0.8)\n",
        "val_size = int(len(train_merged) * 0.1)\n",
        "test_size = len(train_merged) - train_size - val_size\n",
        "\n",
        "# Find the dates at the calculated split points\n",
        "train_split_date = train_merged.iloc[train_size]['Date']\n",
        "val_split_date = train_merged.iloc[train_size + val_size]['Date']\n",
        "\n",
        "# Find the closest Friday to the split dates within the dataset\n",
        "# We can iterate through the dates in the merged dataframe to find the exact Friday\n",
        "def find_closest_friday(date, df_dates):\n",
        "    closest_friday = None\n",
        "    for d in df_dates:\n",
        "        if d >= date and d.dayofweek == 4: # Friday is dayofweek 4\n",
        "            closest_friday = d\n",
        "            break\n",
        "    return closest_friday\n",
        "\n",
        "all_dates = train_merged['Date'].unique()\n",
        "train_split_date_friday = find_closest_friday(train_split_date, all_dates)\n",
        "val_split_date_friday = find_closest_friday(val_split_date, all_dates)\n",
        "\n",
        "print(f\"Original train split date: {train_split_date}\")\n",
        "print(f\"Adjusted train split date (Friday): {train_split_date_friday}\")\n",
        "print(f\"Original validation split date: {val_split_date}\")\n",
        "print(f\"Adjusted validation split date (Friday): {val_split_date_friday}\")\n",
        "\n",
        "# Split the data based on the adjusted Friday dates\n",
        "train_df = train_merged[train_merged['Date'] < train_split_date_friday]\n",
        "val_df = train_merged[(train_merged['Date'] >= train_split_date_friday) & (train_merged['Date'] < val_split_date_friday)]\n",
        "test_df = train_merged[train_merged['Date'] >= val_split_date_friday]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Training set shape after resetting index:\", train_df.shape)\n",
        "print(\"Validation set shape after resetting index:\", val_df.shape)\n",
        "print(\"Test set shape after resetting index:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGL4MaqbjYDF",
        "outputId": "bcf0cf71-e4d4-4faa-cff9-89620116b417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train split date: 2012-04-13 00:00:00\n",
            "Adjusted train split date (Friday): 2012-04-13 00:00:00\n",
            "Original validation split date: 2012-07-20 00:00:00\n",
            "Adjusted validation split date (Friday): 2012-07-20 00:00:00\n",
            "Training set shape after resetting index: (335761, 32)\n",
            "Validation set shape after resetting index: (41394, 32)\n",
            "Test set shape after resetting index: (44415, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "x_Me1Ssij6xR",
        "outputId": "78ad107c-bcc4-4f53-b6f6-18efe8c9ea31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtvani22\u001b[0m (\u001b[33mfinal-project-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Weekly_Sales\"\n",
        "DROP_COLS = [\"Date\", TARGET]\n",
        "\n",
        "X_train = train_df.drop(columns=DROP_COLS)\n",
        "X_val = val_df.drop(columns=DROP_COLS)\n",
        "X_test = test_df.drop(columns=DROP_COLS)\n",
        "\n",
        "y_train = train_df[TARGET]\n",
        "y_val = val_df[TARGET]\n",
        "y_test = test_df[TARGET]"
      ],
      "metadata": {
        "id": "wnyvGN5Aj9MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def WMAE(y_true, y_pred, weights):\n",
        "    return np.round(np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights), 5)\n",
        "\n",
        "train_weights = np.where(train_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "val_weights = np.where(val_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "test_weights = np.where(test_df[\"IsHoliday\"] == 1, 5, 1)"
      ],
      "metadata": {
        "id": "zTnw0jZekMwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Val shape:\", val_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "# Check date ranges\n",
        "print(\"\\n--- DATE RANGES ---\")\n",
        "print(\"Train: \", train_df['Date'].min(), \"->\", train_df['Date'].max())\n",
        "print(\"Val:   \", val_df['Date'].min(), \"->\", val_df['Date'].max())\n",
        "print(\"Test:  \", test_df['Date'].min(), \"->\", test_df['Date'].max())\n",
        "\n",
        "# Confirm time order integrity\n",
        "if train_df['Date'].max() < val_df['Date'].min() < test_df['Date'].min():\n",
        "    print(\"\\n✅ Date-based split looks consistent (no obvious leakage).\")\n",
        "else:\n",
        "    print(\"\\n⚠️ WARNING: Date-based splits may be overlapping or unordered.\")\n",
        "\n",
        "# Check for overlapping rows (based on Date + Store + Dept as unique keys)\n",
        "train_keys = set(zip(train_df['Date'], train_df['Store'], train_df['Dept']))\n",
        "val_keys = set(zip(val_df['Date'], val_df['Store'], val_df['Dept']))\n",
        "test_keys = set(zip(test_df['Date'], test_df['Store'], test_df['Dept']))\n",
        "\n",
        "print(\"\\n--- DUPLICATE CHECKS ---\")\n",
        "print(\"Train ∩ Val overlap:\", len(train_keys & val_keys))\n",
        "print(\"Train ∩ Test overlap:\", len(train_keys & test_keys))\n",
        "print(\"Val ∩ Test overlap:\", len(val_keys & test_keys))\n",
        "\n",
        "if len(train_keys & test_keys) == 0 and len(val_keys & test_keys) == 0:\n",
        "    print(\"\\n✅ No row leakage between train/val/test.\")\n",
        "else:\n",
        "    print(\"\\n❌ Data leakage detected between splits!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWNqaUsQKKaQ",
        "outputId": "634f1155-9600-48d2-e4df-b4ca760b87bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (335761, 32)\n",
            "Val shape: (41394, 32)\n",
            "Test shape: (44415, 32)\n",
            "\n",
            "--- DATE RANGES ---\n",
            "Train:  2010-02-05 00:00:00 -> 2012-04-06 00:00:00\n",
            "Val:    2012-04-13 00:00:00 -> 2012-07-13 00:00:00\n",
            "Test:   2012-07-20 00:00:00 -> 2012-10-26 00:00:00\n",
            "\n",
            "✅ Date-based split looks consistent (no obvious leakage).\n",
            "\n",
            "--- DUPLICATE CHECKS ---\n",
            "Train ∩ Val overlap: 0\n",
            "Train ∩ Test overlap: 0\n",
            "Val ∩ Test overlap: 0\n",
            "\n",
            "✅ No row leakage between train/val/test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6JS-k4RkPHR",
        "outputId": "31e8c2da-12c9-40b6-abf7-4272449f9726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((335761, 30), (41394, 30), (44415, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"walmart-forecasting\",\n",
        "    entity=\"final-project-ml\",\n",
        "    name=\"lightGBM1\",\n",
        "    config={\n",
        "        \"model\": \"LightGBM\",\n",
        "        \"params\": {\n",
        "            \"num_leaves\": 70,\n",
        "            \"max_depth\": 15,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"min_child_samples\": 20,\n",
        "            \"subsample\": 0.8,\n",
        "            \"n_estimators\": 1000\n",
        "        },\n",
        "        \"metric\": \"WMAE\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Prepare model\n",
        "model = lgb.LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    num_leaves=70,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    n_estimators=1000,\n",
        "    random_state=42,\n",
        "    verbose=-1 # Suppress LightGBM output\n",
        ")\n",
        "\n",
        "# Track best score manually\n",
        "best_val_wmae = float(\"inf\")\n",
        "early_stopping_rounds = 50\n",
        "no_improve = 0\n",
        "\n",
        "for i in range(1, 51):\n",
        "    model.n_estimators = i\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_preds = model.predict(X_train)\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "    val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": i,\n",
        "        \"train_WMAE\": train_wmae,\n",
        "        \"val_WMAE\": val_wmae\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "    if val_wmae < best_val_wmae:\n",
        "        best_val_wmae = val_wmae\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    if no_improve >= early_stopping_rounds:\n",
        "        print(f\"Early stopping at epoch {i}\")\n",
        "        break\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_hmRuBT9LcZG",
        "outputId": "c262cef8-e1af-4347-e2ca-f8e61167d9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_WMAE</td><td>█▆▅▄▃▂▁</td></tr><tr><td>val_WMAE</td><td>█▇▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_WMAE</td><td>7737.12991</td></tr><tr><td>val_WMAE</td><td>7443.19162</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM1</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/jiomzep6' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/jiomzep6</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_112735-jiomzep6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_112858-j9oyjziu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">lightGBM1</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train WMAE = 13904.68, Val WMAE = 13727.58\n",
            "Epoch 2: Train WMAE = 12570.42, Val WMAE = 12383.81\n",
            "Epoch 3: Train WMAE = 11382.58, Val WMAE = 11175.04\n",
            "Epoch 4: Train WMAE = 10316.89, Val WMAE = 10090.68\n",
            "Epoch 5: Train WMAE = 9362.06, Val WMAE = 9114.21\n",
            "Epoch 6: Train WMAE = 8505.91, Val WMAE = 8236.79\n",
            "Epoch 7: Train WMAE = 7737.13, Val WMAE = 7443.19\n",
            "Epoch 8: Train WMAE = 7047.57, Val WMAE = 6736.73\n",
            "Epoch 9: Train WMAE = 6425.96, Val WMAE = 6100.87\n",
            "Epoch 10: Train WMAE = 5866.51, Val WMAE = 5533.35\n",
            "Epoch 11: Train WMAE = 5371.84, Val WMAE = 5018.94\n",
            "Epoch 12: Train WMAE = 4928.85, Val WMAE = 4564.49\n",
            "Epoch 13: Train WMAE = 4534.29, Val WMAE = 4155.98\n",
            "Epoch 14: Train WMAE = 4178.23, Val WMAE = 3790.81\n",
            "Epoch 15: Train WMAE = 3863.25, Val WMAE = 3468.67\n",
            "Epoch 16: Train WMAE = 3581.19, Val WMAE = 3180.51\n",
            "Epoch 17: Train WMAE = 3330.58, Val WMAE = 2921.86\n",
            "Epoch 18: Train WMAE = 3107.22, Val WMAE = 2695.53\n",
            "Epoch 19: Train WMAE = 2908.60, Val WMAE = 2491.75\n",
            "Epoch 20: Train WMAE = 2736.82, Val WMAE = 2313.32\n",
            "Epoch 21: Train WMAE = 2582.87, Val WMAE = 2155.70\n",
            "Epoch 22: Train WMAE = 2447.50, Val WMAE = 2018.44\n",
            "Epoch 23: Train WMAE = 2330.52, Val WMAE = 1897.00\n",
            "Epoch 24: Train WMAE = 2225.79, Val WMAE = 1790.94\n",
            "Epoch 25: Train WMAE = 2128.26, Val WMAE = 1698.70\n",
            "Epoch 26: Train WMAE = 2042.19, Val WMAE = 1619.18\n",
            "Epoch 27: Train WMAE = 1968.98, Val WMAE = 1553.58\n",
            "Epoch 28: Train WMAE = 1903.59, Val WMAE = 1495.20\n",
            "Epoch 29: Train WMAE = 1847.25, Val WMAE = 1444.05\n",
            "Epoch 30: Train WMAE = 1796.11, Val WMAE = 1398.99\n",
            "Epoch 31: Train WMAE = 1751.25, Val WMAE = 1361.68\n",
            "Epoch 32: Train WMAE = 1709.70, Val WMAE = 1325.59\n",
            "Epoch 33: Train WMAE = 1671.54, Val WMAE = 1293.87\n",
            "Epoch 34: Train WMAE = 1639.13, Val WMAE = 1265.75\n",
            "Epoch 35: Train WMAE = 1609.31, Val WMAE = 1240.64\n",
            "Epoch 36: Train WMAE = 1583.28, Val WMAE = 1220.25\n",
            "Epoch 37: Train WMAE = 1558.36, Val WMAE = 1199.98\n",
            "Epoch 38: Train WMAE = 1537.72, Val WMAE = 1183.25\n",
            "Epoch 39: Train WMAE = 1516.19, Val WMAE = 1170.23\n",
            "Epoch 40: Train WMAE = 1495.30, Val WMAE = 1155.98\n",
            "Epoch 41: Train WMAE = 1477.86, Val WMAE = 1143.99\n",
            "Epoch 42: Train WMAE = 1459.98, Val WMAE = 1130.45\n",
            "Epoch 43: Train WMAE = 1447.03, Val WMAE = 1124.29\n",
            "Epoch 44: Train WMAE = 1436.16, Val WMAE = 1116.03\n",
            "Epoch 45: Train WMAE = 1425.10, Val WMAE = 1112.71\n",
            "Epoch 46: Train WMAE = 1411.70, Val WMAE = 1105.62\n",
            "Epoch 47: Train WMAE = 1399.55, Val WMAE = 1096.77\n",
            "Epoch 48: Train WMAE = 1387.10, Val WMAE = 1088.09\n",
            "Epoch 49: Train WMAE = 1377.33, Val WMAE = 1081.13\n",
            "Epoch 50: Train WMAE = 1367.60, Val WMAE = 1074.63\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>█▇▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>1367.60498</td></tr><tr><td>val_WMAE</td><td>1074.63362</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM1</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/j9oyjziu</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_112858-j9oyjziu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New Experiment**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jmyJK6j1VBBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YoY Sales Feature (same week, previous year) ---\n",
        "train_merged['YoY_Sales'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52).fillna(0)\n",
        "\n",
        "# --- Fourier Features for Weekly Seasonality ---\n",
        "train_merged['sin_week'] = np.sin(2 * np.pi * train_merged['Week'] / 52)\n",
        "train_merged['cos_week'] = np.cos(2 * np.pi * train_merged['Week'] / 52)\n",
        "\n",
        "test_merged['sin_week'] = np.sin(2 * np.pi * test_merged['Week'] / 52)\n",
        "test_merged['cos_week'] = np.cos(2 * np.pi * test_merged['Week'] / 52)"
      ],
      "metadata": {
        "id": "BqpBJJ8oSUyo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the split points (e.g., 80% for training, 10% for validation, 10% for testing)\n",
        "train_size = int(len(train_merged) * 0.8)\n",
        "val_size = int(len(train_merged) * 0.1)\n",
        "test_size = len(train_merged) - train_size - val_size\n",
        "\n",
        "# Find the dates at the calculated split points\n",
        "train_split_date = train_merged.iloc[train_size]['Date']\n",
        "val_split_date = train_merged.iloc[train_size + val_size]['Date']\n",
        "\n",
        "# Find the closest Friday to the split dates within the dataset\n",
        "# We can iterate through the dates in the merged dataframe to find the exact Friday\n",
        "def find_closest_friday(date, df_dates):\n",
        "    closest_friday = None\n",
        "    for d in df_dates:\n",
        "        if d >= date and d.dayofweek == 4: # Friday is dayofweek 4\n",
        "            closest_friday = d\n",
        "            break\n",
        "    return closest_friday\n",
        "\n",
        "all_dates = train_merged['Date'].unique()\n",
        "train_split_date_friday = find_closest_friday(train_split_date, all_dates)\n",
        "val_split_date_friday = find_closest_friday(val_split_date, all_dates)\n",
        "\n",
        "print(f\"Original train split date: {train_split_date}\")\n",
        "print(f\"Adjusted train split date (Friday): {train_split_date_friday}\")\n",
        "print(f\"Original validation split date: {val_split_date}\")\n",
        "print(f\"Adjusted validation split date (Friday): {val_split_date_friday}\")\n",
        "\n",
        "# Split the data based on the adjusted Friday dates\n",
        "train_df = train_merged[train_merged['Date'] < train_split_date_friday]\n",
        "val_df = train_merged[(train_merged['Date'] >= train_split_date_friday) & (train_merged['Date'] < val_split_date_friday)]\n",
        "test_df = train_merged[train_merged['Date'] >= val_split_date_friday]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Training set shape after resetting index:\", train_df.shape)\n",
        "print(\"Validation set shape after resetting index:\", val_df.shape)\n",
        "print(\"Test set shape after resetting index:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1Q7hf7XXfNz",
        "outputId": "1790574c-c5a7-4771-d0a5-363bc45730ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train split date: 2012-04-13 00:00:00\n",
            "Adjusted train split date (Friday): 2012-04-13 00:00:00\n",
            "Original validation split date: 2012-07-20 00:00:00\n",
            "Adjusted validation split date (Friday): 2012-07-20 00:00:00\n",
            "Training set shape after resetting index: (335761, 35)\n",
            "Validation set shape after resetting index: (41394, 35)\n",
            "Test set shape after resetting index: (44415, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"Weekly_Sales\"\n",
        "DROP_COLS = [\"Date\", TARGET]\n",
        "\n",
        "X_train = train_df.drop(columns=DROP_COLS)\n",
        "X_val = val_df.drop(columns=DROP_COLS)\n",
        "X_test = test_df.drop(columns=DROP_COLS)\n",
        "\n",
        "y_train = train_df[TARGET]\n",
        "y_val = val_df[TARGET]\n",
        "y_test = test_df[TARGET]"
      ],
      "metadata": {
        "id": "cCQ1zVrNXoXS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import wandb\n",
        "\n",
        "# --- Define Hyperparameter Grid ---\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 70],\n",
        "    'max_depth': [10, 15],\n",
        "    'learning_rate': [0.05],\n",
        "    'min_child_samples': [20],\n",
        "    'subsample': [0.8],\n",
        "    'feature_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_freq': [1, 5]\n",
        "}\n",
        "\n",
        "# --- Iterate Over All Combinations ---\n",
        "for combo in itertools.product(*param_grid.values()):\n",
        "    params = dict(zip(param_grid.keys(), combo))\n",
        "\n",
        "    # Initialize wandb run\n",
        "    wandb.init(\n",
        "        project=\"walmart-forecasting\",\n",
        "        name=f\"lightGBM adv | {params}\",\n",
        "        config=params\n",
        "    )\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        objective=\"regression\",\n",
        "        n_estimators=1000,\n",
        "        random_state=42,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    best_val_wmae = float(\"inf\")\n",
        "    early_stopping_rounds = 50\n",
        "    rounds_no_improve = 0\n",
        "\n",
        "    for i in range(1, 51):\n",
        "        model.n_estimators = i\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        train_preds = model.predict(X_train)\n",
        "        val_preds = model.predict(X_val)\n",
        "\n",
        "        train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "        val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": i,\n",
        "            \"train_WMAE\": train_wmae,\n",
        "            \"val_WMAE\": val_wmae\n",
        "        })\n",
        "\n",
        "        print(f\"[{params}] Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "        if val_wmae < best_val_wmae:\n",
        "            best_val_wmae = val_wmae\n",
        "            rounds_no_improve = 0\n",
        "        else:\n",
        "            rounds_no_improve += 1\n",
        "\n",
        "        if rounds_no_improve >= early_stopping_rounds:\n",
        "            print(f\"Early stopping for {params} at epoch {i}\")\n",
        "            break\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mIbTCjxNXt2R",
        "outputId": "b3c9a7f3-77f7-4fbf-ff28-9438e3a15b57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_WMAE</td><td>█▇▆▅▄▃▂▂▁</td></tr><tr><td>val_WMAE</td><td>█▇▆▅▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_WMAE</td><td>10054.94894</td></tr><tr><td>val_WMAE</td><td>9710.89563</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/972331bp' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/972331bp</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123334-972331bp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_123405-dnu36rwq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14672.21, Val WMAE = 14472.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13982.61, Val WMAE = 13760.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13323.21, Val WMAE = 13084.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12698.48, Val WMAE = 12447.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12107.39, Val WMAE = 11841.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11553.42, Val WMAE = 11261.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11031.09, Val WMAE = 10716.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10525.46, Val WMAE = 10199.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10054.95, Val WMAE = 9710.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.78, Val WMAE = 9238.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.22, Val WMAE = 8793.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8784.80, Val WMAE = 8369.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8394.59, Val WMAE = 7972.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8026.59, Val WMAE = 7595.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7684.52, Val WMAE = 7231.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7351.87, Val WMAE = 6887.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7037.63, Val WMAE = 6563.44\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6744.97, Val WMAE = 6252.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6465.65, Val WMAE = 5959.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6212.20, Val WMAE = 5690.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5961.66, Val WMAE = 5432.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5724.19, Val WMAE = 5188.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5498.82, Val WMAE = 4956.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5292.81, Val WMAE = 4733.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5100.53, Val WMAE = 4528.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4908.30, Val WMAE = 4329.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4726.61, Val WMAE = 4144.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4556.19, Val WMAE = 3971.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4398.09, Val WMAE = 3808.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4248.43, Val WMAE = 3650.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4103.43, Val WMAE = 3502.29\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3966.03, Val WMAE = 3361.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3837.37, Val WMAE = 3231.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3718.16, Val WMAE = 3108.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3604.33, Val WMAE = 2991.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3497.36, Val WMAE = 2880.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3395.24, Val WMAE = 2779.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3303.22, Val WMAE = 2680.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3213.03, Val WMAE = 2590.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3126.61, Val WMAE = 2506.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3046.60, Val WMAE = 2425.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2969.51, Val WMAE = 2349.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2898.73, Val WMAE = 2278.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2831.86, Val WMAE = 2212.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2769.24, Val WMAE = 2152.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2710.46, Val WMAE = 2095.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2654.37, Val WMAE = 2042.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2602.40, Val WMAE = 1993.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2553.51, Val WMAE = 1944.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2511.53, Val WMAE = 1900.70\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2511.52568</td></tr><tr><td>val_WMAE</td><td>1900.70206</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/dnu36rwq</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123405-dnu36rwq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_123726-0ntms0an</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14672.21, Val WMAE = 14472.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13983.66, Val WMAE = 13759.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13324.48, Val WMAE = 13089.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12698.94, Val WMAE = 12452.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12109.24, Val WMAE = 11843.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11561.91, Val WMAE = 11263.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11037.94, Val WMAE = 10718.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10536.70, Val WMAE = 10201.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10065.97, Val WMAE = 9713.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9619.47, Val WMAE = 9241.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9199.57, Val WMAE = 8798.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8794.92, Val WMAE = 8376.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8402.98, Val WMAE = 7977.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8031.81, Val WMAE = 7598.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7689.84, Val WMAE = 7235.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7359.57, Val WMAE = 6894.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7048.80, Val WMAE = 6571.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6757.30, Val WMAE = 6261.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6475.70, Val WMAE = 5966.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6224.89, Val WMAE = 5696.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5974.58, Val WMAE = 5439.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5734.47, Val WMAE = 5192.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5507.06, Val WMAE = 4961.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5300.65, Val WMAE = 4740.73\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5107.03, Val WMAE = 4536.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4915.75, Val WMAE = 4336.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4736.75, Val WMAE = 4154.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4565.51, Val WMAE = 3981.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4404.01, Val WMAE = 3814.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4255.74, Val WMAE = 3653.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4112.33, Val WMAE = 3504.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3976.91, Val WMAE = 3362.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3849.25, Val WMAE = 3232.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3728.82, Val WMAE = 3105.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3617.34, Val WMAE = 2987.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3510.41, Val WMAE = 2879.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3407.92, Val WMAE = 2776.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3312.67, Val WMAE = 2676.63\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3219.18, Val WMAE = 2587.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3132.51, Val WMAE = 2501.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3051.70, Val WMAE = 2422.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2978.17, Val WMAE = 2347.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2906.69, Val WMAE = 2276.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2839.62, Val WMAE = 2211.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2777.64, Val WMAE = 2149.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2719.08, Val WMAE = 2091.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2661.76, Val WMAE = 2038.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2608.29, Val WMAE = 1990.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2559.77, Val WMAE = 1943.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2515.52, Val WMAE = 1898.40\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>█▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2515.52185</td></tr><tr><td>val_WMAE</td><td>1898.40246</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/0ntms0an</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_123726-0ntms0an/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124105-1e9pdx1h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14671.59, Val WMAE = 14471.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13982.55, Val WMAE = 13759.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13322.87, Val WMAE = 13083.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12696.80, Val WMAE = 12446.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12103.74, Val WMAE = 11841.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11549.92, Val WMAE = 11262.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11028.03, Val WMAE = 10717.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10523.66, Val WMAE = 10198.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10054.16, Val WMAE = 9710.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.78, Val WMAE = 9237.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.10, Val WMAE = 8794.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8783.95, Val WMAE = 8372.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8392.80, Val WMAE = 7973.95\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8026.58, Val WMAE = 7591.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7684.31, Val WMAE = 7228.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7352.99, Val WMAE = 6887.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7039.35, Val WMAE = 6562.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6748.09, Val WMAE = 6249.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6466.21, Val WMAE = 5958.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6213.55, Val WMAE = 5689.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5965.29, Val WMAE = 5431.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5727.22, Val WMAE = 5184.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5502.15, Val WMAE = 4951.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5295.76, Val WMAE = 4732.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5103.52, Val WMAE = 4529.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4912.49, Val WMAE = 4330.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4728.46, Val WMAE = 4145.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4559.03, Val WMAE = 3972.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4399.39, Val WMAE = 3809.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4252.13, Val WMAE = 3655.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4107.21, Val WMAE = 3507.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3969.86, Val WMAE = 3365.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3842.40, Val WMAE = 3236.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3722.70, Val WMAE = 3111.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3611.06, Val WMAE = 2986.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3504.82, Val WMAE = 2876.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3401.64, Val WMAE = 2774.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3309.60, Val WMAE = 2676.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3217.56, Val WMAE = 2587.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3130.69, Val WMAE = 2502.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3049.55, Val WMAE = 2422.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2972.17, Val WMAE = 2345.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2900.57, Val WMAE = 2275.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2833.22, Val WMAE = 2209.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2771.87, Val WMAE = 2149.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2713.66, Val WMAE = 2091.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2657.19, Val WMAE = 2037.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2605.32, Val WMAE = 1989.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2556.24, Val WMAE = 1942.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2513.13, Val WMAE = 1898.54\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2513.1291</td></tr><tr><td>val_WMAE</td><td>1898.53506</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/1e9pdx1h</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124105-1e9pdx1h/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124442-kooa2xxw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14671.59, Val WMAE = 14471.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13983.53, Val WMAE = 13759.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13322.23, Val WMAE = 13084.51\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12695.89, Val WMAE = 12446.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12101.62, Val WMAE = 11840.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11547.89, Val WMAE = 11261.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11024.92, Val WMAE = 10716.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10519.85, Val WMAE = 10198.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10048.96, Val WMAE = 9710.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9599.61, Val WMAE = 9236.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9180.95, Val WMAE = 8794.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8776.76, Val WMAE = 8371.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8389.32, Val WMAE = 7970.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8020.48, Val WMAE = 7595.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7680.31, Val WMAE = 7235.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7352.83, Val WMAE = 6890.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7040.01, Val WMAE = 6563.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6748.71, Val WMAE = 6253.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6468.41, Val WMAE = 5962.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6217.72, Val WMAE = 5692.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5965.32, Val WMAE = 5434.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5726.35, Val WMAE = 5189.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5501.37, Val WMAE = 4958.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5296.05, Val WMAE = 4737.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5102.61, Val WMAE = 4534.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4912.03, Val WMAE = 4335.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4733.84, Val WMAE = 4153.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4563.04, Val WMAE = 3976.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4399.31, Val WMAE = 3812.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4250.47, Val WMAE = 3653.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4106.63, Val WMAE = 3509.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3968.95, Val WMAE = 3367.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3840.39, Val WMAE = 3238.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3723.41, Val WMAE = 3116.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3611.98, Val WMAE = 2996.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3505.47, Val WMAE = 2884.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3403.27, Val WMAE = 2782.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3310.21, Val WMAE = 2681.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3218.18, Val WMAE = 2592.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3131.03, Val WMAE = 2506.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3048.59, Val WMAE = 2425.29\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2972.96, Val WMAE = 2347.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2901.54, Val WMAE = 2276.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2833.63, Val WMAE = 2211.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2772.52, Val WMAE = 2151.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2713.77, Val WMAE = 2093.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2658.68, Val WMAE = 2040.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2605.94, Val WMAE = 1992.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2554.71, Val WMAE = 1946.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2511.30, Val WMAE = 1901.53\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2511.3015</td></tr><tr><td>val_WMAE</td><td>1901.53248</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/kooa2xxw</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124442-kooa2xxw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_124841-nyrbluhl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14670.09, Val WMAE = 14472.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13981.81, Val WMAE = 13757.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13318.68, Val WMAE = 13083.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12696.31, Val WMAE = 12446.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12101.59, Val WMAE = 11839.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11551.91, Val WMAE = 11260.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 7: Train WMAE = 11030.10, Val WMAE = 10715.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10522.95, Val WMAE = 10198.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10051.91, Val WMAE = 9710.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9606.62, Val WMAE = 9239.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9188.47, Val WMAE = 8795.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8783.31, Val WMAE = 8373.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8391.48, Val WMAE = 7975.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 14: Train WMAE = 8025.94, Val WMAE = 7597.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7686.19, Val WMAE = 7233.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7353.27, Val WMAE = 6890.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7040.27, Val WMAE = 6561.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6750.44, Val WMAE = 6253.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6467.65, Val WMAE = 5961.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6214.09, Val WMAE = 5692.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5964.30, Val WMAE = 5431.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5724.86, Val WMAE = 5186.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5502.53, Val WMAE = 4953.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5296.34, Val WMAE = 4734.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5105.23, Val WMAE = 4528.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4914.12, Val WMAE = 4328.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4734.06, Val WMAE = 4146.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4562.63, Val WMAE = 3971.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4402.12, Val WMAE = 3810.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4254.77, Val WMAE = 3652.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4111.23, Val WMAE = 3505.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3973.60, Val WMAE = 3366.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3846.07, Val WMAE = 3235.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3725.05, Val WMAE = 3112.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3612.61, Val WMAE = 2993.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3506.69, Val WMAE = 2881.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3403.47, Val WMAE = 2780.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3309.33, Val WMAE = 2682.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3215.77, Val WMAE = 2588.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3127.18, Val WMAE = 2501.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3046.34, Val WMAE = 2422.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2970.44, Val WMAE = 2347.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2898.11, Val WMAE = 2276.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2829.73, Val WMAE = 2210.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2768.53, Val WMAE = 2151.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2709.73, Val WMAE = 2093.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2654.32, Val WMAE = 2039.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2601.15, Val WMAE = 1990.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2551.94, Val WMAE = 1943.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2508.44, Val WMAE = 1899.41\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2508.44183</td></tr><tr><td>val_WMAE</td><td>1899.40623</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/nyrbluhl</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_124841-nyrbluhl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_125217-i5dooxcz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14670.09, Val WMAE = 14472.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13981.81, Val WMAE = 13757.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13318.68, Val WMAE = 13083.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12696.31, Val WMAE = 12446.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12101.59, Val WMAE = 11839.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11551.91, Val WMAE = 11260.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 7: Train WMAE = 11030.10, Val WMAE = 10715.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10522.95, Val WMAE = 10198.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10051.91, Val WMAE = 9710.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9606.62, Val WMAE = 9239.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9188.47, Val WMAE = 8795.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8783.31, Val WMAE = 8373.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8391.48, Val WMAE = 7975.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 14: Train WMAE = 8025.94, Val WMAE = 7597.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7686.19, Val WMAE = 7233.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7353.27, Val WMAE = 6890.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7040.27, Val WMAE = 6561.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6750.44, Val WMAE = 6253.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6467.65, Val WMAE = 5961.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6214.09, Val WMAE = 5692.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5964.30, Val WMAE = 5431.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5724.86, Val WMAE = 5186.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5502.53, Val WMAE = 4953.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5296.34, Val WMAE = 4734.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5105.23, Val WMAE = 4528.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4914.12, Val WMAE = 4328.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4734.06, Val WMAE = 4146.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4562.63, Val WMAE = 3971.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4402.12, Val WMAE = 3810.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4254.77, Val WMAE = 3652.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4111.23, Val WMAE = 3505.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3973.60, Val WMAE = 3366.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3846.07, Val WMAE = 3235.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3725.05, Val WMAE = 3112.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3612.61, Val WMAE = 2993.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3506.69, Val WMAE = 2881.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3403.47, Val WMAE = 2780.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3309.33, Val WMAE = 2682.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3215.77, Val WMAE = 2588.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3127.18, Val WMAE = 2501.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3046.34, Val WMAE = 2422.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2970.44, Val WMAE = 2347.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2898.11, Val WMAE = 2276.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2829.73, Val WMAE = 2210.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2768.53, Val WMAE = 2151.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2709.73, Val WMAE = 2093.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2654.32, Val WMAE = 2039.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2601.15, Val WMAE = 1990.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2551.94, Val WMAE = 1943.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2508.44, Val WMAE = 1899.41\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2508.44183</td></tr><tr><td>val_WMAE</td><td>1899.40623</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/i5dooxcz</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_125217-i5dooxcz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_125616-650eeo7y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14658.17, Val WMAE = 14478.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13958.90, Val WMAE = 13770.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13296.46, Val WMAE = 13095.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12672.80, Val WMAE = 12456.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12074.58, Val WMAE = 11852.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11513.12, Val WMAE = 11276.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10993.46, Val WMAE = 10731.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10490.01, Val WMAE = 10210.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10010.03, Val WMAE = 9717.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9552.88, Val WMAE = 9247.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.63, Val WMAE = 8804.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8730.47, Val WMAE = 8381.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8347.48, Val WMAE = 7981.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7983.76, Val WMAE = 7602.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7642.29, Val WMAE = 7238.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7315.19, Val WMAE = 6895.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7007.14, Val WMAE = 6570.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6714.75, Val WMAE = 6262.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6433.59, Val WMAE = 5967.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6171.73, Val WMAE = 5690.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5924.51, Val WMAE = 5426.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5688.45, Val WMAE = 5182.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5464.44, Val WMAE = 4947.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5258.94, Val WMAE = 4729.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5065.02, Val WMAE = 4521.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4879.18, Val WMAE = 4320.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4700.02, Val WMAE = 4133.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4531.32, Val WMAE = 3958.57\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4370.87, Val WMAE = 3791.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4217.34, Val WMAE = 3633.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4076.63, Val WMAE = 3486.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3940.52, Val WMAE = 3344.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3811.43, Val WMAE = 3213.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3690.51, Val WMAE = 3088.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3573.82, Val WMAE = 2969.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3468.69, Val WMAE = 2855.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3367.77, Val WMAE = 2752.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3269.90, Val WMAE = 2654.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3178.10, Val WMAE = 2563.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3088.95, Val WMAE = 2476.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3011.13, Val WMAE = 2397.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2934.32, Val WMAE = 2320.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2862.49, Val WMAE = 2251.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2794.19, Val WMAE = 2184.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2729.91, Val WMAE = 2120.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2671.90, Val WMAE = 2063.63\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2616.46, Val WMAE = 2010.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2563.94, Val WMAE = 1960.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2514.17, Val WMAE = 1913.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2469.01, Val WMAE = 1869.15\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2469.01018</td></tr><tr><td>val_WMAE</td><td>1869.14746</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/650eeo7y</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_125616-650eeo7y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130030-fnnkz2m9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14658.17, Val WMAE = 14478.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13955.60, Val WMAE = 13770.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13298.72, Val WMAE = 13095.42\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12671.40, Val WMAE = 12455.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12078.10, Val WMAE = 11850.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11516.17, Val WMAE = 11275.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10995.16, Val WMAE = 10727.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10489.22, Val WMAE = 10206.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10013.01, Val WMAE = 9713.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9559.55, Val WMAE = 9246.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9141.41, Val WMAE = 8803.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8731.95, Val WMAE = 8381.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8345.42, Val WMAE = 7979.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7985.15, Val WMAE = 7598.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7643.29, Val WMAE = 7232.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7317.58, Val WMAE = 6889.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7006.25, Val WMAE = 6560.49\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6714.92, Val WMAE = 6249.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6437.36, Val WMAE = 5957.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6178.23, Val WMAE = 5683.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5928.39, Val WMAE = 5424.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5177.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5468.34, Val WMAE = 4943.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5260.21, Val WMAE = 4721.15\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5063.97, Val WMAE = 4513.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4875.20, Val WMAE = 4318.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4696.71, Val WMAE = 4132.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4528.65, Val WMAE = 3955.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4368.80, Val WMAE = 3788.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4216.64, Val WMAE = 3631.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4074.79, Val WMAE = 3482.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3940.46, Val WMAE = 3341.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3813.32, Val WMAE = 3210.99\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3692.87, Val WMAE = 3087.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3576.48, Val WMAE = 2966.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3465.59, Val WMAE = 2853.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3364.06, Val WMAE = 2750.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3268.49, Val WMAE = 2653.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3176.08, Val WMAE = 2561.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3089.64, Val WMAE = 2473.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3008.89, Val WMAE = 2392.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2933.71, Val WMAE = 2316.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2863.66, Val WMAE = 2247.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2796.44, Val WMAE = 2180.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2733.05, Val WMAE = 2117.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2674.28, Val WMAE = 2060.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2618.66, Val WMAE = 2006.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2563.88, Val WMAE = 1956.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2514.74, Val WMAE = 1909.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2468.37, Val WMAE = 1866.74\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.37435</td></tr><tr><td>val_WMAE</td><td>1866.74286</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/fnnkz2m9</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130030-fnnkz2m9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130515-6id0nzdf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14660.66, Val WMAE = 14475.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13958.61, Val WMAE = 13768.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13298.75, Val WMAE = 13096.40\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.93, Val WMAE = 12455.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12081.36, Val WMAE = 11849.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11519.56, Val WMAE = 11274.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10998.20, Val WMAE = 10728.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10488.34, Val WMAE = 10207.47\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10008.43, Val WMAE = 9714.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9553.80, Val WMAE = 9247.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.23, Val WMAE = 8801.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8727.44, Val WMAE = 8378.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8341.21, Val WMAE = 7978.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7978.31, Val WMAE = 7600.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7634.00, Val WMAE = 7234.74\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7309.49, Val WMAE = 6889.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7000.54, Val WMAE = 6569.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6706.90, Val WMAE = 6260.16\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6431.80, Val WMAE = 5967.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6170.79, Val WMAE = 5691.17\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5923.22, Val WMAE = 5429.90\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5687.13, Val WMAE = 5182.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5462.88, Val WMAE = 4948.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5255.07, Val WMAE = 4726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5059.63, Val WMAE = 4520.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4872.75, Val WMAE = 4321.51\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4694.25, Val WMAE = 4136.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4527.77, Val WMAE = 3962.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4367.77, Val WMAE = 3795.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4215.38, Val WMAE = 3635.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4072.13, Val WMAE = 3486.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3938.68, Val WMAE = 3345.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3807.99, Val WMAE = 3214.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3687.37, Val WMAE = 3090.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3573.10, Val WMAE = 2972.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3468.08, Val WMAE = 2859.91\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3365.11, Val WMAE = 2754.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3268.48, Val WMAE = 2657.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3176.97, Val WMAE = 2565.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3090.47, Val WMAE = 2479.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3010.86, Val WMAE = 2399.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2935.63, Val WMAE = 2322.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2861.77, Val WMAE = 2251.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2794.20, Val WMAE = 2182.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2728.37, Val WMAE = 2119.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2668.91, Val WMAE = 2063.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2613.76, Val WMAE = 2010.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2561.80, Val WMAE = 1960.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2512.73, Val WMAE = 1914.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2467.01, Val WMAE = 1869.56\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2467.00893</td></tr><tr><td>val_WMAE</td><td>1869.56022</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6id0nzdf</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130515-6id0nzdf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_130941-x3x5ccy0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14660.66, Val WMAE = 14475.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13961.92, Val WMAE = 13765.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13299.76, Val WMAE = 13093.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12676.06, Val WMAE = 12456.23\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12082.03, Val WMAE = 11848.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11520.10, Val WMAE = 11271.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10997.17, Val WMAE = 10725.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10488.74, Val WMAE = 10206.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10008.78, Val WMAE = 9716.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9557.33, Val WMAE = 9244.93\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9137.73, Val WMAE = 8799.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8726.91, Val WMAE = 8376.85\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8343.59, Val WMAE = 7979.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7979.35, Val WMAE = 7599.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7639.26, Val WMAE = 7239.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7313.78, Val WMAE = 6895.77\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7004.19, Val WMAE = 6572.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6714.14, Val WMAE = 6262.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6439.64, Val WMAE = 5970.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6177.64, Val WMAE = 5692.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5928.11, Val WMAE = 5430.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5691.57, Val WMAE = 5183.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5470.89, Val WMAE = 4950.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5263.18, Val WMAE = 4729.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5067.89, Val WMAE = 4523.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4877.63, Val WMAE = 4323.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4702.96, Val WMAE = 4135.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4534.05, Val WMAE = 3959.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4374.79, Val WMAE = 3794.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4222.51, Val WMAE = 3636.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4080.11, Val WMAE = 3488.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3944.60, Val WMAE = 3347.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3816.10, Val WMAE = 3216.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3695.75, Val WMAE = 3090.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3583.38, Val WMAE = 2971.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3478.36, Val WMAE = 2859.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3373.44, Val WMAE = 2757.32\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3273.09, Val WMAE = 2659.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3181.17, Val WMAE = 2567.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3095.58, Val WMAE = 2481.44\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3014.35, Val WMAE = 2402.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2936.94, Val WMAE = 2325.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2866.13, Val WMAE = 2254.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2798.38, Val WMAE = 2190.01\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2733.64, Val WMAE = 2126.94\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2674.77, Val WMAE = 2069.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2620.08, Val WMAE = 2012.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2566.19, Val WMAE = 1960.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2517.33, Val WMAE = 1914.61\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2472.05, Val WMAE = 1872.79\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2472.04602</td></tr><tr><td>val_WMAE</td><td>1872.78625</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/x3x5ccy0</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_130941-x3x5ccy0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_131421-7a12rrlm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14661.70, Val WMAE = 14474.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13960.41, Val WMAE = 13765.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13303.25, Val WMAE = 13091.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.37, Val WMAE = 12453.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12080.51, Val WMAE = 11845.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11514.42, Val WMAE = 11270.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10994.03, Val WMAE = 10726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10486.18, Val WMAE = 10208.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 9: Train WMAE = 10005.06, Val WMAE = 9715.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9553.68, Val WMAE = 9249.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9136.26, Val WMAE = 8805.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8728.52, Val WMAE = 8381.35\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8347.24, Val WMAE = 7983.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7979.55, Val WMAE = 7602.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7635.82, Val WMAE = 7237.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7308.41, Val WMAE = 6891.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 17: Train WMAE = 7000.19, Val WMAE = 6565.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6711.60, Val WMAE = 6256.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6433.36, Val WMAE = 5963.88\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6172.56, Val WMAE = 5686.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5926.89, Val WMAE = 5427.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5179.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5468.06, Val WMAE = 4946.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5261.71, Val WMAE = 4723.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5066.25, Val WMAE = 4518.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4877.69, Val WMAE = 4320.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4699.42, Val WMAE = 4137.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4531.13, Val WMAE = 3961.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4371.26, Val WMAE = 3796.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4219.60, Val WMAE = 3638.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4076.49, Val WMAE = 3490.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3940.28, Val WMAE = 3349.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3811.58, Val WMAE = 3218.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3693.12, Val WMAE = 3095.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3580.77, Val WMAE = 2974.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3470.44, Val WMAE = 2862.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3368.37, Val WMAE = 2758.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3270.77, Val WMAE = 2660.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3181.24, Val WMAE = 2568.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3094.89, Val WMAE = 2482.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3014.11, Val WMAE = 2401.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2935.01, Val WMAE = 2323.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2863.68, Val WMAE = 2252.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2795.76, Val WMAE = 2185.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2732.13, Val WMAE = 2123.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2673.35, Val WMAE = 2065.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2616.55, Val WMAE = 2010.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2563.71, Val WMAE = 1958.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2514.45, Val WMAE = 1911.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2468.55, Val WMAE = 1867.11\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.55389</td></tr><tr><td>val_WMAE</td><td>1867.10872</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/7a12rrlm</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_131421-7a12rrlm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_131829-rc1e2a7i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14661.70, Val WMAE = 14474.52\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13960.41, Val WMAE = 13765.59\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13303.25, Val WMAE = 13091.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12675.37, Val WMAE = 12453.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12080.51, Val WMAE = 11845.92\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11514.42, Val WMAE = 11270.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10994.03, Val WMAE = 10726.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10486.18, Val WMAE = 10208.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10005.06, Val WMAE = 9715.81\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9553.68, Val WMAE = 9249.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9136.26, Val WMAE = 8805.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8728.52, Val WMAE = 8381.35\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8347.24, Val WMAE = 7983.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7979.55, Val WMAE = 7602.82\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7635.82, Val WMAE = 7237.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7308.41, Val WMAE = 6891.87\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 17: Train WMAE = 7000.19, Val WMAE = 6565.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6711.60, Val WMAE = 6256.05\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6433.36, Val WMAE = 5963.88\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6172.56, Val WMAE = 5686.76\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5926.89, Val WMAE = 5427.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5692.29, Val WMAE = 5179.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5468.06, Val WMAE = 4946.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5261.71, Val WMAE = 4723.84\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5066.25, Val WMAE = 4518.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4877.69, Val WMAE = 4320.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 27: Train WMAE = 4699.42, Val WMAE = 4137.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 28: Train WMAE = 4531.13, Val WMAE = 3961.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 29: Train WMAE = 4371.26, Val WMAE = 3796.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 30: Train WMAE = 4219.60, Val WMAE = 3638.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 31: Train WMAE = 4076.49, Val WMAE = 3490.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 32: Train WMAE = 3940.28, Val WMAE = 3349.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 33: Train WMAE = 3811.58, Val WMAE = 3218.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 34: Train WMAE = 3693.12, Val WMAE = 3095.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 35: Train WMAE = 3580.77, Val WMAE = 2974.75\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 36: Train WMAE = 3470.44, Val WMAE = 2862.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 37: Train WMAE = 3368.37, Val WMAE = 2758.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 38: Train WMAE = 3270.77, Val WMAE = 2660.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 39: Train WMAE = 3181.24, Val WMAE = 2568.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 40: Train WMAE = 3094.89, Val WMAE = 2482.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 41: Train WMAE = 3014.11, Val WMAE = 2401.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 42: Train WMAE = 2935.01, Val WMAE = 2323.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 43: Train WMAE = 2863.68, Val WMAE = 2252.24\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 44: Train WMAE = 2795.76, Val WMAE = 2185.30\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 45: Train WMAE = 2732.13, Val WMAE = 2123.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 46: Train WMAE = 2673.35, Val WMAE = 2065.65\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 47: Train WMAE = 2616.55, Val WMAE = 2010.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 48: Train WMAE = 2563.71, Val WMAE = 1958.86\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 49: Train WMAE = 2514.45, Val WMAE = 1911.97\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}] Epoch 50: Train WMAE = 2468.55, Val WMAE = 1867.11\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2468.55389</td></tr><tr><td>val_WMAE</td><td>1867.10872</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 0.9, 'bagging_fraction': 1.0, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rc1e2a7i</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_131829-rc1e2a7i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_132233-6uw83lvq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 1: Train WMAE = 14658.38, Val WMAE = 14479.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 2: Train WMAE = 13965.27, Val WMAE = 13771.09\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 3: Train WMAE = 13302.66, Val WMAE = 13096.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 4: Train WMAE = 12675.45, Val WMAE = 12456.67\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 5: Train WMAE = 12081.50, Val WMAE = 11851.34\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 6: Train WMAE = 11518.57, Val WMAE = 11276.25\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 7: Train WMAE = 10982.40, Val WMAE = 10730.79\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 8: Train WMAE = 10475.42, Val WMAE = 10213.62\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 9: Train WMAE = 9999.56, Val WMAE = 9721.22\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 10: Train WMAE = 9548.32, Val WMAE = 9249.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 11: Train WMAE = 9123.83, Val WMAE = 8807.08\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 12: Train WMAE = 8722.04, Val WMAE = 8385.19\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 13: Train WMAE = 8335.28, Val WMAE = 7986.21\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 14: Train WMAE = 7976.10, Val WMAE = 7604.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 15: Train WMAE = 7628.50, Val WMAE = 7239.07\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 16: Train WMAE = 7298.51, Val WMAE = 6895.98\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 17: Train WMAE = 6986.12, Val WMAE = 6567.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 18: Train WMAE = 6697.69, Val WMAE = 6260.39\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 19: Train WMAE = 6423.04, Val WMAE = 5969.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 20: Train WMAE = 6160.69, Val WMAE = 5693.03\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 21: Train WMAE = 5914.50, Val WMAE = 5431.53\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 22: Train WMAE = 5680.80, Val WMAE = 5186.02\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 23: Train WMAE = 5457.90, Val WMAE = 4952.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 24: Train WMAE = 5251.10, Val WMAE = 4731.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 25: Train WMAE = 5052.86, Val WMAE = 4524.28\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 26: Train WMAE = 4863.99, Val WMAE = 4325.37\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 27: Train WMAE = 4686.07, Val WMAE = 4140.96\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 28: Train WMAE = 4518.38, Val WMAE = 3962.26\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 29: Train WMAE = 4361.36, Val WMAE = 3795.80\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 30: Train WMAE = 4209.57, Val WMAE = 3635.43\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 31: Train WMAE = 4066.81, Val WMAE = 3485.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 32: Train WMAE = 3932.57, Val WMAE = 3343.48\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 33: Train WMAE = 3804.26, Val WMAE = 3210.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 34: Train WMAE = 3683.94, Val WMAE = 3087.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 35: Train WMAE = 3569.77, Val WMAE = 2969.89\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 36: Train WMAE = 3462.25, Val WMAE = 2860.69\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 37: Train WMAE = 3359.90, Val WMAE = 2755.04\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 38: Train WMAE = 3262.31, Val WMAE = 2656.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 39: Train WMAE = 3172.72, Val WMAE = 2563.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 40: Train WMAE = 3085.07, Val WMAE = 2474.71\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 41: Train WMAE = 3003.97, Val WMAE = 2392.41\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 42: Train WMAE = 2925.34, Val WMAE = 2314.14\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 43: Train WMAE = 2854.16, Val WMAE = 2242.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 44: Train WMAE = 2786.58, Val WMAE = 2174.83\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 45: Train WMAE = 2723.25, Val WMAE = 2112.58\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 46: Train WMAE = 2663.97, Val WMAE = 2055.55\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 47: Train WMAE = 2608.25, Val WMAE = 2000.64\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 48: Train WMAE = 2554.78, Val WMAE = 1949.06\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 49: Train WMAE = 2504.06, Val WMAE = 1902.46\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}] Epoch 50: Train WMAE = 2457.09, Val WMAE = 1858.56\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>██▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>2457.09293</td></tr><tr><td>val_WMAE</td><td>1858.56373</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 1}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/6uw83lvq</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_132233-6uw83lvq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_132642-q7dsxe4q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 1: Train WMAE = 14658.38, Val WMAE = 14479.00\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 2: Train WMAE = 13961.74, Val WMAE = 13772.27\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 3: Train WMAE = 13299.84, Val WMAE = 13099.68\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 4: Train WMAE = 12671.59, Val WMAE = 12460.10\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 5: Train WMAE = 12079.45, Val WMAE = 11852.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 6: Train WMAE = 11520.72, Val WMAE = 11277.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 7: Train WMAE = 10989.30, Val WMAE = 10728.36\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 8: Train WMAE = 10483.66, Val WMAE = 10209.54\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 9: Train WMAE = 10005.70, Val WMAE = 9719.13\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 10: Train WMAE = 9553.37, Val WMAE = 9253.72\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 11: Train WMAE = 9124.44, Val WMAE = 8808.11\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 12: Train WMAE = 8715.10, Val WMAE = 8386.18\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 13: Train WMAE = 8328.54, Val WMAE = 7985.78\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 14: Train WMAE = 7968.92, Val WMAE = 7609.56\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 15: Train WMAE = 7626.74, Val WMAE = 7244.31\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 16: Train WMAE = 7300.35, Val WMAE = 6897.12\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 17: Train WMAE = 6988.23, Val WMAE = 6569.70\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 18: Train WMAE = 6697.34, Val WMAE = 6263.33\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 19: Train WMAE = 6423.56, Val WMAE = 5971.20\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 20: Train WMAE = 6165.74, Val WMAE = 5696.45\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 21: Train WMAE = 5919.92, Val WMAE = 5433.50\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 22: Train WMAE = 5684.23, Val WMAE = 5185.47\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 23: Train WMAE = 5463.35, Val WMAE = 4953.66\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 24: Train WMAE = 5252.55, Val WMAE = 4731.60\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 25: Train WMAE = 5055.20, Val WMAE = 4523.38\n",
            "[{'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}] Epoch 26: Train WMAE = 4868.11, Val WMAE = 4325.11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-1892232168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         return self._Booster.predict(  # type: ignore[union-attr]\n\u001b[0m\u001b[1;32m   1037\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4746\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4748\u001b[0;31m         return predictor.predict(\n\u001b[0m\u001b[1;32m   4749\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4750\u001b[0m             \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             )\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             preds, nrow = self.__pred_for_np2d(\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             return self.__inner_predict_np2d(\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mout_num_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         _safe_call(\n\u001b[0;32m-> 1291\u001b[0;31m             _LIB.LGBM_BoosterPredictForMat(\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"walmart-forecasting\",\n",
        "    entity=\"final-project-ml\",\n",
        "    name=\"lightGBM Last\",\n",
        "    config={\n",
        "        \"model\": \"LightGBM\",\n",
        "        \"params\": {\n",
        "            \"num_leaves\": 70,\n",
        "            \"max_depth\": 15,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"min_child_samples\": 20,\n",
        "            \"subsample\": 0.8,\n",
        "            \"feature_fraction\": 0.9,\n",
        "            \"bagging_fraction\": 0.9,\n",
        "            \"bagging_freq\": 5,\n",
        "            \"n_estimators\": 1000\n",
        "        },\n",
        "        \"metric\": \"WMAE\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Prepare model\n",
        "model = lgb.LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    num_leaves=70,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    feature_fraction=0.9,\n",
        "    bagging_fraction=0.9,\n",
        "    bagging_freq=5,\n",
        "    n_estimators=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Track best score manually\n",
        "best_val_wmae = float(\"inf\")\n",
        "early_stopping_rounds = 50\n",
        "no_improve = 0\n",
        "\n",
        "for i in range(1, 51):\n",
        "    model.n_estimators = i\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_preds = model.predict(X_train)\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    train_wmae = WMAE(y_train, train_preds, train_weights)\n",
        "    val_wmae = WMAE(y_val, val_preds, val_weights)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": i,\n",
        "        \"train_WMAE\": train_wmae,\n",
        "        \"val_WMAE\": val_wmae\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {i}: Train WMAE = {train_wmae:.2f}, Val WMAE = {val_wmae:.2f}\")\n",
        "\n",
        "    if val_wmae < best_val_wmae:\n",
        "        best_val_wmae = val_wmae\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    if no_improve >= early_stopping_rounds:\n",
        "        print(f\"Early stopping at epoch {i}\")\n",
        "        break\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "La4V92fEoDuw",
        "outputId": "d9f69067-541d-4535-8e2d-2e3dc907c71a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_WMAE</td><td>██▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_WMAE</td><td>4868.11249</td></tr><tr><td>val_WMAE</td><td>4325.10699</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM adv | {'num_leaves': 31, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_samples': 20, 'subsample': 0.8, 'feature_fraction': 1.0, 'bagging_fraction': 0.8, 'bagging_freq': 5}</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/q7dsxe4q</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_132642-q7dsxe4q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_133212-rynldwyk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">lightGBM Last</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train WMAE = 13904.13, Val WMAE = 13728.75\n",
            "Epoch 2: Train WMAE = 12572.83, Val WMAE = 12383.03\n",
            "Epoch 3: Train WMAE = 11387.31, Val WMAE = 11174.81\n",
            "Epoch 4: Train WMAE = 10317.00, Val WMAE = 10084.97\n",
            "Epoch 5: Train WMAE = 9362.62, Val WMAE = 9111.22\n",
            "Epoch 6: Train WMAE = 8504.55, Val WMAE = 8228.25\n",
            "Epoch 7: Train WMAE = 7747.89, Val WMAE = 7435.69\n",
            "Epoch 8: Train WMAE = 7052.26, Val WMAE = 6727.27\n",
            "Epoch 9: Train WMAE = 6430.14, Val WMAE = 6087.99\n",
            "Epoch 10: Train WMAE = 5878.61, Val WMAE = 5520.06\n",
            "Epoch 11: Train WMAE = 5395.82, Val WMAE = 4996.53\n",
            "Epoch 12: Train WMAE = 4947.51, Val WMAE = 4536.17\n",
            "Epoch 13: Train WMAE = 4550.49, Val WMAE = 4126.11\n",
            "Epoch 14: Train WMAE = 4196.10, Val WMAE = 3763.04\n",
            "Epoch 15: Train WMAE = 3880.93, Val WMAE = 3440.64\n",
            "Epoch 16: Train WMAE = 3591.48, Val WMAE = 3152.92\n",
            "Epoch 17: Train WMAE = 3341.09, Val WMAE = 2899.38\n",
            "Epoch 18: Train WMAE = 3120.48, Val WMAE = 2682.02\n",
            "Epoch 19: Train WMAE = 2923.50, Val WMAE = 2482.68\n",
            "Epoch 20: Train WMAE = 2753.97, Val WMAE = 2311.26\n",
            "Epoch 21: Train WMAE = 2596.67, Val WMAE = 2153.09\n",
            "Epoch 22: Train WMAE = 2457.56, Val WMAE = 2016.00\n",
            "Epoch 23: Train WMAE = 2333.20, Val WMAE = 1896.12\n",
            "Epoch 24: Train WMAE = 2227.50, Val WMAE = 1788.03\n",
            "Epoch 25: Train WMAE = 2133.64, Val WMAE = 1698.91\n",
            "Epoch 26: Train WMAE = 2051.53, Val WMAE = 1621.55\n",
            "Epoch 27: Train WMAE = 1975.90, Val WMAE = 1554.71\n",
            "Epoch 28: Train WMAE = 1911.20, Val WMAE = 1495.94\n",
            "Epoch 29: Train WMAE = 1851.48, Val WMAE = 1442.44\n",
            "Epoch 30: Train WMAE = 1800.26, Val WMAE = 1395.87\n",
            "Epoch 31: Train WMAE = 1753.09, Val WMAE = 1356.72\n",
            "Epoch 32: Train WMAE = 1713.50, Val WMAE = 1324.49\n",
            "Epoch 33: Train WMAE = 1677.52, Val WMAE = 1297.01\n",
            "Epoch 34: Train WMAE = 1643.73, Val WMAE = 1267.88\n",
            "Epoch 35: Train WMAE = 1616.19, Val WMAE = 1247.04\n",
            "Epoch 36: Train WMAE = 1593.17, Val WMAE = 1228.51\n",
            "Epoch 37: Train WMAE = 1567.02, Val WMAE = 1209.14\n",
            "Epoch 38: Train WMAE = 1545.44, Val WMAE = 1194.29\n",
            "Epoch 39: Train WMAE = 1525.58, Val WMAE = 1179.83\n",
            "Epoch 40: Train WMAE = 1507.62, Val WMAE = 1166.57\n",
            "Epoch 41: Train WMAE = 1490.80, Val WMAE = 1155.45\n",
            "Epoch 42: Train WMAE = 1474.74, Val WMAE = 1143.41\n",
            "Epoch 43: Train WMAE = 1456.19, Val WMAE = 1128.12\n",
            "Epoch 44: Train WMAE = 1440.57, Val WMAE = 1117.55\n",
            "Epoch 45: Train WMAE = 1429.60, Val WMAE = 1110.55\n",
            "Epoch 46: Train WMAE = 1416.69, Val WMAE = 1104.69\n",
            "Epoch 47: Train WMAE = 1406.02, Val WMAE = 1101.72\n",
            "Epoch 48: Train WMAE = 1394.11, Val WMAE = 1097.18\n",
            "Epoch 49: Train WMAE = 1382.63, Val WMAE = 1090.40\n",
            "Epoch 50: Train WMAE = 1370.71, Val WMAE = 1081.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_WMAE</td><td>█▇▇▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_WMAE</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_WMAE</td><td>1370.71208</td></tr><tr><td>val_WMAE</td><td>1081.61612</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lightGBM Last</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting/runs/rynldwyk</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-forecasting' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_133212-rynldwyk/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}