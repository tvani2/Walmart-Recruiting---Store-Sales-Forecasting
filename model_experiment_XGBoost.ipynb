{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tSNt8fdQatHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUr2Sfl_at7y",
        "outputId": "b07f055c-18cd-4f75-e2c8-011c1ec1dd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PE_91yclciqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import logging\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import xgboost as xgb\n",
        "import wandb\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "dIMDeTGw6wTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('walmart_sales_prediction.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "aA6df3NV8AN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and preprocessing the data"
      ],
      "metadata": {
        "id": "c2w0ZuVeco8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self):\n",
        "        self.train_df = None\n",
        "        self.test_df = None\n",
        "        self.features_df = None\n",
        "        self.stores_df = None\n",
        "        self.data_info = {}\n",
        "\n",
        "    def load_datasets(self, train_path, test_path, features_path, stores_path):\n",
        "        logger.info(\"Loading datasets...\")\n",
        "\n",
        "        self.train_df = pd.read_csv(train_path)\n",
        "        self.test_df = pd.read_csv(test_path)\n",
        "        self.features_df = pd.read_csv(features_path)\n",
        "        self.stores_df = pd.read_csv(stores_path)\n",
        "\n",
        "        shapes = {\n",
        "            'train_shape': self.train_df.shape,\n",
        "            'test_shape': self.test_df.shape,\n",
        "            'features_shape': self.features_df.shape,\n",
        "            'stores_shape': self.stores_df.shape\n",
        "        }\n",
        "\n",
        "        wandb.log(shapes)\n",
        "        self.data_info.update(shapes)\n",
        "        self._log_dataset_info()\n",
        "        return self.train_df, self.test_df, self.features_df, self.stores_df\n",
        "\n",
        "    def _log_dataset_info(self):\n",
        "        logger.info(\"Creating data exploration visualizations\")\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#5D737E']\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "        fig.suptitle('Walmart Sales Data Exploration', fontsize=20, fontweight='bold')\n",
        "\n",
        "        axes[0, 0].hist(self.train_df['Weekly_Sales'], bins=50, alpha=0.8,\n",
        "                       color=colors[0], edgecolor='black', linewidth=0.5)\n",
        "        axes[0, 0].set_title('Weekly Sales Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Weekly Sales ($)')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        store_sales = self.train_df.groupby('Store')['Weekly_Sales'].mean().sort_values(ascending=False)\n",
        "        axes[0, 1].bar(range(len(store_sales)), store_sales.values,\n",
        "                      color=colors[1], alpha=0.8)\n",
        "        axes[0, 1].set_title('Average Sales by Store', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Store Rank')\n",
        "        axes[0, 1].set_ylabel('Average Weekly Sales ($)')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        holiday_counts = self.train_df['IsHoliday'].value_counts()\n",
        "        colors_pie = [colors[2], colors[3]]\n",
        "        axes[0, 2].pie(holiday_counts.values, labels=['Non-Holiday', 'Holiday'],\n",
        "                      autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
        "        axes[0, 2].set_title('Holiday vs Non-Holiday Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "        missing_data = pd.concat([self.train_df, self.features_df], axis=1).isnull().sum()\n",
        "        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "        if len(missing_data) > 0:\n",
        "            axes[1, 0].bar(range(len(missing_data)), missing_data.values,\n",
        "                          color=colors[4], alpha=0.8)\n",
        "            axes[1, 0].set_title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
        "            axes[1, 0].set_xlabel('Columns')\n",
        "            axes[1, 0].set_ylabel('Missing Count')\n",
        "            axes[1, 0].set_xticks(range(len(missing_data)))\n",
        "            axes[1, 0].set_xticklabels(missing_data.index, rotation=45)\n",
        "        else:\n",
        "            axes[1, 0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center',\n",
        "                           transform=axes[1, 0].transAxes, fontsize=12)\n",
        "            axes[1, 0].set_title('Missing Values Status', fontsize=14, fontweight='bold')\n",
        "\n",
        "        self.train_df['Date'] = pd.to_datetime(self.train_df['Date'])\n",
        "        monthly_sales = self.train_df.groupby(self.train_df['Date'].dt.to_period('M'))['Weekly_Sales'].mean()\n",
        "        axes[1, 1].plot(monthly_sales.index.astype(str), monthly_sales.values,\n",
        "                       color=colors[0], linewidth=2, marker='o', markersize=4)\n",
        "        axes[1, 1].set_title('Monthly Sales Trend', fontsize=14, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Month')\n",
        "        axes[1, 1].set_ylabel('Average Weekly Sales ($)')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        dept_sales = self.train_df.groupby('Dept')['Weekly_Sales'].mean().sort_values(ascending=False).head(10)\n",
        "        axes[1, 2].bar(range(len(dept_sales)), dept_sales.values,\n",
        "                      color=colors[1], alpha=0.8)\n",
        "        axes[1, 2].set_title('Top 10 Departments by Avg Sales', fontsize=14, fontweight='bold')\n",
        "        axes[1, 2].set_xlabel('Department Rank')\n",
        "        axes[1, 2].set_ylabel('Average Weekly Sales ($)')\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        store_type_counts = self.stores_df['Type'].value_counts()\n",
        "        axes[2, 0].bar(store_type_counts.index, store_type_counts.values,\n",
        "                      color=colors[2], alpha=0.8)\n",
        "        axes[2, 0].set_title('Store Types Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[2, 0].set_xlabel('Store Type')\n",
        "        axes[2, 0].set_ylabel('Count')\n",
        "        axes[2, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[2, 1].hist(self.stores_df['Size'], bins=30, alpha=0.8,\n",
        "                       color=colors[3], edgecolor='black', linewidth=0.5)\n",
        "        axes[2, 1].set_title('Store Size Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[2, 1].set_xlabel('Store Size')\n",
        "        axes[2, 1].set_ylabel('Frequency')\n",
        "        axes[2, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        holiday_sales = self.train_df.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
        "        axes[2, 2].bar(['Non-Holiday', 'Holiday'], holiday_sales.values,\n",
        "                      color=[colors[4], colors[0]], alpha=0.8)\n",
        "        axes[2, 2].set_title('Average Sales: Holiday vs Non-Holiday', fontsize=14, fontweight='bold')\n",
        "        axes[2, 2].set_ylabel('Average Weekly Sales ($)')\n",
        "        axes[2, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')\n",
        "        wandb.log({\"data_exploration\": wandb.Image('data_exploration.png')})\n",
        "        plt.close()\n",
        "\n",
        "        basic_stats = {\n",
        "            'total_records_train': len(self.train_df),\n",
        "            'total_records_test': len(self.test_df),\n",
        "            'unique_stores': self.train_df['Store'].nunique(),\n",
        "            'unique_departments': self.train_df['Dept'].nunique(),\n",
        "            'date_range_days': (self.train_df['Date'].max() - self.train_df['Date'].min()).days,\n",
        "            'avg_weekly_sales': self.train_df['Weekly_Sales'].mean(),\n",
        "            'median_weekly_sales': self.train_df['Weekly_Sales'].median(),\n",
        "            'std_weekly_sales': self.train_df['Weekly_Sales'].std(),\n",
        "            'min_weekly_sales': self.train_df['Weekly_Sales'].min(),\n",
        "            'max_weekly_sales': self.train_df['Weekly_Sales'].max(),\n",
        "            'holiday_percentage': (self.train_df['IsHoliday'].sum() / len(self.train_df)) * 100,\n",
        "            'negative_sales_count': (self.train_df['Weekly_Sales'] < 0).sum(),\n",
        "            'zero_sales_count': (self.train_df['Weekly_Sales'] == 0).sum()\n",
        "        }\n",
        "\n",
        "        wandb.log(basic_stats)\n",
        "        self.data_info.update(basic_stats)\n",
        "        logger.info(\"Data exploration completed and logged\")\n",
        "        logger.info(f\"Key insights:\")\n",
        "        logger.info(f\"Total training records: {basic_stats['total_records_train']:,}\")\n",
        "        logger.info(f\"Unique stores: {basic_stats['unique_stores']}\")\n",
        "        logger.info(f\"Unique departments: {basic_stats['unique_departments']}\")\n",
        "        logger.info(f\"Average weekly sales: ${basic_stats['avg_weekly_sales']:,.2f}\")\n",
        "        logger.info(f\"Holiday percentage: {basic_stats['holiday_percentage']:.1f}%\")\n",
        "\n",
        "\n",
        "class BasicPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        self.preprocessing_metrics = {}\n",
        "\n",
        "    def merge_datasets(self, train_df, test_df, features_df, stores_df):\n",
        "        logger.info(\"Merging datasets...\")\n",
        "\n",
        "        for df in [train_df, test_df, features_df]:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        initial_train_shape = train_df.shape\n",
        "        initial_test_shape = test_df.shape\n",
        "\n",
        "        train_merged = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "        test_merged = test_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "        train_merged = train_merged.merge(stores_df, on='Store', how='left')\n",
        "        test_merged = test_merged.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        merge_stats = {\n",
        "            'train_shape_before_merge': initial_train_shape,\n",
        "            'test_shape_before_merge': initial_test_shape,\n",
        "            'train_shape_after_merge': train_merged.shape,\n",
        "            'test_shape_after_merge': test_merged.shape,\n",
        "            'columns_added': train_merged.shape[1] - initial_train_shape[1]\n",
        "        }\n",
        "\n",
        "        wandb.log(merge_stats)\n",
        "        self.preprocessing_metrics.update(merge_stats)\n",
        "\n",
        "        logger.info(f\"Datasets merged successfully\")\n",
        "        logger.info(f\"Train shape: {initial_train_shape} → {train_merged.shape}\")\n",
        "        logger.info(f\"Test shape: {initial_test_shape} → {test_merged.shape}\")\n",
        "\n",
        "        return train_merged, test_merged\n",
        "\n",
        "    def handle_missing_values(self, train_df, test_df):\n",
        "        logger.info(\"Handling missing values...\")\n",
        "\n",
        "        train_clean = train_df.copy()\n",
        "        test_clean = test_df.copy()\n",
        "\n",
        "        missing_before = {}\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "        for col in markdown_cols:\n",
        "            missing_before[f'{col}_missing_before'] = train_clean[col].isnull().sum()\n",
        "            train_clean[col] = train_clean[col].fillna(0)\n",
        "            test_clean[col] = test_clean[col].fillna(0)\n",
        "\n",
        "        for col in ['CPI', 'Unemployment']:\n",
        "            missing_before[f'{col}_missing_before'] = train_clean[col].isnull().sum()\n",
        "\n",
        "            train_clean = train_clean.sort_values(['Store', 'Date'])\n",
        "            test_clean = test_clean.sort_values(['Store', 'Date'])\n",
        "            train_clean[col] = train_clean.groupby('Store')[col].ffill().bfill()\n",
        "            test_clean[col] = test_clean.groupby('Store')[col].ffill().bfill()\n",
        "\n",
        "            if train_clean[col].isna().any() or test_clean[col].isna().any():\n",
        "                overall_median = train_clean[col].median()\n",
        "                train_clean[col] = train_clean[col].fillna(overall_median)\n",
        "                test_clean[col] = test_clean[col].fillna(overall_median)\n",
        "\n",
        "            missing_after = train_clean[col].isnull().sum()\n",
        "            logger.info(f\"   • {col}: {missing_before[f'{col}_missing_before']} → {missing_after} missing values\")\n",
        "\n",
        "        train_clean['IsHoliday'] = train_clean['IsHoliday'].astype(int)\n",
        "        test_clean['IsHoliday'] = test_clean['IsHoliday'].astype(int)\n",
        "\n",
        "        self.label_encoders['Type'] = LabelEncoder()\n",
        "        train_clean['Type'] = self.label_encoders['Type'].fit_transform(train_clean['Type'])\n",
        "        test_clean['Type'] = self.label_encoders['Type'].transform(test_clean['Type'])\n",
        "\n",
        "        missing_stats = {\n",
        "            'markdown_columns_filled': len(markdown_cols),\n",
        "            'economic_columns_filled': 2,\n",
        "            'total_missing_values_handled': sum(missing_before.values())\n",
        "        }\n",
        "\n",
        "        wandb.log(missing_stats)\n",
        "        self.preprocessing_metrics.update(missing_stats)\n",
        "\n",
        "        logger.info(\"Missing values handled successfully\")\n",
        "        logger.info(f\"Markdown columns filled with 0: {len(markdown_cols)}\")\n",
        "        logger.info(f\"Economic indicators interpolated: CPI, Unemployment\")\n",
        "        logger.info(f\"Store type encoded: {len(self.label_encoders['Type'].classes_)} classes\")\n",
        "\n",
        "        return train_clean, test_clean\n",
        "\n",
        "    def create_preprocessing_summary(self, train_df, test_df):\n",
        "        logger.info(\"Creating preprocessing summary...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        fig.suptitle('Basic Preprocessing Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "\n",
        "        # Data completeness\n",
        "        completeness = (1 - train_df.isnull().sum() / len(train_df)) * 100\n",
        "        axes[0, 0].bar(range(len(completeness)), completeness.values,\n",
        "                      color=colors[0], alpha=0.8)\n",
        "        axes[0, 0].set_title('Data Completeness by Column', fontweight='bold')\n",
        "        axes[0, 0].set_ylabel('Completeness (%)')\n",
        "        axes[0, 0].set_xlabel('Column Index')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Store type distribution after encoding\n",
        "        type_dist = train_df['Type'].value_counts()\n",
        "        axes[0, 1].bar(type_dist.index, type_dist.values,\n",
        "                      color=colors[1], alpha=0.8)\n",
        "        axes[0, 1].set_title('Store Type Distribution (Encoded)', fontweight='bold')\n",
        "        axes[0, 1].set_ylabel('Count')\n",
        "        axes[0, 1].set_xlabel('Store Type (Encoded)')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Holiday distribution\n",
        "        holiday_dist = train_df['IsHoliday'].value_counts()\n",
        "        axes[1, 0].bar(['Non-Holiday', 'Holiday'], holiday_dist.values,\n",
        "                      color=[colors[2], colors[3]], alpha=0.8)\n",
        "        axes[1, 0].set_title('Holiday Distribution', fontweight='bold')\n",
        "        axes[1, 0].set_ylabel('Count')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Dataset size comparison\n",
        "        sizes = [train_df.shape[0], test_df.shape[0]]\n",
        "        axes[1, 1].bar(['Training', 'Test'], sizes,\n",
        "                      color=[colors[0], colors[1]], alpha=0.8)\n",
        "        axes[1, 1].set_title('Dataset Sizes', fontweight='bold')\n",
        "        axes[1, 1].set_ylabel('Number of Records')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        for i, v in enumerate(sizes):\n",
        "            axes[1, 1].text(i, v + max(sizes) * 0.01, f'{v:,}',\n",
        "                           ha='center', va='bottom', fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('preprocessing_summary.png', dpi=300, bbox_inches='tight')\n",
        "        wandb.log({\"preprocessing_summary\": wandb.Image('preprocessing_summary.png')})\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(\"Preprocessing summary created and logged\")\n",
        "\n",
        "        return train_df, test_df\n",
        "\n",
        "def run_data_loading_preprocessing(train_path, test_path, features_path, stores_path):\n",
        "    logger.info(\"Starting Data Loading and Preprocessing Pipeline\")\n",
        "    data_loader = DataLoader()\n",
        "    preprocessor = BasicPreprocessor()\n",
        "    train_df, test_df, features_df, stores_df = data_loader.load_datasets(\n",
        "        train_path, test_path, features_path, stores_path\n",
        "    )\n",
        "    train_merged, test_merged = preprocessor.merge_datasets(\n",
        "        train_df, test_df, features_df, stores_df\n",
        "    )\n",
        "    train_clean, test_clean = preprocessor.handle_missing_values(\n",
        "        train_merged, test_merged\n",
        "    )\n",
        "    train_final, test_final = preprocessor.create_preprocessing_summary(\n",
        "        train_clean, test_clean\n",
        "    )\n",
        "    logger.info(\"Data Loading and Preprocessing completed successfully\")\n",
        "\n",
        "    return train_final, test_final, preprocessor"
      ],
      "metadata": {
        "id": "gnPLY62Q6wWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engineering Time-based and Holiday-related Features\n",
        "Creating new features to capture the effects of holidays and temporal patterns on the data.\n"
      ],
      "metadata": {
        "id": "9sqKMmuLczt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostEngineering:\n",
        "    def __init__(self, project_name=\"walmart-sales-prediction\"):\n",
        "        self.project_name = project_name\n",
        "        self.train_df = None\n",
        "        self.test_df = None\n",
        "        self.wandb_run = None\n",
        "\n",
        "    def initialize_wandb(self):\n",
        "        self.wandb_run = wandb.init(\n",
        "            project=self.project_name,\n",
        "            name=\"engineering\",\n",
        "            tags=[\"feature-engineering\", \"feature-selection\", \"preprocessing\", \"xgboost\"],\n",
        "            reinit=True\n",
        "        )\n",
        "\n",
        "    def run_feature_engineering(self, train_df, test_df):\n",
        "        if self.wandb_run is None:\n",
        "            self.initialize_wandb()\n",
        "\n",
        "        self.train_df = train_df.copy()\n",
        "        self.test_df = test_df.copy()\n",
        "\n",
        "        initial_features = len(self.train_df.columns)\n",
        "\n",
        "        self.add_time_features()\n",
        "        self.add_holiday_features()\n",
        "        self.add_statistical_features()\n",
        "        self.add_markdown_features()\n",
        "        self.add_economic_features()\n",
        "\n",
        "        final_features = len(self.train_df.columns)\n",
        "\n",
        "        feature_engineering_metrics = {\n",
        "            \"initial_features\": initial_features,\n",
        "            \"final_features\": final_features,\n",
        "            \"new_features_created\": final_features - initial_features,\n",
        "            \"feature_density\": final_features / len(self.train_df),\n",
        "            \"avg_total_markdown\": self.train_df['Total_MarkDown'].mean(),\n",
        "            \"avg_economic_health\": self.train_df['Economic_Health'].mean(),\n",
        "            \"holiday_records_ratio\": self.train_df['IsHoliday'].mean()\n",
        "        }\n",
        "\n",
        "        wandb.log(feature_engineering_metrics)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.hist(self.train_df['Total_MarkDown'], bins=30, alpha=0.7)\n",
        "        plt.title('Total Markdown Distribution')\n",
        "        plt.xlabel('Total Markdown')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.hist(self.train_df['Economic_Health'], bins=30, alpha=0.7)\n",
        "        plt.title('Economic Health Distribution')\n",
        "        plt.xlabel('Economic Health')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        monthly_sales = self.train_df.groupby('Month')['Weekly_Sales'].mean()\n",
        "        plt.plot(monthly_sales.index, monthly_sales.values, marker='o')\n",
        "        plt.title('Average Sales by Month')\n",
        "        plt.xlabel('Month')\n",
        "        plt.ylabel('Average Sales')\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        holiday_sales = self.train_df.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
        "        plt.bar(['Non-Holiday', 'Holiday'], holiday_sales.values)\n",
        "        plt.title('Average Sales by Holiday Status')\n",
        "        plt.ylabel('Average Sales')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_engineering_analysis.png', dpi=200, bbox_inches='tight')\n",
        "        wandb.log({\"feature_engineering_analysis\": wandb.Image('feature_engineering_analysis.png')})\n",
        "        plt.close()\n",
        "\n",
        "        return self.train_df, self.test_df\n",
        "\n",
        "    def add_time_features(self):\n",
        "        for df in [self.train_df, self.test_df]:\n",
        "            df['Year'] = df['Date'].dt.year\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            df['Week'] = df['Date'].dt.isocalendar().week\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "            df['Quarter'] = df['Date'].dt.quarter\n",
        "            df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "            df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "            df['DayOfMonth'] = df['Date'].dt.day\n",
        "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "            df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "            df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "            df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "            df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "            df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "            df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "    def add_holiday_features(self):\n",
        "        super_bowl_dates = pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'])\n",
        "        labor_day_dates = pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'])\n",
        "        thanksgiving_dates = pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'])\n",
        "        christmas_dates = pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])\n",
        "\n",
        "        for df in [self.train_df, self.test_df]:\n",
        "            df['IsSuperBowl'] = df['Date'].isin(super_bowl_dates).astype(int)\n",
        "            df['IsLaborDay'] = df['Date'].isin(labor_day_dates).astype(int)\n",
        "            df['IsThanksgiving'] = df['Date'].isin(thanksgiving_dates).astype(int)\n",
        "            df['IsChristmas'] = df['Date'].isin(christmas_dates).astype(int)\n",
        "\n",
        "        self.train_df = self.add_holiday_proximity(self.train_df)\n",
        "        self.test_df = self.add_holiday_proximity(self.test_df)\n",
        "\n",
        "    def add_holiday_proximity(self, df):\n",
        "        df = df.copy()\n",
        "        df['HolidayProximity'] = 0\n",
        "\n",
        "        holidays = df.loc[df['IsHoliday'] == 1, 'Date'].unique()\n",
        "\n",
        "        for holiday in holidays:\n",
        "            days_diff = (df['Date'] - holiday).dt.days\n",
        "\n",
        "            mask_2weeks_before = (days_diff >= -14) & (days_diff < -7)\n",
        "            mask_1week_before = (days_diff >= -7) & (days_diff < 0)\n",
        "            mask_holiday = (days_diff == 0)\n",
        "            mask_1week_after = (days_diff > 0) & (days_diff <= 7)\n",
        "            mask_2weeks_after = (days_diff > 7) & (days_diff <= 14)\n",
        "\n",
        "            df.loc[mask_2weeks_before, 'HolidayProximity'] = -2\n",
        "            df.loc[mask_1week_before, 'HolidayProximity'] = -1\n",
        "            df.loc[mask_holiday, 'HolidayProximity'] = 0\n",
        "            df.loc[mask_1week_after, 'HolidayProximity'] = 1\n",
        "            df.loc[mask_2weeks_after, 'HolidayProximity'] = 2\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_statistical_features(self):\n",
        "        store_stats = self.train_df.groupby('Store')['Weekly_Sales'].agg(['mean', 'std', 'median']).reset_index()\n",
        "        store_stats.columns = ['Store', 'Store_Sales_Mean', 'Store_Sales_Std', 'Store_Sales_Median']\n",
        "\n",
        "        dept_stats = self.train_df.groupby('Dept')['Weekly_Sales'].agg(['mean', 'std', 'median']).reset_index()\n",
        "        dept_stats.columns = ['Dept', 'Dept_Sales_Mean', 'Dept_Sales_Std', 'Dept_Sales_Median']\n",
        "\n",
        "        store_dept_stats = self.train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].agg(['mean', 'std', 'median']).reset_index()\n",
        "        store_dept_stats.columns = ['Store', 'Dept', 'StoreDept_Sales_Mean', 'StoreDept_Sales_Std', 'StoreDept_Sales_Median']\n",
        "\n",
        "        self.train_df = self.train_df.merge(store_stats, on='Store', how='left')\n",
        "        self.test_df = self.test_df.merge(store_stats, on='Store', how='left')\n",
        "\n",
        "        self.train_df = self.train_df.merge(dept_stats, on='Dept', how='left')\n",
        "        self.test_df = self.test_df.merge(dept_stats, on='Dept', how='left')\n",
        "\n",
        "        self.train_df = self.train_df.merge(store_dept_stats, on=['Store', 'Dept'], how='left')\n",
        "        self.test_df = self.test_df.merge(store_dept_stats, on=['Store', 'Dept'], how='left')\n",
        "\n",
        "        stat_cols = ['Store_Sales_Mean', 'Store_Sales_Std', 'Store_Sales_Median',\n",
        "                    'Dept_Sales_Mean', 'Dept_Sales_Std', 'Dept_Sales_Median',\n",
        "                    'StoreDept_Sales_Mean', 'StoreDept_Sales_Std', 'StoreDept_Sales_Median']\n",
        "\n",
        "        for col in stat_cols:\n",
        "            if col in self.train_df.columns:\n",
        "                mean_val = self.train_df[col].mean()\n",
        "                self.train_df[col] = self.train_df[col].fillna(mean_val)\n",
        "                self.test_df[col] = self.test_df[col].fillna(mean_val)\n",
        "\n",
        "    def add_markdown_features(self):\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "        for df in [self.train_df, self.test_df]:\n",
        "            df['Total_MarkDown'] = df[markdown_cols].sum(axis=1)\n",
        "            df['Active_MarkDowns'] = (df[markdown_cols] > 0).sum(axis=1)\n",
        "            df['Max_MarkDown'] = df[markdown_cols].max(axis=1)\n",
        "            df['MarkDown_Intensity'] = df['Total_MarkDown'] / (df['Size'] + 1)\n",
        "\n",
        "    def add_economic_features(self):\n",
        "        for df in [self.train_df, self.test_df]:\n",
        "            df['Economic_Health'] = df['CPI'] / df['Unemployment']\n",
        "            df['Fuel_Impact'] = df['Fuel_Price'] / df['CPI']\n",
        "            df['CPI_Normalized'] = df['CPI'] / df['CPI'].mean()\n",
        "            df['Unemployment_Normalized'] = df['Unemployment'] / df['Unemployment'].mean()\n",
        "            df['Fuel_Price_Normalized'] = df['Fuel_Price'] / df['Fuel_Price'].mean()\n",
        "\n",
        "    def run_feature_selection(self, train_df, test_df, n_features=50):\n",
        "        if self.wandb_run is None:\n",
        "            self.initialize_wandb()\n",
        "\n",
        "        exclude_cols = ['Weekly_Sales', 'Date']\n",
        "        feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "        X = train_df[feature_cols].fillna(0)\n",
        "        y = train_df['Weekly_Sales']\n",
        "\n",
        "        selector = SelectKBest(score_func=f_regression, k=n_features)\n",
        "        X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "        selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
        "        feature_scores = selector.scores_\n",
        "\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': feature_cols,\n",
        "            'score': feature_scores,\n",
        "            'selected': selector.get_support()\n",
        "        }).sort_values('score', ascending=False)\n",
        "\n",
        "        selection_metrics = {\n",
        "            \"total_features\": len(feature_cols),\n",
        "            \"selected_features\": len(selected_features),\n",
        "            \"selection_ratio\": len(selected_features) / len(feature_cols),\n",
        "            \"avg_feature_score\": feature_scores.mean(),\n",
        "            \"max_feature_score\": feature_scores.max(),\n",
        "            \"min_selected_score\": feature_importance_df[feature_importance_df['selected']]['score'].min()\n",
        "        }\n",
        "\n",
        "        wandb.log(selection_metrics)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        top_features = feature_importance_df.head(20)\n",
        "        colors = ['red' if selected else 'blue' for selected in top_features['selected']]\n",
        "        plt.barh(range(len(top_features)), top_features['score'], color=colors)\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Feature Score')\n",
        "        plt.title('Top 20 Features (Red = Selected)')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_selection_analysis.png', dpi=200, bbox_inches='tight')\n",
        "        wandb.log({\"feature_selection_analysis\": wandb.Image('feature_selection_analysis.png')})\n",
        "        plt.close()\n",
        "\n",
        "        wandb.log({\n",
        "            \"selected_features_table\": wandb.Table(\n",
        "                data=[[f, s] for f, s in zip(selected_features,\n",
        "                     feature_importance_df[feature_importance_df['selected']]['score'].values)],\n",
        "                columns=[\"Feature\", \"Score\"]\n",
        "            )\n",
        "        })\n",
        "\n",
        "        return selected_features, feature_importance_df\n",
        "\n",
        "    def run_data_preprocessing(self, train_df, test_df, selected_features):\n",
        "        if self.wandb_run is None:\n",
        "            self.initialize_wandb()\n",
        "\n",
        "        X_train = train_df[selected_features].fillna(0)\n",
        "        y_train = train_df['Weekly_Sales']\n",
        "        X_test = test_df[selected_features].fillna(0)\n",
        "\n",
        "        weights = np.where(train_df['IsHoliday'] == 1, 5, 1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
        "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
        "\n",
        "        preprocessing_metrics = {\n",
        "            \"train_samples\": len(X_train_scaled),\n",
        "            \"test_samples\": len(X_test_scaled),\n",
        "            \"features_count\": len(selected_features),\n",
        "            \"holiday_samples\": np.sum(weights == 5),\n",
        "            \"non_holiday_samples\": np.sum(weights == 1),\n",
        "            \"feature_mean_after_scaling\": X_train_scaled.mean().mean(),\n",
        "            \"feature_std_after_scaling\": X_train_scaled.std().mean(),\n",
        "            \"target_mean\": y_train.mean(),\n",
        "            \"target_std\": y_train.std()\n",
        "        }\n",
        "\n",
        "        wandb.log(preprocessing_metrics)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.hist(y_train, bins=30, alpha=0.7)\n",
        "        plt.title('Target Distribution')\n",
        "        plt.xlabel('Weekly Sales')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.hist(weights, bins=10, alpha=0.7)\n",
        "        plt.title('Sample Weights Distribution')\n",
        "        plt.xlabel('Weight')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        correlation_matrix = X_train_scaled.corr()\n",
        "        plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
        "        plt.title('Feature Correlation Matrix')\n",
        "        plt.colorbar()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('preprocessing_analysis.png', dpi=200, bbox_inches='tight')\n",
        "        wandb.log({\"preprocessing_analysis\": wandb.Image('preprocessing_analysis.png')})\n",
        "        plt.close()\n",
        "\n",
        "        return X_train_scaled, X_test_scaled, y_train, weights, scaler\n",
        "\n",
        "    def finish_wandb(self):\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.finish()\n",
        "\n",
        "def get_feature_columns(df):\n",
        "    exclude_cols = ['Weekly_Sales', 'Date']\n",
        "    return [col for col in df.columns if col not in exclude_cols]"
      ],
      "metadata": {
        "id": "6haoL_fgNZHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key steps for training:\n",
        "\n",
        "* Prepare training and validation data, applying weighted errors for holidays.\n",
        "\n",
        "* Train an XGBoost regression model with custom hyperparameters.\n",
        "\n",
        "* Evaluate performance using MAE, RMSE, R², and a custom Weighted MAE.\n",
        "\n",
        "* Visualize results: learning curves, residuals, feature importances.\n",
        "\n",
        "* Generate and analyze predictions on test data.\n",
        "\n",
        "* Log everything to wandb for reproducibility and monitoring."
      ],
      "metadata": {
        "id": "qqd1rvy5dPGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class XGBoostTrainer:\n",
        "    def __init__(self, project_name=\"walmart-sales-prediction\"):\n",
        "        self.project_name = project_name\n",
        "        self.model = None\n",
        "        self.feature_importance = None\n",
        "        self.training_metrics = {}\n",
        "        self.scaler = None\n",
        "        self.selected_features = None\n",
        "        self.wandb_run = None\n",
        "\n",
        "    def initialize_wandb(self, run_name=\"training\"):\n",
        "        if self.wandb_run is None:\n",
        "            self.wandb_run = wandb.init(\n",
        "                project=self.project_name,\n",
        "                name=run_name,\n",
        "                tags=[\"training\", \"xgboost\", \"model\"],\n",
        "                reinit=True\n",
        "            )\n",
        "\n",
        "    def weighted_mean_absolute_error(self, y_true, y_pred, weights):\n",
        "        return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "    def prepare_training_data(self, train_df, selected_features):\n",
        "        logger.info(\"Preparing training data...\")\n",
        "\n",
        "        X = train_df[selected_features].fillna(0)\n",
        "        y = train_df['Weekly_Sales']\n",
        "\n",
        "        weights = np.where(train_df['IsHoliday'] == 1, 5, 1)\n",
        "\n",
        "        X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
        "            X, y, weights, test_size=0.2, random_state=42,\n",
        "            stratify=train_df['IsHoliday']\n",
        "        )\n",
        "\n",
        "        prep_metrics = {\n",
        "            'total_training_samples': len(X),\n",
        "            'train_split_samples': len(X_train),\n",
        "            'val_split_samples': len(X_val),\n",
        "            'total_features': len(selected_features),\n",
        "            'holiday_samples': np.sum(weights == 5),\n",
        "            'non_holiday_samples': np.sum(weights == 1),\n",
        "            'holiday_weight_ratio': 5.0\n",
        "        }\n",
        "\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log(prep_metrics)\n",
        "        self.training_metrics.update(prep_metrics)\n",
        "\n",
        "        logger.info(f\"Training data prepared: {len(X_train)} train, {len(X_val)} validation samples\")\n",
        "        logger.info(f\"Holiday samples: {np.sum(w_train == 5)} train, {np.sum(w_val == 5)} validation\")\n",
        "\n",
        "        return X_train, X_val, y_train, y_val, w_train, w_val\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, w_train, w_val):\n",
        "        logger.info(\"Starting XGBoost model training...\")\n",
        "\n",
        "        training_start_time = datetime.now()\n",
        "\n",
        "        params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'eval_metric': 'mae',\n",
        "            'max_depth': 8,\n",
        "            'learning_rate': 0.05,\n",
        "            'n_estimators': 2700,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'colsample_bylevel': 0.8,\n",
        "            'reg_alpha': 1,\n",
        "            'reg_lambda': 1,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'tree_method': 'hist',\n",
        "            'early_stopping_rounds': 50\n",
        "        }\n",
        "\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log({\"model_params\": params})\n",
        "\n",
        "        self.model = xgb.XGBRegressor(**params)\n",
        "\n",
        "        logger.info(\"Training XGBoost model...\")\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            sample_weight=w_train,\n",
        "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "            sample_weight_eval_set=[w_train, w_val],\n",
        "            verbose=100\n",
        "        )\n",
        "\n",
        "        training_end_time = datetime.now()\n",
        "        training_duration = (training_end_time - training_start_time).total_seconds()\n",
        "\n",
        "        logger.info(f\"Training completed in {training_duration:.2f} seconds\")\n",
        "\n",
        "        return training_duration\n",
        "\n",
        "    def evaluate_model(self, X_train, y_train, X_val, y_val, w_train, w_val, training_duration):\n",
        "        logger.info(\"Evaluating model performance...\")\n",
        "\n",
        "        train_pred = self.model.predict(X_train)\n",
        "        val_pred = self.model.predict(X_val)\n",
        "\n",
        "        train_wmae = self.weighted_mean_absolute_error(y_train, train_pred, w_train)\n",
        "        val_wmae = self.weighted_mean_absolute_error(y_val, val_pred, w_val)\n",
        "\n",
        "        train_mae = mean_absolute_error(y_train, train_pred)\n",
        "        val_mae = mean_absolute_error(y_val, val_pred)\n",
        "\n",
        "        train_mse = mean_squared_error(y_train, train_pred)\n",
        "        val_mse = mean_squared_error(y_val, val_pred)\n",
        "\n",
        "        train_rmse = np.sqrt(train_mse)\n",
        "        val_rmse = np.sqrt(val_mse)\n",
        "\n",
        "        train_r2 = r2_score(y_train, train_pred)\n",
        "        val_r2 = r2_score(y_val, val_pred)\n",
        "\n",
        "        metrics = {\n",
        "            'train_wmae': train_wmae,\n",
        "            'val_wmae': val_wmae,\n",
        "            'train_mae': train_mae,\n",
        "            'val_mae': val_mae,\n",
        "            'train_mse': train_mse,\n",
        "            'val_mse': val_mse,\n",
        "            'train_rmse': train_rmse,\n",
        "            'val_rmse': val_rmse,\n",
        "            'train_r2': train_r2,\n",
        "            'val_r2': val_r2,\n",
        "            'training_duration_seconds': training_duration,\n",
        "            'overfitting_ratio': val_wmae / train_wmae\n",
        "        }\n",
        "\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log(metrics)\n",
        "        self.training_metrics.update(metrics)\n",
        "\n",
        "        logger.info(f\"Training WMAE: {train_wmae:.2f}\")\n",
        "        logger.info(f\"Validation WMAE: {val_wmae:.2f}\")\n",
        "        logger.info(f\"Training MAE: {train_mae:.2f}\")\n",
        "        logger.info(f\"Validation MAE: {val_mae:.2f}\")\n",
        "        logger.info(f\"Training R²: {train_r2:.4f}\")\n",
        "        logger.info(f\"Validation R²: {val_r2:.4f}\")\n",
        "        logger.info(f\"Overfitting ratio: {val_wmae/train_wmae:.3f}\")\n",
        "\n",
        "        return train_pred, val_pred, metrics\n",
        "\n",
        "    def analyze_feature_importance(self, feature_names):\n",
        "        logger.info(\"Analyzing feature importance...\")\n",
        "\n",
        "        self.feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        logger.info(\"Top 15 Most Important Features:\")\n",
        "        top_features = self.feature_importance.head(15)\n",
        "        for idx, row in top_features.iterrows():\n",
        "            logger.info(f\"{row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log({\n",
        "                \"feature_importance_table\": wandb.Table(\n",
        "                    data=self.feature_importance.head(20).values.tolist(),\n",
        "                    columns=[\"Feature\", \"Importance\"]\n",
        "                )\n",
        "            })\n",
        "\n",
        "        return self.feature_importance\n",
        "\n",
        "    def create_training_plots(self, X_train, y_train, X_val, y_val, train_pred, val_pred):\n",
        "        logger.info(\"Creating training visualization plots...\")\n",
        "\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        plt.subplot(2, 4, 1)\n",
        "        plt.scatter(y_train, train_pred, alpha=0.5, s=1)\n",
        "        plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual Sales')\n",
        "        plt.ylabel('Predicted Sales')\n",
        "        plt.title('Training: Actual vs Predicted')\n",
        "\n",
        "        plt.subplot(2, 4, 2)\n",
        "        plt.scatter(y_val, val_pred, alpha=0.5, s=1)\n",
        "        plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual Sales')\n",
        "        plt.ylabel('Predicted Sales')\n",
        "        plt.title('Validation: Actual vs Predicted')\n",
        "\n",
        "        plt.subplot(2, 4, 3)\n",
        "        residuals_train = y_train - train_pred\n",
        "        plt.scatter(train_pred, residuals_train, alpha=0.5, s=1)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Sales')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Training Residuals')\n",
        "\n",
        "        plt.subplot(2, 4, 4)\n",
        "        residuals_val = y_val - val_pred\n",
        "        plt.scatter(val_pred, residuals_val, alpha=0.5, s=1)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Sales')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Validation Residuals')\n",
        "\n",
        "        plt.subplot(2, 4, 5)\n",
        "        top_features = self.feature_importance.head(15)\n",
        "        plt.barh(range(len(top_features)), top_features['importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Importance')\n",
        "        plt.title('Top 15 Feature Importances')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        plt.subplot(2, 4, 6)\n",
        "        plt.hist(residuals_train, bins=50, alpha=0.7, label='Training', density=True)\n",
        "        plt.hist(residuals_val, bins=50, alpha=0.7, label='Validation', density=True)\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Residuals Distribution')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(2, 4, 7)\n",
        "        train_errors = np.abs(residuals_train)\n",
        "        val_errors = np.abs(residuals_val)\n",
        "        plt.hist(train_errors, bins=50, alpha=0.7, label='Training', density=True)\n",
        "        plt.hist(val_errors, bins=50, alpha=0.7, label='Validation', density=True)\n",
        "        plt.xlabel('Absolute Error')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Absolute Error Distribution')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(2, 4, 8)\n",
        "        learning_curve = self.model.evals_result()\n",
        "        if learning_curve:\n",
        "            train_mae = learning_curve['validation_0']['mae']\n",
        "            val_mae = learning_curve['validation_1']['mae']\n",
        "            plt.plot(train_mae, label='Training MAE')\n",
        "            plt.plot(val_mae, label='Validation MAE')\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.title('Learning Curve')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log({\"training_analysis\": wandb.Image('training_analysis.png')})\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(\"Training plots created and saved\")\n",
        "\n",
        "    def generate_predictions(self, test_df, selected_features):\n",
        "        logger.info(\"Generating test predictions...\")\n",
        "\n",
        "        X_test = test_df[selected_features].fillna(0)\n",
        "        test_predictions = self.model.predict(X_test)\n",
        "\n",
        "        return test_predictions\n",
        "\n",
        "    def create_prediction_analysis(self, test_df, predictions):\n",
        "        logger.info(\"Creating prediction analysis plots...\")\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.hist(predictions, bins=50, alpha=0.7, edgecolor='black')\n",
        "        plt.title('Prediction Distribution')\n",
        "        plt.xlabel('Predicted Sales')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        test_df_copy = test_df.copy()\n",
        "        test_df_copy['Predictions'] = predictions\n",
        "        monthly_pred = test_df_copy.groupby(test_df_copy['Date'].dt.to_period('M'))['Predictions'].mean()\n",
        "        monthly_pred.plot()\n",
        "        plt.title('Monthly Prediction Trend')\n",
        "        plt.xlabel('Month')\n",
        "        plt.ylabel('Average Predicted Sales')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(2, 3, 3)\n",
        "        store_pred = test_df_copy.groupby('Store')['Predictions'].mean()\n",
        "        store_pred.plot(kind='bar')\n",
        "        plt.title('Average Predictions by Store')\n",
        "        plt.xlabel('Store')\n",
        "        plt.ylabel('Average Predicted Sales')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(2, 3, 4)\n",
        "        holiday_pred = test_df_copy.groupby('IsHoliday')['Predictions'].mean()\n",
        "        holiday_pred.plot(kind='bar')\n",
        "        plt.title('Holiday vs Non-Holiday Predictions')\n",
        "        plt.xlabel('IsHoliday')\n",
        "        plt.ylabel('Average Predicted Sales')\n",
        "\n",
        "        plt.subplot(2, 3, 5)\n",
        "        dept_pred = test_df_copy.groupby('Dept')['Predictions'].mean().sort_values(ascending=False).head(10)\n",
        "        dept_pred.plot(kind='bar')\n",
        "        plt.title('Top 10 Departments by Predicted Sales')\n",
        "        plt.xlabel('Department')\n",
        "        plt.ylabel('Average Predicted Sales')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.boxplot([test_df_copy[test_df_copy['IsHoliday'] == 0]['Predictions'],\n",
        "                    test_df_copy[test_df_copy['IsHoliday'] == 1]['Predictions']],\n",
        "                   labels=['Non-Holiday', 'Holiday'])\n",
        "        plt.title('Prediction Distribution by Holiday Status')\n",
        "        plt.ylabel('Predicted Sales')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log({\"prediction_analysis\": wandb.Image('prediction_analysis.png')})\n",
        "        plt.close()\n",
        "\n",
        "        pred_stats = {\n",
        "            'prediction_mean': predictions.mean(),\n",
        "            'prediction_median': np.median(predictions),\n",
        "            'prediction_std': predictions.std(),\n",
        "            'prediction_min': predictions.min(),\n",
        "            'prediction_max': predictions.max(),\n",
        "            'negative_predictions': np.sum(predictions < 0),\n",
        "            'zero_predictions': np.sum(predictions == 0),\n",
        "            'holiday_avg_prediction': test_df_copy[test_df_copy['IsHoliday'] == 1]['Predictions'].mean(),\n",
        "            'non_holiday_avg_prediction': test_df_copy[test_df_copy['IsHoliday'] == 0]['Predictions'].mean()\n",
        "        }\n",
        "\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.log(pred_stats)\n",
        "        logger.info(f\"Prediction statistics: {pred_stats}\")\n",
        "\n",
        "        return pred_stats\n",
        "\n",
        "    def run_complete_training(self, train_df, test_df, selected_features):\n",
        "        self.initialize_wandb(\"training\")\n",
        "\n",
        "        X_train, X_val, y_train, y_val, w_train, w_val = self.prepare_training_data(train_df, selected_features)\n",
        "\n",
        "        training_duration = self.train_model(X_train, y_train, X_val, y_val, w_train, w_val)\n",
        "\n",
        "        train_pred, val_pred, metrics = self.evaluate_model(X_train, y_train, X_val, y_val, w_train, w_val, training_duration)\n",
        "\n",
        "        self.analyze_feature_importance(selected_features)\n",
        "\n",
        "        self.create_training_plots(X_train, y_train, X_val, y_val, train_pred, val_pred)\n",
        "\n",
        "        test_predictions = self.generate_predictions(test_df, selected_features)\n",
        "\n",
        "        pred_stats = self.create_prediction_analysis(test_df, test_predictions)\n",
        "\n",
        "        return self.model, test_predictions, metrics, pred_stats\n",
        "\n",
        "    def finish_wandb(self):\n",
        "        if self.wandb_run is not None:\n",
        "            wandb.finish()\n",
        "            self.wandb_run = None\n",
        "\n",
        "\n",
        "def main():\n",
        "    wandb.init(\n",
        "        project=\"walmart-sales-prediction\",\n",
        "        name=f\"walmart_sales_complete_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        config={\n",
        "            \"model_type\": \"XGBoost\",\n",
        "            \"objective\": \"sales_forecasting\",\n",
        "            \"dataset\": \"walmart_sales\",\n",
        "            \"pipeline_version\": \"v1.0\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    logger.info(\" Starting Walmart Sales Prediction Pipeline\")\n",
        "    pipeline_start_time = datetime.now()\n",
        "\n",
        "    train_path = '/content/drive/MyDrive/ML/final/train.csv'\n",
        "    test_path = '/content/drive/MyDrive/ML/final/test.csv'\n",
        "    features_path = '/content/drive/MyDrive/ML/final/features.csv'\n",
        "    stores_path = '/content/drive/MyDrive/ML/final/stores.csv'\n",
        "\n",
        "    try:\n",
        "        logger.info(\" Step 1: Data Loading and Preprocessing\")\n",
        "        train_df, test_df, preprocessor = run_data_loading_preprocessing(\n",
        "            train_path, test_path, features_path, stores_path\n",
        "        )\n",
        "\n",
        "        logger.info(\" Step 2: Feature Engineering\")\n",
        "        feature_engineer = XGBoostEngineering()\n",
        "        train_engineered, test_engineered = feature_engineer.run_feature_engineering(train_df, test_df)\n",
        "\n",
        "        logger.info(\" Step 3: Feature Selection\")\n",
        "        selected_features, feature_importance_df = feature_engineer.run_feature_selection(\n",
        "            train_engineered, test_engineered, n_features=50\n",
        "        )\n",
        "\n",
        "        logger.info(\" Step 4: Data Preprocessing for Training\")\n",
        "        X_train_scaled, X_test_scaled, y_train, weights, scaler = feature_engineer.run_data_preprocessing(\n",
        "            train_engineered, test_engineered, selected_features\n",
        "        )\n",
        "\n",
        "        logger.info(\"Step 5: XGBoost Model Training\")\n",
        "        trainer = XGBoostTrainer()\n",
        "        model, test_predictions, training_metrics, pred_stats = trainer.run_complete_training(\n",
        "            train_engineered, test_engineered, selected_features\n",
        "        )\n",
        "\n",
        "        logger.info(\"Step 6: Creating Submission File\")\n",
        "        submission = pd.DataFrame({\n",
        "            'Id': test_engineered['Store'].astype(str) + '_' +\n",
        "                  test_engineered['Dept'].astype(str) + '_' +\n",
        "                  test_engineered['Date'].dt.strftime('%Y-%m-%d'),\n",
        "            'Weekly_Sales': test_predictions\n",
        "        })\n",
        "\n",
        "        pipeline_end_time = datetime.now()\n",
        "        total_pipeline_duration = (pipeline_end_time - pipeline_start_time).total_seconds()\n",
        "\n",
        "        final_metrics = {\n",
        "            'submission_records': len(submission),\n",
        "            'unique_store_dept_combinations': len(submission['Id'].unique()),\n",
        "            'total_pipeline_duration_seconds': total_pipeline_duration,\n",
        "            'total_pipeline_duration_minutes': total_pipeline_duration / 60,\n",
        "            'pipeline_success': True\n",
        "        }\n",
        "\n",
        "        wandb.log(final_metrics)\n",
        "\n",
        "        submission.to_csv('walmart_sales_predictions.csv', index=False)\n",
        "        logger.info(\"Predictions saved to 'walmart_sales_predictions.csv'\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"submission_sample\": wandb.Table(\n",
        "                data=submission.head(10).values.tolist(),\n",
        "                columns=[\"Id\", \"Weekly_Sales\"]\n",
        "            )\n",
        "        })\n",
        "\n",
        "        wandb.save('walmart_sales_predictions.csv')\n",
        "\n",
        "        logger.info(\"Pipeline completed successfully!\")\n",
        "        logger.info(f\"Total pipeline duration: {total_pipeline_duration:.2f} seconds ({total_pipeline_duration/60:.2f} minutes)\")\n",
        "        logger.info(f\"Submission shape: {submission.shape}\")\n",
        "        logger.info(f\"Best validation WMAE: {training_metrics.get('val_wmae', 'N/A')}\")\n",
        "        logger.info(f\"Model R² score: {training_metrics.get('val_r2', 'N/A')}\")\n",
        "\n",
        "        logger.info(\"Submission preview:\")\n",
        "        logger.info(submission.head().to_string())\n",
        "\n",
        "        wandb.finish()\n",
        "\n",
        "        return model, submission, training_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed with error: {str(e)}\")\n",
        "        wandb.log({\"pipeline_success\": False, \"error\": str(e)})\n",
        "        wandb.finish()\n",
        "        raise e\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, submission, metrics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEYabCEdB0Lq",
        "outputId": "292eafd2-0aa7-43bd-cb61-ed00f4f9bafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_economic_health</td><td>▁</td></tr><tr><td>avg_feature_score</td><td>▁</td></tr><tr><td>avg_total_markdown</td><td>▁</td></tr><tr><td>feature_density</td><td>▁</td></tr><tr><td>final_features</td><td>▁</td></tr><tr><td>holiday_records_ratio</td><td>▁</td></tr><tr><td>initial_features</td><td>▁</td></tr><tr><td>max_feature_score</td><td>▁</td></tr><tr><td>min_selected_score</td><td>▁</td></tr><tr><td>new_features_created</td><td>▁</td></tr><tr><td>selected_features</td><td>▁</td></tr><tr><td>selection_ratio</td><td>▁</td></tr><tr><td>total_features</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_economic_health</td><td>22.83277</td></tr><tr><td>avg_feature_score</td><td>179647.37661</td></tr><tr><td>avg_total_markdown</td><td>6684.04143</td></tr><tr><td>feature_density</td><td>0.00013</td></tr><tr><td>final_features</td><td>54</td></tr><tr><td>holiday_records_ratio</td><td>0.07036</td></tr><tr><td>initial_features</td><td>16</td></tr><tr><td>max_feature_score</td><td>3935213.15216</td></tr><tr><td>min_selected_score</td><td>0</td></tr><tr><td>new_features_created</td><td>38</td></tr><tr><td>selected_features</td><td>50</td></tr><tr><td>selection_ratio</td><td>0.96154</td></tr><tr><td>total_features</td><td>52</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">engineering</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/kvkwi9wn' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/kvkwi9wn</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250707_143036-kvkwi9wn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250707_143141-gtfg72gb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/gtfg72gb' target=\"_blank\">walmart_sales_complete_pipeline_20250707_143141</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/gtfg72gb' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/gtfg72gb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_weekly_sales</td><td>▁</td></tr><tr><td>columns_added</td><td>▁</td></tr><tr><td>date_range_days</td><td>▁</td></tr><tr><td>economic_columns_filled</td><td>▁</td></tr><tr><td>holiday_percentage</td><td>▁</td></tr><tr><td>markdown_columns_filled</td><td>▁</td></tr><tr><td>max_weekly_sales</td><td>▁</td></tr><tr><td>median_weekly_sales</td><td>▁</td></tr><tr><td>min_weekly_sales</td><td>▁</td></tr><tr><td>negative_sales_count</td><td>▁</td></tr><tr><td>std_weekly_sales</td><td>▁</td></tr><tr><td>total_missing_values_handled</td><td>▁</td></tr><tr><td>total_records_test</td><td>▁</td></tr><tr><td>total_records_train</td><td>▁</td></tr><tr><td>unique_departments</td><td>▁</td></tr><tr><td>unique_stores</td><td>▁</td></tr><tr><td>zero_sales_count</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_weekly_sales</td><td>15981.25812</td></tr><tr><td>columns_added</td><td>11</td></tr><tr><td>date_range_days</td><td>994</td></tr><tr><td>economic_columns_filled</td><td>2</td></tr><tr><td>holiday_percentage</td><td>7.03584</td></tr><tr><td>markdown_columns_filled</td><td>5</td></tr><tr><td>max_weekly_sales</td><td>693099.36</td></tr><tr><td>median_weekly_sales</td><td>7612.03</td></tr><tr><td>min_weekly_sales</td><td>-4988.94</td></tr><tr><td>negative_sales_count</td><td>1285</td></tr><tr><td>std_weekly_sales</td><td>22711.18352</td></tr><tr><td>total_missing_values_handled</td><td>1422431</td></tr><tr><td>total_records_test</td><td>115064</td></tr><tr><td>total_records_train</td><td>421570</td></tr><tr><td>unique_departments</td><td>81</td></tr><tr><td>unique_stores</td><td>45</td></tr><tr><td>zero_sales_count</td><td>73</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">walmart_sales_complete_pipeline_20250707_143141</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/gtfg72gb' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/gtfg72gb</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a><br>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250707_143141-gtfg72gb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250707_143157-b656rr4i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/b656rr4i' target=\"_blank\">engineering</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/b656rr4i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/b656rr4i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_economic_health</td><td>▁</td></tr><tr><td>avg_feature_score</td><td>▁</td></tr><tr><td>avg_total_markdown</td><td>▁</td></tr><tr><td>feature_density</td><td>▁</td></tr><tr><td>feature_mean_after_scaling</td><td>▁</td></tr><tr><td>feature_std_after_scaling</td><td>▁</td></tr><tr><td>features_count</td><td>▁</td></tr><tr><td>final_features</td><td>▁</td></tr><tr><td>holiday_records_ratio</td><td>▁</td></tr><tr><td>holiday_samples</td><td>▁</td></tr><tr><td>initial_features</td><td>▁</td></tr><tr><td>max_feature_score</td><td>▁</td></tr><tr><td>min_selected_score</td><td>▁</td></tr><tr><td>new_features_created</td><td>▁</td></tr><tr><td>non_holiday_samples</td><td>▁</td></tr><tr><td>selected_features</td><td>▁</td></tr><tr><td>selection_ratio</td><td>▁</td></tr><tr><td>target_mean</td><td>▁</td></tr><tr><td>target_std</td><td>▁</td></tr><tr><td>test_samples</td><td>▁</td></tr><tr><td>total_features</td><td>▁</td></tr><tr><td>train_samples</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_economic_health</td><td>22.83277</td></tr><tr><td>avg_feature_score</td><td>179647.37661</td></tr><tr><td>avg_total_markdown</td><td>6684.04143</td></tr><tr><td>feature_density</td><td>0.00013</td></tr><tr><td>feature_mean_after_scaling</td><td>-0.0</td></tr><tr><td>feature_std_after_scaling</td><td>0.98</td></tr><tr><td>features_count</td><td>50</td></tr><tr><td>final_features</td><td>54</td></tr><tr><td>holiday_records_ratio</td><td>0.07036</td></tr><tr><td>holiday_samples</td><td>29661</td></tr><tr><td>initial_features</td><td>16</td></tr><tr><td>max_feature_score</td><td>3935213.15216</td></tr><tr><td>min_selected_score</td><td>0</td></tr><tr><td>new_features_created</td><td>38</td></tr><tr><td>non_holiday_samples</td><td>391909</td></tr><tr><td>selected_features</td><td>50</td></tr><tr><td>selection_ratio</td><td>0.96154</td></tr><tr><td>target_mean</td><td>15981.25812</td></tr><tr><td>target_std</td><td>22711.18352</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>total_features</td><td>52</td></tr><tr><td>train_samples</td><td>421570</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">engineering</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/b656rr4i' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/b656rr4i</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a><br>Synced 5 W&B file(s), 4 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250707_143157-b656rr4i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250707_143214-cyzi0mxv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/cyzi0mxv' target=\"_blank\">training</a></strong> to <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/cyzi0mxv' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/cyzi0mxv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-mae:14651.87986\tvalidation_1-mae:14751.21257\n",
            "[100]\tvalidation_0-mae:1816.70698\tvalidation_1-mae:1977.11188\n",
            "[200]\tvalidation_0-mae:1514.50806\tvalidation_1-mae:1714.66426\n",
            "[300]\tvalidation_0-mae:1357.38421\tvalidation_1-mae:1587.32525\n",
            "[400]\tvalidation_0-mae:1250.19743\tvalidation_1-mae:1508.59793\n",
            "[500]\tvalidation_0-mae:1165.05481\tvalidation_1-mae:1447.37914\n",
            "[600]\tvalidation_0-mae:1099.58853\tvalidation_1-mae:1404.92889\n",
            "[700]\tvalidation_0-mae:1046.96577\tvalidation_1-mae:1371.55526\n",
            "[800]\tvalidation_0-mae:1006.80519\tvalidation_1-mae:1348.66384\n",
            "[900]\tvalidation_0-mae:966.38259\tvalidation_1-mae:1325.13685\n",
            "[1000]\tvalidation_0-mae:933.31555\tvalidation_1-mae:1307.21165\n",
            "[1100]\tvalidation_0-mae:901.73826\tvalidation_1-mae:1290.35726\n",
            "[1200]\tvalidation_0-mae:873.27153\tvalidation_1-mae:1275.68019\n",
            "[1300]\tvalidation_0-mae:845.87437\tvalidation_1-mae:1261.97867\n",
            "[1400]\tvalidation_0-mae:821.91526\tvalidation_1-mae:1250.91354\n",
            "[1500]\tvalidation_0-mae:798.44644\tvalidation_1-mae:1239.27698\n",
            "[1600]\tvalidation_0-mae:777.37973\tvalidation_1-mae:1229.38753\n",
            "[1700]\tvalidation_0-mae:759.00513\tvalidation_1-mae:1221.20533\n",
            "[1800]\tvalidation_0-mae:741.01594\tvalidation_1-mae:1213.64097\n",
            "[1900]\tvalidation_0-mae:723.62604\tvalidation_1-mae:1206.29191\n",
            "[2000]\tvalidation_0-mae:706.98393\tvalidation_1-mae:1199.29404\n",
            "[2100]\tvalidation_0-mae:691.94731\tvalidation_1-mae:1193.56017\n",
            "[2200]\tvalidation_0-mae:677.81298\tvalidation_1-mae:1188.09831\n",
            "[2300]\tvalidation_0-mae:663.31055\tvalidation_1-mae:1181.76143\n",
            "[2400]\tvalidation_0-mae:650.10526\tvalidation_1-mae:1176.53090\n",
            "[2500]\tvalidation_0-mae:636.81661\tvalidation_1-mae:1171.53530\n",
            "[2600]\tvalidation_0-mae:624.75414\tvalidation_1-mae:1167.27665\n",
            "[2699]\tvalidation_0-mae:613.50002\tvalidation_1-mae:1163.57425\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>holiday_avg_prediction</td><td>▁</td></tr><tr><td>holiday_samples</td><td>▁</td></tr><tr><td>holiday_weight_ratio</td><td>▁</td></tr><tr><td>negative_predictions</td><td>▁</td></tr><tr><td>non_holiday_avg_prediction</td><td>▁</td></tr><tr><td>non_holiday_samples</td><td>▁</td></tr><tr><td>overfitting_ratio</td><td>▁</td></tr><tr><td>prediction_max</td><td>▁</td></tr><tr><td>prediction_mean</td><td>▁</td></tr><tr><td>prediction_median</td><td>▁</td></tr><tr><td>prediction_min</td><td>▁</td></tr><tr><td>prediction_std</td><td>▁</td></tr><tr><td>submission_records</td><td>▁</td></tr><tr><td>total_features</td><td>▁</td></tr><tr><td>total_pipeline_duration_minutes</td><td>▁</td></tr><tr><td>total_pipeline_duration_seconds</td><td>▁</td></tr><tr><td>total_training_samples</td><td>▁</td></tr><tr><td>train_mae</td><td>▁</td></tr><tr><td>train_mse</td><td>▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>train_rmse</td><td>▁</td></tr><tr><td>train_split_samples</td><td>▁</td></tr><tr><td>train_wmae</td><td>▁</td></tr><tr><td>training_duration_seconds</td><td>▁</td></tr><tr><td>unique_store_dept_combinations</td><td>▁</td></tr><tr><td>val_mae</td><td>▁</td></tr><tr><td>val_mse</td><td>▁</td></tr><tr><td>val_r2</td><td>▁</td></tr><tr><td>val_rmse</td><td>▁</td></tr><tr><td>val_split_samples</td><td>▁</td></tr><tr><td>val_wmae</td><td>▁</td></tr><tr><td>zero_predictions</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>holiday_avg_prediction</td><td>18752.87109</td></tr><tr><td>holiday_samples</td><td>29661</td></tr><tr><td>holiday_weight_ratio</td><td>5</td></tr><tr><td>negative_predictions</td><td>2671</td></tr><tr><td>non_holiday_avg_prediction</td><td>16568.99023</td></tr><tr><td>non_holiday_samples</td><td>391909</td></tr><tr><td>overfitting_ratio</td><td>1.89662</td></tr><tr><td>pipeline_success</td><td>True</td></tr><tr><td>prediction_max</td><td>616618.375</td></tr><tr><td>prediction_mean</td><td>16738.44141</td></tr><tr><td>prediction_median</td><td>7970.24219</td></tr><tr><td>prediction_min</td><td>-2805.96997</td></tr><tr><td>prediction_std</td><td>23636.26172</td></tr><tr><td>submission_records</td><td>115064</td></tr><tr><td>total_features</td><td>50</td></tr><tr><td>total_pipeline_duration_minutes</td><td>13.46572</td></tr><tr><td>total_pipeline_duration_seconds</td><td>807.94334</td></tr><tr><td>total_training_samples</td><td>421570</td></tr><tr><td>train_mae</td><td>670.52362</td></tr><tr><td>train_mse</td><td>1097390.97472</td></tr><tr><td>train_r2</td><td>0.99786</td></tr><tr><td>train_rmse</td><td>1047.56431</td></tr><tr><td>train_split_samples</td><td>337256</td></tr><tr><td>train_wmae</td><td>613.50002</td></tr><tr><td>training_duration_seconds</td><td>676.66589</td></tr><tr><td>unique_store_dept_combinations</td><td>115064</td></tr><tr><td>val_mae</td><td>1088.3857</td></tr><tr><td>val_mse</td><td>5115182.87516</td></tr><tr><td>val_r2</td><td>0.99022</td></tr><tr><td>val_rmse</td><td>2261.67701</td></tr><tr><td>val_split_samples</td><td>84314</td></tr><tr><td>val_wmae</td><td>1163.57425</td></tr><tr><td>zero_predictions</td><td>0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">training</strong> at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/cyzi0mxv' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction/runs/cyzi0mxv</a><br> View project at: <a href='https://wandb.ai/final-project-ml/walmart-sales-prediction' target=\"_blank\">https://wandb.ai/final-project-ml/walmart-sales-prediction</a><br>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250707_143214-cyzi0mxv/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}